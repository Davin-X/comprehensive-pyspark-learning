{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interview Challenge 4: Data Processing Pipeline\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "You are given a dataset of user transactions with the following schema:\n",
    "- `user_id` (string)\n",
    "- `transaction_id` (string) \n",
    "- `amount` (double)\n",
    "- `timestamp` (string in 'yyyy-MM-dd HH:mm:ss' format)\n",
    "- `category` (string)\n",
    "\n",
    "## Tasks\n",
    "\n",
    "1. **Data Cleaning**: Remove transactions with null `user_id`, negative amounts, or invalid timestamps\n",
    "2. **Data Transformation**: \n",
    "   - Convert `timestamp` to proper timestamp type\n",
    "   - Add a `date` column (just the date part)\n",
    "   - Add a `hour` column (hour of the day)\n",
    "3. **Aggregations**:\n",
    "   - Calculate total amount per user per day\n",
    "   - Calculate average transaction amount per category per hour\n",
    "4. **Anomaly Detection**: Flag transactions that are 3 standard deviations above the user's daily average\n",
    "\n",
    "## Requirements\n",
    "- Use PySpark DataFrame API\n",
    "- Handle edge cases (empty data, malformed records)\n",
    "- Optimize for performance where possible\n",
    "- Include comments explaining your approach\n",
    "\n",
    "## Sample Data\n",
    "```python\n",
    "data = [\n",
    "    ('user1', 'txn1', 100.0, '2023-01-01 10:30:00', 'shopping'),\n",
    "    ('user1', 'txn2', 50.0, '2023-01-01 14:20:00', 'food'),\n",
    "    ('user2', 'txn3', 200.0, '2023-01-01 11:15:00', 'shopping'),\n",
    "    # ... more data\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ Try It Yourself\n",
    "\n",
    "Implement the solution using the code structure below. Start with data cleaning, then add transformations, aggregations, and finally anomaly detection.\n",
    "\n",
    "**Tip:** Work through each task step by step. Test your code after each major section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(\"InterviewChallenge4\").getOrCreate()\n",
    "\n",
    "# Sample data (in real scenario, this would be read from file)\n",
    "data = [\n",
    "    ('user1', 'txn1', 100.0, '2023-01-01 10:30:00', 'shopping'),\n",
    "    ('user1', 'txn2', 50.0, '2023-01-01 14:20:00', 'food'),\n",
    "    ('user2', 'txn3', 200.0, '2023-01-01 11:15:00', 'shopping'),\n",
    "    ('user1', 'txn4', -10.0, '2023-01-01 16:45:00', 'invalid'),  # negative amount\n",
    "    (None, 'txn5', 75.0, '2023-01-01 12:00:00', 'food'),  # null user_id\n",
    "    ('user2', 'txn6', 300.0, '2023-01-01 09:30:00', 'shopping'),\n",
    "]\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField('user_id', StringType(), True),\n",
    "    StructField('transaction_id', StringType(), True),\n",
    "    StructField('amount', DoubleType(), True),\n",
    "    StructField('timestamp', StringType(), True),\n",
    "    StructField('category', StringType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()\n",
    "\n",
    "# === YOUR SOLUTION GOES HERE ===\n",
    "# Implement the 4 tasks below\n",
    "\n",
    "# Task 1: Data Cleaning\n",
    "# Remove transactions with null user_id, negative amounts, or invalid timestamps\n",
    "\n",
    "# Task 2: Data Transformation\n",
    "# Convert timestamp to proper format, add date and hour columns\n",
    "\n",
    "# Task 3: Aggregations\n",
    "# Calculate total amount per user per day\n",
    "# Calculate average transaction amount per category per hour\n",
    "\n",
    "# Task 4: Anomaly Detection\n",
    "# Flag transactions that are 3 standard deviations above user's daily average\n",
    "\n",
    "# Display your results\n",
    "print(\"Implementation completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… Solution Reference\n",
    "\n",
    "**âš ï¸ Only refer to this after attempting the problem yourself!**\n",
    "\n",
    "Here's a complete solution implementation. Compare it with your approach and understand the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION IMPLEMENTATION\n",
    "\n",
    "# 1. Data Cleaning\n",
    "print(\"1. Data Cleaning:\")\n",
    "cleaned_df = df.filter(\n",
    "    (col('user_id').isNotNull()) & \n",
    "    (col('amount') > 0) & \n",
    "    (col('timestamp').rlike(r'\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}'))\n",
    ")\n",
    "cleaned_df.show()\n",
    "print(f\"Original records: {df.count()}, Cleaned records: {cleaned_df.count()}\")\n",
    "\n",
    "# 2. Data Transformation\n",
    "print(\"\\n2. Data Transformation:\")\n",
    "transformed_df = cleaned_df.withColumn(\n",
    "    'parsed_timestamp', to_timestamp('timestamp')\n",
    ").withColumn(\n",
    "    'date', date_format('parsed_timestamp', 'yyyy-MM-dd')\n",
    ").withColumn(\n",
    "    'hour', hour('parsed_timestamp')\n",
    ").drop('timestamp').withColumnRenamed('parsed_timestamp', 'timestamp')\n",
    "\n",
    "transformed_df.show()\n",
    "\n",
    "# 3. Aggregations\n",
    "print(\"\\n3. Aggregations:\")\n",
    "\n",
    "# Total amount per user per day\n",
    "user_daily_totals = transformed_df.groupBy('user_id', 'date').agg(\n",
    "    sum('amount').alias('daily_total'),\n",
    "    count('transaction_id').alias('transaction_count')\n",
    ")\n",
    "print(\"User Daily Totals:\")\n",
    "user_daily_totals.show()\n",
    "\n",
    "# Average transaction amount per category per hour\n",
    "category_hourly_avg = transformed_df.groupBy('category', 'hour').agg(\n",
    "    avg('amount').alias('avg_amount'),\n",
    "    count('transaction_id').alias('transaction_count')\n",
    ")\n",
    "print(\"Category Hourly Averages:\")\n",
    "category_hourly_avg.show()\n",
    "\n",
    "# 4. Anomaly Detection\n",
    "print(\"\\n4. Anomaly Detection:\")\n",
    "\n",
    "# Calculate daily averages per user\n",
    "user_daily_stats = transformed_df.groupBy('user_id', 'date').agg(\n",
    "    avg('amount').alias('daily_avg'),\n",
    "    stddev('amount').alias('daily_stddev'),\n",
    "    count('transaction_id').alias('daily_count')\n",
    ")\n",
    "\n",
    "# Join back and flag anomalies (3 standard deviations above daily average)\n",
    "anomalies_df = transformed_df.join(\n",
    "    user_daily_stats,\n",
    "    ['user_id', 'date']\n",
    ").withColumn(\n",
    "    'is_anomaly',\n",
    "    when(\n",
    "        (col('amount') > col('daily_avg') + 3 * col('daily_stddev')) & \n",
    "        (col('daily_stddev') > 0),  # Avoid division by zero\n",
    "        True\n",
    "    ).otherwise(False)\n",
    ").select(\n",
    "    'user_id', 'transaction_id', 'amount', 'date', 'hour', 'category',\n",
    "    'daily_avg', 'daily_stddev', 'is_anomaly'\n",
    ")\n",
    "\n",
    "print(\"Anomaly Detection Results:\")\n",
    "anomalies_df.show()\n",
    "\n",
    "print(f\"\\nTotal anomalies detected: {anomalies_df.filter('is_anomaly = true').count()}\")\n",
    "\n",
    "print(\"\\nâœ… Solution completed! Review the results and understand each step.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
