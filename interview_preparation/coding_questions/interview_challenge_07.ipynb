{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interview Challenge 13: Multi-Source Data Pipeline & Cleaning\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "Build a comprehensive ETL pipeline that reads data from multiple sources (CSV, JSON, different formats), performs extensive data cleaning and transformation, and creates a unified clean dataset ready for analytics.\n",
    "\n",
    "## Data Sources\n",
    "\n",
    "**Source 1 - Customer CSV:**\n",
    "- Format: CSV with headers\n",
    "- Issues: Missing values, inconsistent formats, duplicates\n",
    "- Fields: customer_id, name, email, phone, address, signup_date\n",
    "\n",
    "**Source 2 - Orders JSON:**\n",
    "- Format: JSON Lines (one JSON object per line)\n",
    "- Issues: Nested structures, missing fields, invalid dates\n",
    "- Fields: order_id, customer_id, items[], order_date, total_amount, status\n",
    "\n",
    "**Source 3 - Product Database Export (TSV):**\n",
    "- Format: Tab-separated values\n",
    "- Issues: Special characters, encoding problems, malformed rows\n",
    "- Fields: product_id, name, category, price, description, created_date\n",
    "\n",
    "**Source 4 - User Activity Logs (Custom Format):**\n",
    "- Format: Custom delimited log format\n",
    "- Issues: Irregular delimiters, mixed data types, corrupted entries\n",
    "- Fields: timestamp, user_id, action, page, session_id, metadata\n",
    "\n",
    "## Tasks\n",
    "\n",
    "1. **Multi-Source Data Reading**\n",
    "   - Read data from CSV, JSON, TSV, and custom formats\n",
    "   - Handle different schemas and data types\n",
    "   - Implement robust error handling for malformed data\n",
    "   - Use appropriate Spark readers for each format\n",
    "\n",
    "2. **Data Cleaning & Standardization**\n",
    "   - Remove duplicates and handle missing values\n",
    "   - Standardize formats (dates, phone numbers, addresses)\n",
    "   - Validate data integrity and business rules\n",
    "   - Handle encoding issues and special characters\n",
    "\n",
    "3. **Data Transformation**\n",
    "   - Flatten nested JSON structures\n",
    "   - Normalize and denormalize data as needed\n",
    "   - Create derived features and calculated fields\n",
    "   - Implement complex business logic transformations\n",
    "\n",
    "4. **Data Quality Validation**\n",
    "   - Implement comprehensive data quality checks\n",
    "   - Generate data quality reports\n",
    "   - Flag suspicious or outlier data\n",
    "   - Create data profiling summaries\n",
    "\n",
    "5. **Unified Output**\n",
    "   - Merge data from all sources into consistent schema\n",
    "   - Create master tables with proper relationships\n",
    "   - Optimize output format for downstream analytics\n",
    "   - Generate data dictionary and documentation\n",
    "\n",
    "## Technical Requirements\n",
    "- Handle multiple file formats and schemas\n",
    "- Implement robust error handling and recovery\n",
    "- Use appropriate data validation and cleaning techniques\n",
    "- Optimize for performance with large datasets\n",
    "- Create reusable and maintainable pipeline code\n",
    "- Include comprehensive logging and monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ Try It Yourself\n",
    "\n",
    "Build a production-ready ETL pipeline that handles multiple data sources and extensive cleaning. Start by reading each data source, then implement cleaning and transformation logic.\n",
    "\n",
    "**Steps to follow:**\n",
    "1. Set up readers for each data source format\n",
    "2. Implement data cleaning functions for each source\n",
    "3. Create transformation logic to standardize data\n",
    "4. Merge and validate the unified dataset\n",
    "5. Generate quality reports and output optimized results\n",
    "\n",
    "**Tip:** Focus on error handling - real-world data is messy and requires robust processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MultiSourceDataPipeline\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample data (in production, these would be file paths)\n",
    "customers_csv_data = [\n",
    "    \"customer_id,name,email,phone,address,signup_date\",\n",
    "    \"CUST001,John Doe,john@email.com,555-1234,123 Main St,2023-01-15\",\n",
    "    \"CUST002,Jane Smith,,555-5678,456 Oak Ave,2023-02-20\",\n",
    "    \"CUST003,Bob Johnson,bob@email.com,INVALID,789 Pine St,2023-03-10\",\n",
    "    \"CUST001,John Doe,john@email.com,555-1234,123 Main St,2023-01-15\",  # Duplicate\n",
    "    \"CUST004,Alice Brown,alice@email.com,555-9012,,2023-04-05\"  # Missing address\n",
    "]\n",
    "\n",
    "orders_json_data = [\n",
    "    '{\"order_id\": \"ORD001\", \"customer_id\": \"CUST001\", \"items\": [{\"product_id\": \"PROD001\", \"quantity\": 2}], \"order_date\": \"2023-01-15\", \"total_amount\": 299.98, \"status\": \"completed\"}',\n",
    "    '{\"order_id\": \"ORD002\", \"customer_id\": \"CUST002\", \"items\": [{\"product_id\": \"PROD002\", \"quantity\": 1}], \"order_date\": \"2023-01-20\", \"total_amount\": 149.99, \"status\": \"pending\"}',\n",
    "    '{\"order_id\": \"ORD003\", \"customer_id\": \"CUST003\", \"items\": [], \"order_date\": \"2023-01-25\", \"total_amount\": 0, \"status\": \"cancelled\"}',  # Empty items\n",
    "    '{\"order_id\": \"ORD004\", \"customer_id\": \"CUST001\", \"items\": [{\"product_id\": \"PROD003\", \"quantity\": 1}], \"total_amount\": 199.99}'  # Missing fields\n",
    "]\n",
    "\n",
    "products_tsv_data = [\n",
    "    \"product_id\\tname\\tcategory\\tprice\\tdescription\\tcreated_date\",\n",
    "    \"PROD001\\tLaptop Pro\\tElectronics\\t299.99\\tHigh-performance laptop\\t2023-01-01\",\n",
    "    \"PROD002\\tWireless Headphones\\tElectronics\\t149.99\\tNoise-cancelling wireless\\t2023-01-15\",\n",
    "    \"PROD003\\tOffice Chair\\tFurniture\\t199.99\\tErgonomic office chair\\t2023-02-01\",\n",
    "    \"PROD004\\tCoffee Mug\\tKitchen\\t12.99\\tCeramic coffee mug\\t2023-02-15\"\n",
    "]\n",
    "\n",
    "activity_logs_data = [\n",
    "    \"2023-01-15 10:30:45|CUST001|VIEW|home|sess_abc123|{\\\"page_load\\\": 2.1}\",\n",
    "    \"2023-01-15 10:31:12|CUST001|CLICK|products|sess_abc123|{\\\"element\\\": \\\"add_to_cart\\\"}\",\n",
    "    \"2023-01-15 10:32:01|CUST002|LOGIN|login|sess_def456|{\\\"login_method\\\": \\\"email\\\"}\",\n",
    "    \"2023-01-15 10:32:30|CUST001|PURCHASE|checkout|sess_abc123|{\\\"payment\\\": \\\"credit_card\\\"}\",\n",
    "    \"invalid log entry with wrong format\",\n",
    "    \"2023-01-15 10:33:15|CUST003|SEARCH|products|sess_ghi789|{\\\"query\\\": \\\"laptop\\\"}\"\n",
    "]\n",
    "\n",
    "# Helper function to create RDD from sample data\n",
    "def create_sample_rdd(data_list):\n",
    "    \"\"\"Create RDD from sample data list\"\"\"\n",
    "    return spark.sparkContext.parallelize(data_list)\n",
    "\n",
    "print(\"Data sources prepared!\")\n",
    "print(f\"Customers CSV: {len(customers_csv_data)} lines\")\n",
    "print(f\"Orders JSON: {len(orders_json_data)} records\")\n",
    "print(f\"Products TSV: {len(products_tsv_data)} lines\")\n",
    "print(f\"Activity logs: {len(activity_logs_data)} entries\")\n",
    "\n",
    "# === YOUR SOLUTION GOES HERE ===\n",
    "# Implement multi-source ETL pipeline\n",
    "\n",
    "# Task 1: Read data from multiple sources\n",
    "# 1a. Read CSV data with proper options\n",
    "# 1b. Parse JSON data with error handling\n",
    "# 1c. Read TSV data with custom separator\n",
    "# 1d. Parse custom log format\n",
    "\n",
    "# Task 2: Data Cleaning & Standardization\n",
    "# 2a. Clean customer data (duplicates, missing values, format validation)\n",
    "# 2b. Clean orders data (nested JSON, missing fields, validation)\n",
    "# 2c. Clean products data (encoding, special characters)\n",
    "# 2d. Clean activity logs (parsing, validation)\n",
    "\n",
    "# Task 3: Data Transformation\n",
    "# 3a. Flatten JSON structures\n",
    "# 3b. Normalize data formats\n",
    "# 3c. Create derived features\n",
    "# 3d. Implement business logic transformations\n",
    "\n",
    "# Task 4: Data Quality Validation\n",
    "# 4a. Implement quality checks\n",
    "# 4b. Generate quality reports\n",
    "# 4c. Flag data quality issues\n",
    "\n",
    "# Task 5: Create Unified Output\n",
    "# 5a. Merge all sources into master tables\n",
    "# 5b. Create relationships and foreign keys\n",
    "# 5c. Optimize output for analytics\n",
    "\n",
    "print(\"Implement your multi-source ETL pipeline above!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
