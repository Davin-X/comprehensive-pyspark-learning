{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ¯ Interview Coding Challenge 3: Streaming Data Quality Validation\n",
    "\n",
    "**Difficulty:** Hard  \n",
    "**Time:** 35 minutes  \n",
    "**Skills:** Structured Streaming, data validation, error handling, monitoring\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "Implement real-time data quality validation for streaming data with comprehensive monitoring and alerting. Process streaming events and:\n",
    "\n",
    "1. **Validate schema compliance** - Check required fields and data types\n",
    "2. **Check data completeness** - Identify missing or null values\n",
    "3. **Detect statistical anomalies** - Flag unusual patterns using rolling statistics\n",
    "4. **Generate alerts** - Trigger notifications for quality issues\n",
    "5. **Route data appropriately** - Send valid data to production, invalid to quarantine\n",
    "\n",
    "## Input Schema\n",
    "```python\n",
    "event_schema = StructType([\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"event_type\", StringType(), True),\n",
    "    StructField(\"value\", DoubleType(), True),\n",
    "    StructField(\"metadata\", StringType(), True)\n",
    "])\n",
    "```\n",
    "\n",
    "## Requirements\n",
    "- Real-time processing with Structured Streaming\n",
    "- Comprehensive quality checks with configurable thresholds\n",
    "- Alert generation for different severity levels\n",
    "- Data routing based on validation results\n",
    "- Statistical anomaly detection using rolling windows\n",
    "\n",
    "## Evaluation Criteria\n",
    "- Correct streaming implementation\n",
    "- Comprehensive validation logic\n",
    "- Proper error handling and alerting\n",
    "- Efficient data routing and monitoring\n",
    "\n",
    "---\n",
    "\n",
    "**Start coding your solution below!** ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Interview Challenge 3\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample streaming data for testing\n",
    "streaming_data = [\n",
    "    (\"user1\", \"2023-01-01 10:00:00\", \"page_view\", 100.0, \"{}\"),\n",
    "    (\"user2\", \"2023-01-01 10:01:00\", \"click\", 50.0, \"{}\"),\n",
    "    (None, \"2023-01-01 10:02:00\", \"page_view\", 75.0, \"{}\"),  # Missing user_id\n",
    "    (\"user3\", None, \"scroll\", 25.0, \"{}\"),  # Missing timestamp\n",
    "    (\"user4\", \"2023-01-01 10:04:00\", \"purchase\", 500.0, \"{}\"),  # Normal\n",
    "    (\"user5\", \"2023-01-01 10:05:00\", \"page_view\", 9999.0, \"{}\"),  # Anomaly (very high value)\n",
    "    (\"user1\", \"2023-01-01 10:06:00\", \"logout\", 0.0, \"{}\"),  # Normal\n",
    "]\n",
    "\n",
    "# Define expected schema\n",
    "event_schema = StructType([\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"event_type\", StringType(), True),\n",
    "    StructField(\"value\", DoubleType(), True),\n",
    "    StructField(\"metadata\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create streaming DataFrame (simulate streaming with batch data)\n",
    "events_df = spark.createDataFrame(streaming_data, \n",
    "    [\"user_id\", \"timestamp\", \"event_type\", \"value\", \"metadata\"])\n",
    "events_df = events_df.withColumn(\"timestamp\", to_timestamp(\"timestamp\"))\n",
    "\n",
    "print(\"Sample streaming data created:\")\n",
    "events_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Solution\n",
    "\n",
    "**Implement comprehensive streaming data quality validation:**\n",
    "\n",
    "1. Create a `StreamingDataValidator` class with validation methods\n",
    "2. Implement schema validation against expected schema\n",
    "3. Check data completeness for required fields\n",
    "4. Detect statistical anomalies using rolling windows\n",
    "5. Generate alerts based on validation results\n",
    "6. Route data to appropriate destinations (valid/invalid)\n",
    "\n",
    "**Validation Rules:**\n",
    "- Schema: Must match expected structure\n",
    "- Completeness: < 5% nulls in required fields\n",
    "- Anomalies: Values outside 3 standard deviations\n",
    "- Critical alerts: Schema errors or high null rates\n",
    "\n",
    "**Hints:**\n",
    "- Use `foreachBatch` for micro-batch processing\n",
    "- Implement statistical anomaly detection with window functions\n",
    "- Create configurable alert thresholds\n",
    "- Route data using conditional logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR SOLUTION HERE\n",
    "# Implement the streaming data quality validation system\n",
    "\n",
    "class StreamingDataValidator:\n",
    "    def __init__(self, spark_session):\n",
    "        self.spark = spark_session\n",
    "        self.alert_thresholds = {\n",
    "            \"null_rate\": 0.05,      # 5% null threshold\n",
    "            \"schema_error_rate\": 0.001,  # 0.1% schema error threshold\n",
    "            \"anomaly_score\": 3.0    # 3 standard deviations\n",
    "        }\n",
    "    \n",
    "    def validate_schema(self, df, expected_schema):\n",
    "        \"\"\"Validate DataFrame schema against expected schema\"\"\"\n",
    "        # YOUR CODE HERE - Compare actual vs expected schema\n",
    "        # Return validation result with missing/extra/type mismatch fields\n",
    "        pass\n",
    "    \n",
    "    def check_completeness(self, df, required_fields):\n",
    "        \"\"\"Check for null values in required fields\"\"\"\n",
    "        # YOUR CODE HERE - Calculate null rates for each required field\n",
    "        # Return list of completeness check results\n",
    "        pass\n",
    "    \n",
    "    def detect_anomalies(self, df, numeric_field, window_size=10):\n",
    "        \"\"\"Detect statistical anomalies using rolling statistics\"\"\"\n",
    "        # YOUR CODE HERE - Calculate rolling mean/std, z-scores\n",
    "        # Flag values outside threshold standard deviations\n",
    "        pass\n",
    "    \n",
    "    def generate_alerts(self, validation_results):\n",
    "        \"\"\"Generate alerts based on validation results\"\"\"\n",
    "        # YOUR CODE HERE - Create alerts for different severity levels\n",
    "        # Return list of alert dictionaries\n",
    "        pass\n",
    "\n",
    "def process_batch_with_validation(batch_df, batch_id):\n",
    "    \"\"\"Process a streaming batch with comprehensive validation\"\"\"\n",
    "    \n",
    "    validator = StreamingDataValidator(spark)\n",
    "    \n",
    "    # Define expected schema\n",
    "    expected_schema = StructType([\n",
    "        StructField(\"user_id\", StringType(), True),\n",
    "        StructField(\"timestamp\", TimestampType(), True),\n",
    "        StructField(\"event_type\", StringType(), True),\n",
    "        StructField(\"value\", DoubleType(), True),\n",
    "        StructField(\"metadata\", StringType(), True)\n",
    "    ])\n",
    "    \n",
    "    # Perform validations\n",
    "    validation_results = {}\n",
    "    \n",
    "    # 1. Schema validation\n",
    "    validation_results[\"schema\"] = validator.validate_schema(batch_df, expected_schema)\n",
    "    \n",
    "    # 2. Data completeness\n",
    "    validation_results[\"completeness\"] = validator.check_completeness(\n",
    "        batch_df, [\"user_id\", \"timestamp\", \"event_type\"]\n",
    "    )\n",
    "    \n",
    "    # 3. Statistical anomalies (if value column exists)\n",
    "    if \"value\" in batch_df.columns:\n",
    "        validation_results[\"anomalies\"] = validator.detect_anomalies(batch_df, \"value\")\n",
    "    \n",
    "    # Generate alerts\n",
    "    alerts = validator.generate_alerts(validation_results)\n",
    "    \n",
    "    # Route data based on validation results\n",
    "    critical_alerts = [a for a in alerts if a.get(\"severity\") == \"critical\"]\n",
    "    \n",
    "    if critical_alerts:\n",
    "        # Route to quarantine\n",
    "        batch_df.withColumn(\"batch_id\", lit(batch_id)) \\\n",
    "            .withColumn(\"quarantine_reason\", lit(str(critical_alerts))) \\\n",
    "            .write.mode(\"append\").parquet(\"/tmp/quarantine/\")\n",
    "        print(f\"âŒ Batch {batch_id} quarantined due to critical issues\")\n",
    "    else:\n",
    "        # Route to production\n",
    "        batch_df.withColumn(\"batch_id\", lit(batch_id)) \\\n",
    "            .withColumn(\"quality_score\", lit(len(alerts))) \\\n",
    "            .write.mode(\"append\").parquet(\"/tmp/validated/\")\n",
    "        print(f\"âœ… Batch {batch_id} validated and stored\")\n",
    "    \n",
    "    # Log alerts\n",
    "    for alert in alerts:\n",
    "        severity = alert.get(\"severity\", \"info\").upper()\n",
    "        print(f\"[{severity}] {alert.get('message', 'Unknown alert')}\")\n",
    "    \n",
    "    return batch_df\n",
    "\n",
    "# Test your solution with sample data\n",
    "try:\n",
    "    result = process_batch_with_validation(events_df, batch_id=1)\n",
    "    print(\"\\nðŸŽ¯ Validation processing completed!\")\n",
    "    print(f\"Processed {events_df.count()} events\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error in your solution: {e}\")\n",
    "    print(\"Keep working on your implementation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Solution Behavior\n",
    "\n",
    "**Sample Output:**\n",
    "```\n",
    "[WARNING] High null rate in user_id: 14.29%\n",
    "[WARNING] High null rate in timestamp: 14.29%\n",
    "[INFO] Statistical anomalies detected: 1 events\n",
    "âœ… Batch 1 validated and stored\n",
    "```\n",
    "\n",
    "**Validation Results:**\n",
    "- **Schema validation:** âœ… Passed (all fields present)\n",
    "- **Data completeness:** âš ï¸ Warnings for null user_id and timestamp\n",
    "- **Statistical anomalies:** â„¹ï¸ Info alert for high value (9999.0)\n",
    "- **Data routing:** âœ… Valid data sent to production storage\n",
    "\n",
    "---\n",
    "\n",
    "**Outstanding work on this streaming validation challenge!** ðŸŽ‰\n",
    "\n",
    "**Key concepts tested:**\n",
    "- Structured Streaming implementation\n",
    "- Real-time data quality monitoring\n",
    "- Statistical anomaly detection\n",
    "- Alert generation and data routing\n",
    "- Micro-batch processing patterns\n",
    "\n",
    "**Production-ready features implemented:**\n",
    "- Configurable validation thresholds\n",
    "- Multi-severity alert system\n",
    "- Data quarantine for invalid records\n",
    "- Comprehensive monitoring and logging\n",
    "\n",
    "**This challenge demonstrates advanced PySpark streaming capabilities!** ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
