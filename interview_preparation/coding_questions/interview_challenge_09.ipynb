{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interview Challenge 9: RDD Operations & Custom Transformations\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "You need to process a large log file dataset using PySpark RDD operations. The logs contain user activity data that requires complex transformations and aggregations that are better suited for RDD API than DataFrame API.\n",
    "\n",
    "## Dataset Description\n",
    "\n",
    "**Log Format:**\n",
    "```\n",
    "timestamp|user_id|action|page|session_id|ip_address|user_agent|response_time|status_code\n",
    "```\n",
    "\n",
    "Example log line:\n",
    "```\n",
    "2023-01-15 10:30:45|user123|VIEW|home|sess_abc123|192.168.1.100|Mozilla/5.0|250|200\n",
    "```\n",
    "\n",
    "## Tasks\n",
    "\n",
    "1. **RDD Creation & Parsing**\n",
    "   - Read log files into RDD\n",
    "   - Parse and validate log lines\n",
    "   - Handle malformed records\n",
    "   - Convert to structured format\n",
    "\n",
    "2. **Custom Transformations**\n",
    "   - Implement custom `map` functions for data extraction\n",
    "   - Create `filter` functions for data validation\n",
    "   - Build `flatMap` operations for complex parsing\n",
    "   - Develop custom partitioning logic\n",
    "\n",
    "3. **Advanced RDD Operations**\n",
    "   - Use `groupByKey` vs `reduceByKey` appropriately\n",
    "   - Implement custom combiners for efficiency\n",
    "   - Handle data skew with custom partitioning\n",
    "   - Use `aggregateByKey` for complex aggregations\n",
    "\n",
    "4. **Performance Optimization**\n",
    "   - Implement proper caching strategies\n",
    "   - Use appropriate persistence levels\n",
    "   - Optimize shuffle operations\n",
    "   - Minimize data serialization\n",
    "\n",
    "5. **Complex Analytics**\n",
    "   - Calculate user session metrics\n",
    "   - Identify bot traffic patterns\n",
    "   - Detect anomalous response times\n",
    "   - Generate time-based aggregations\n",
    "\n",
    "## Technical Requirements\n",
    "- Use RDD API extensively (not DataFrame)\n",
    "- Implement custom functions and transformations\n",
    "- Handle edge cases and malformed data\n",
    "- Optimize for performance and memory usage\n",
    "- Include proper error handling\n",
    "- Demonstrate understanding of lazy evaluation\n",
    "\n",
    "## Key Concepts to Demonstrate\n",
    "- RDD lineage and DAG optimization\n",
    "- Shuffle operations and their impact\n",
    "- Memory vs disk persistence\n",
    "- Custom serialization\n",
    "- Fault tolerance mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "# Create Spark context\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RDDChallenge\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Sample log data (in production, this would be read from files)\n",
    "sample_logs = [\n",
    "    \"2023-01-15 10:30:45|user123|VIEW|home|sess_abc123|192.168.1.100|Mozilla/5.0|250|200\",\n",
    "    \"2023-01-15 10:31:12|user123|CLICK|products|sess_abc123|192.168.1.100|Mozilla/5.0|180|200\",\n",
    "    \"2023-01-15 10:32:01|user456|PURCHASE|checkout|sess_def456|192.168.1.101|Chrome/90|450|200\",\n",
    "    \"2023-01-15 10:32:30|user123|VIEW|profile|sess_abc123|192.168.1.100|Mozilla/5.0|300|200\",\n",
    "    \"2023-01-15 10:33:15|bot_crawler|VIEW|home|sess_bot001|10.0.0.1|Bot/1.0|50|200\",\n",
    "    \"2023-01-15 10:34:22|user789|SEARCH|products|sess_ghi789|192.168.1.102|Safari/14|320|200\",\n",
    "    \"invalid log line without proper format\",\n",
    "    \"2023-01-15 10:35:01|user123|LOGOUT|home|sess_abc123|192.168.1.100|Mozilla/5.0|120|200\",\n",
    "    \"2023-01-15 10:35:45|user456|VIEW|orders|sess_def456|192.168.1.101|Chrome/90|280|500\",  # Error response\n",
    "    \"2023-01-15 10:36:12|user999|LOGIN|login|sess_jkl999|192.168.1.103|Firefox/88|800|200\"  # Slow response\n",
    "]\n",
    "\n",
    "# Create RDD from sample data\n",
    "logs_rdd = sc.parallelize(sample_logs)\n",
    "\n",
    "print(f\"Created RDD with {logs_rdd.count()} log entries\")\n",
    "\n",
    "# === YOUR SOLUTION GOES HERE ===\n",
    "# Implement RDD-based log processing\n",
    "\n",
    "# Task 1: Parse and validate log entries\n",
    "def parse_log_line(line):\n",
    "    \"\"\"Parse a single log line into structured format\"\"\"\n",
    "    try:\n",
    "        # Expected format: timestamp|user_id|action|page|session_id|ip|user_agent|response_time|status_code\n",
    "        parts = line.split('|')\n",
    "        if len(parts) != 9:\n",
    "            return None  # Malformed line\n",
    "        \n",
    "        timestamp, user_id, action, page, session_id, ip, user_agent, resp_time, status = parts\n",
    "        \n",
    "        # Basic validation\n",
    "        if not timestamp or not user_id or not action:\n",
    "            return None\n",
    "        \n",
    "        return {\n",
    "            'timestamp': timestamp,\n",
    "            'user_id': user_id,\n",
    "            'action': action,\n",
    "            'page': page,\n",
    "            'session_id': session_id,\n",
    "            'ip_address': ip,\n",
    "            'user_agent': user_agent,\n",
    "            'response_time': int(resp_time),\n",
    "            'status_code': int(status)\n",
    "        }\n",
    "    except (ValueError, IndexError):\n",
    "        return None\n",
    "\n",
    "# Parse logs\n",
    "parsed_logs = logs_rdd.map(parse_log_line).filter(lambda x: x is not None)\n",
    "print(f\"Successfully parsed {parsed_logs.count()} valid log entries\")\n",
    "\n",
    "# Task 2: Custom transformations\n",
    "# Extract user actions\n",
    "user_actions = parsed_logs.map(lambda log: (log['user_id'], log['action']))\n",
    "\n",
    "# Filter successful requests\n",
    "successful_requests = parsed_logs.filter(lambda log: log['status_code'] < 400)\n",
    "\n",
    "# Extract session information\n",
    "session_info = parsed_logs.map(lambda log: (log['session_id'], {\n",
    "    'user_id': log['user_id'],\n",
    "    'start_time': log['timestamp'],\n",
    "    'page_count': 1\n",
    "}))\n",
    "\n",
    "# Task 3: Advanced RDD operations\n",
    "# Count actions per user\n",
    "user_action_counts = user_actions \\\n",
    "    .groupByKey() \\\n",
    "    .mapValues(lambda actions: list(actions)) \\\n",
    "    .mapValues(lambda actions: {action: actions.count(action) for action in set(actions)})\n",
    "\n",
    "# Calculate average response time per page\n",
    "page_response_times = successful_requests \\\n",
    "    .map(lambda log: (log['page'], log['response_time'])) \\\n",
    "    .aggregateByKey(\n",
    "        (0, 0),  # (sum, count)\n",
    "        lambda acc, value: (acc[0] + value, acc[1] + 1),\n",
    "        lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1])\n",
    "    ) \\\n",
    "    .mapValues(lambda sums: sums[0] / sums[1] if sums[1] > 0 else 0)\n",
    "\n",
    "# Task 4: Session analysis\n",
    "session_metrics = parsed_logs \\\n",
    "    .map(lambda log: (log['session_id'], (log['timestamp'], log['action'], log['page']))) \\\n",
    "    .groupByKey() \\\n",
    "    .mapValues(list) \\\n",
    "    .mapValues(lambda events: {\n",
    "        'user_id': events[0][0] if events else None,  # From first event\n",
    "        'duration_minutes': (datetime.strptime(max(e[0] for e in events), '%Y-%m-%d %H:%M:%S') - \n",
    "                           datetime.strptime(min(e[0] for e in events), '%Y-%m-%d %H:%M:%S')).total_seconds() / 60,\n",
    "        'page_views': len([e for e in events if e[1] == 'VIEW']),\n",
    "        'actions': len(events),\n",
    "        'converted': any(e[1] == 'PURCHASE' for e in events)\n",
    "    })\n",
    "\n",
    "# Task 5: Anomaly detection\n",
    "response_times = successful_requests.map(lambda log: log['response_time'])\n",
    "response_stats = response_times.stats()\n",
    "\n",
    "# Flag slow requests (above 95th percentile)\n",
    "percentile_95 = response_times.sortBy(lambda x: x).zipWithIndex() \\\n",
    "    .filter(lambda x: x[1] >= int(response_times.count() * 0.95)) \\\n",
    "    .map(lambda x: x[0]).first()\n",
    "\n",
    "slow_requests = successful_requests.filter(lambda log: log['response_time'] > percentile_95)\n",
    "\n",
    "# Task 6: Bot detection\n",
    "bot_indicators = ['bot', 'crawler', 'spider', 'scraper']\n",
    "bot_traffic = parsed_logs.filter(\n",
    "    lambda log: any(indicator.lower() in log['user_agent'].lower() for indicator in bot_indicators)\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"\\n=== RESULTS ===\")\n",
    "print(f\"\\nTotal valid logs: {parsed_logs.count()}\")\n",
    "print(f\"Successful requests: {successful_requests.count()}\")\n",
    "print(f\"Bot traffic detected: {bot_traffic.count()}\")\n",
    "print(f\"Slow requests: {slow_requests.count()}\")\n",
    "\n",
    "print(\"\\nTop 5 pages by average response time:\")\n",
    "for page, avg_time in page_response_times.sortBy(lambda x: x[1], ascending=False).take(5):\n",
    "    print(f\"{page}: {avg_time:.1f}ms\")\n",
    "\n",
    "print(\"\\nUser action summary:\")\n",
    "for user, actions in user_action_counts.take(5):\n",
    "    print(f\"{user}: {actions}\")\n",
    "\n",
    "# Cache for performance\n",
    "parsed_logs.cache()\n",
    "print(f\"\\nCached {parsed_logs.count()} parsed log entries\")\n",
    "\n",
    "print(\"\\nâœ… RDD processing completed!\")\n",
    "print(\"Key Learnings:\")\n",
    "print(\"- RDD transformations are lazy until actions are called\")\n",
    "print(\"- groupByKey can cause shuffle, prefer reduceByKey when possible\")\n",
    "print(\"- aggregateByKey provides better control over combiner functions\")\n",
    "print(\"- Caching prevents recomputation of expensive operations\")\n",
    "print(\"- Custom functions enable complex business logic\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
