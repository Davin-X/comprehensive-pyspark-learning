{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interview Challenge 7: Performance Optimization & Troubleshooting\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "You have been given a slow PySpark job that processes customer order data. The job is taking 8+ hours to complete on a 10TB dataset, and you need to optimize it to run in under 2 hours while maintaining data accuracy.\n",
    "\n",
    "## Current Job Description\n",
    "\n",
    "The job performs the following operations:\n",
    "1. Reads customer and order data from CSV files\n",
    "2. Joins customer data with order data\n",
    "3. Calculates customer lifetime value (LTV)\n",
    "4. Applies complex business rules and filters\n",
    "5. Aggregates data by multiple dimensions\n",
    "6. Writes results to Parquet format\n",
    "\n",
    "## Data Schema\n",
    "\n",
    "**Customers Table:**\n",
    "- `customer_id` (string)\n",
    "- `registration_date` (string: yyyy-MM-dd)\n",
    "- `country` (string)\n",
    "- `segment` (string)\n",
    "\n",
    "**Orders Table:**\n",
    "- `order_id` (string)\n",
    "- `customer_id` (string)\n",
    "- `order_date` (string: yyyy-MM-dd)\n",
    "- `amount` (double)\n",
    "- `product_category` (string)\n",
    "- `status` (string)\n",
    "\n",
    "## Performance Issues\n",
    "\n",
    "1. **Data Skew**: Some customers have thousands of orders, others have few\n",
    "2. **Inefficient Joins**: Multiple large table joins without optimization\n",
    "3. **Memory Issues**: OutOfMemory errors during aggregation\n",
    "4. **Serialization Overhead**: Large objects being shuffled\n",
    "5. **Suboptimal File Formats**: Reading from CSV, writing to CSV\n",
    "\n",
    "## Tasks\n",
    "\n",
    "1. **Performance Analysis**\n",
    "   - Identify bottlenecks using Spark UI metrics\n",
    "   - Analyze DAG and execution plans\n",
    "   - Check for data skew and partition imbalances\n",
    "\n",
    "2. **Optimization Strategies**\n",
    "   - Optimize data reading (file formats, partitioning)\n",
    "   - Implement efficient join strategies\n",
    "   - Handle data skew (salting, broadcasting)\n",
    "   - Optimize aggregations and shuffles\n",
    "\n",
    "3. **Resource Tuning**\n",
    "   - Configure optimal executor memory and cores\n",
    "   - Set appropriate parallelism\n",
    "   - Enable dynamic allocation\n",
    "   - Tune garbage collection\n",
    "\n",
    "4. **Code Optimization**\n",
    "   - Use DataFrame API over RDD where possible\n",
    "   - Minimize shuffles and data movement\n",
    "   - Implement caching strategically\n",
    "   - Use broadcast joins for small tables\n",
    "\n",
    "5. **Monitoring & Alerting**\n",
    "   - Implement job metrics collection\n",
    "   - Set up performance monitoring\n",
    "   - Add error handling and retry logic\n",
    "\n",
    "## Technical Requirements\n",
    "- Achieve 4x performance improvement (8 hours â†’ 2 hours)\n",
    "- Maintain data accuracy and completeness\n",
    "- Handle production-scale data (10TB+)\n",
    "- Implement fault tolerance\n",
    "- Document optimization decisions and trade-offs\n",
    "\n",
    "## Success Criteria\n",
    "- Job completion time < 2 hours\n",
    "- No OutOfMemory errors\n",
    "- Balanced resource utilization\n",
    "- Clear performance metrics and monitoring\n",
    "- Production-ready code with proper error handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Window\n",
    "import time\n",
    "\n",
    "# Create optimized Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"OptimizedCustomerAnalytics\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
    "    .config(\"spark.dynamicAllocation.minExecutors\", \"10\") \\\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", \"100\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.memory.fraction\", \"0.8\") \\\n",
    "    .config(\"spark.memory.storageFraction\", \"0.3\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample data (in production, read from S3/ADLS)\n",
    "customers_data = [\n",
    "    ('CUST001', '2020-01-15', 'US', 'Premium'),\n",
    "    ('CUST002', '2020-03-20', 'UK', 'Standard'),\n",
    "    ('CUST003', '2020-02-10', 'US', 'Premium'),\n",
    "    ('CUST004', '2020-04-05', 'DE', 'Standard'),\n",
    "    ('CUST005', '2020-01-30', 'US', 'Basic')\n",
    "]\n",
    "\n",
    "orders_data = [\n",
    "    ('ORD001', 'CUST001', '2023-01-15', 150.0, 'Electronics', 'Completed'),\n",
    "    ('ORD002', 'CUST001', '2023-02-20', 200.0, 'Books', 'Completed'),\n",
    "    ('ORD003', 'CUST002', '2023-01-10', 75.0, 'Clothing', 'Completed'),\n",
    "    ('ORD004', 'CUST001', '2023-03-05', 300.0, 'Electronics', 'Pending'),\n",
    "    ('ORD005', 'CUST003', '2023-02-15', 120.0, 'Home', 'Completed'),\n",
    "    ('ORD006', 'CUST001', '2023-04-01', 90.0, 'Books', 'Completed'),\n",
    "    ('ORD007', 'CUST004', '2023-03-20', 250.0, 'Electronics', 'Completed'),\n",
    "    ('ORD008', 'CUST005', '2023-04-10', 60.0, 'Clothing', 'Cancelled')\n",
    "]\n",
    "\n",
    "# Define schemas\n",
    "customers_schema = StructType([\n",
    "    StructField('customer_id', StringType(), True),\n",
    "    StructField('registration_date', StringType(), True),\n",
    "    StructField('country', StringType(), True),\n",
    "    StructField('segment', StringType(), True)\n",
    "])\n",
    "\n",
    "orders_schema = StructType([\n",
    "    StructField('order_id', StringType(), True),\n",
    "    StructField('customer_id', StringType(), True),\n",
    "    StructField('order_date', StringType(), True),\n",
    "    StructField('amount', DoubleType(), True),\n",
    "    StructField('product_category', StringType(), True),\n",
    "    StructField('status', StringType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrames\n",
    "customers_df = spark.createDataFrame(customers_data, customers_schema)\n",
    "orders_df = spark.createDataFrame(orders_data, orders_schema)\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"ðŸš€ Starting optimized customer analytics job...\")\n",
    "\n",
    "# Your optimization solution here\n",
    "\n",
    "# 1. Optimize Data Reading\n",
    "# In production, read from optimized formats:\n",
    "# customers_df = spark.read.parquet(\"s3://data-lake/customers/\")\n",
    "# orders_df = spark.read.parquet(\"s3://data-lake/orders/\")\n",
    "\n",
    "# Add partitioning hints for large datasets\n",
    "customers_df = customers_df.repartition(\"country\")\n",
    "orders_df = orders_df.repartition(\"customer_id\")\n",
    "\n",
    "# 2. Data Preprocessing with Caching\n",
    "print(\"ðŸ“Š Preprocessing data...\")\n",
    "\n",
    "# Filter only completed orders\n",
    "completed_orders = orders_df.filter(\"status = 'Completed'\")\n",
    "\n",
    "# Parse dates\n",
    "customers_processed = customers_df \\\n",
    "    .withColumn(\"registration_date\", to_date(\"registration_date\"))\n",
    "\n",
    "orders_processed = completed_orders \\\n",
    "    .withColumn(\"order_date\", to_date(\"order_date\"))\n",
    "\n",
    "# Cache processed data (memory + disk)\n",
    "customers_processed.cache()\n",
    "orders_processed.cache()\n",
    "\n",
    "# Force cache materialization\n",
    "customers_processed.count()\n",
    "orders_processed.count()\n",
    "\n",
    "# 3. Optimized Join Strategy\n",
    "print(\"ðŸ”— Performing optimized join...\")\n",
    "\n",
    "# Analyze data sizes for join strategy\n",
    "customer_count = customers_processed.count()\n",
    "order_count = orders_processed.count()\n",
    "\n",
    "print(f\"Customers: {customer_count}, Orders: {order_count}\")\n",
    "\n",
    "# Use broadcast join for small customer table\n",
    "if customer_count < 100000:  # Adjust threshold based on available memory\n",
    "    joined_df = orders_processed.join(\n",
    "        broadcast(customers_processed),\n",
    "        \"customer_id\",\n",
    "        \"left\"\n",
    "    )\n",
    "else:\n",
    "    # Use sort-merge join with salting for skew\n",
    "    joined_df = orders_processed \\\n",
    "        .withColumn(\"salt\", (hash(\"customer_id\") % 10).cast(\"string\")) \\\n",
    "        .join(\n",
    "            customers_processed.withColumn(\"salt\", (hash(\"customer_id\") % 10).cast(\"string\")),\n",
    "            [\"customer_id\", \"salt\"],\n",
    "            \"left\"\n",
    "        ) \\\n",
    "        .drop(\"salt\")\n",
    "\n",
    "# 4. Business Logic with Window Functions\n",
    "print(\"ðŸ§® Calculating customer metrics...\")\n",
    "\n",
    "# Calculate customer lifetime metrics\n",
    "customer_metrics = joined_df.groupBy(\"customer_id\").agg(\n",
    "    count(\"order_id\").alias(\"total_orders\"),\n",
    "    sum(\"amount\").alias(\"total_amount\"),\n",
    "    avg(\"amount\").alias(\"avg_order_value\"),\n",
    "    max(\"order_date\").alias(\"last_order_date\"),\n",
    "    min(\"order_date\").alias(\"first_order_date\")\n",
    ")\n",
    "\n",
    "# Calculate customer tenure and lifetime value\n",
    "customer_ltv = customer_metrics \\\n",
    "    .withColumn(\"customer_tenure_days\", \n",
    "                datediff(\"last_order_date\", \"first_order_date\")) \\\n",
    "    .withColumn(\"ltv_score\", \n",
    "                col(\"total_amount\") * (1 + log(\"total_orders\"))) \\\n",
    "    .withColumn(\"avg_order_frequency\", \n",
    "                col(\"customer_tenure_days\") / col(\"total_orders\"))\n",
    "\n",
    "# 5. Multi-dimensional Aggregations\n",
    "print(\"ðŸ“ˆ Generating business insights...\")\n",
    "\n",
    "# Country-level aggregations\n",
    "country_insights = joined_df.groupBy(\"country\").agg(\n",
    "    countDistinct(\"customer_id\").alias(\"unique_customers\"),\n",
    "    sum(\"amount\").alias(\"total_revenue\"),\n",
    "    avg(\"amount\").alias(\"avg_order_value\"),\n",
    "    count(\"order_id\").alias(\"total_orders\")\n",
    ")\n",
    "\n",
    "# Category performance by segment\n",
    "category_segment = joined_df.groupBy(\"segment\", \"product_category\").agg(\n",
    "    sum(\"amount\").alias(\"category_revenue\"),\n",
    "    count(\"order_id\").alias(\"category_orders\"),\n",
    "    countDistinct(\"customer_id\").alias(\"unique_buyers\")\n",
    ")\n",
    "\n",
    "# 6. Advanced Analytics with Window Functions\n",
    "print(\"ðŸ” Performing advanced analytics...\")\n",
    "\n",
    "# Rank customers by LTV within each country\n",
    "window_spec = Window.partitionBy(\"country\").orderBy(desc(\"total_amount\"))\n",
    "\n",
    "top_customers = customer_ltv \\\n",
    "    .join(joined_df.select(\"customer_id\", \"country\").distinct(), \"customer_id\") \\\n",
    "    .withColumn(\"country_rank\", rank().over(window_spec)) \\\n",
    "    .filter(\"country_rank <= 10\")\n",
    "\n",
    "# 7. Optimized Write Operations\n",
    "print(\"ðŸ’¾ Writing optimized results...\")\n",
    "\n",
    "# Repartition for optimal file sizes (128MB target)\n",
    "optimal_partitions = max(1, int(country_insights.count() / 1000))\n",
    "country_insights = country_insights.repartition(optimal_partitions)\n",
    "\n",
    "# Write to Parquet with compression\n",
    "country_insights.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"country\") \\\n",
    "    .parquet(\"/tmp/country_insights\")\n",
    "\n",
    "category_segment.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"segment\") \\\n",
    "    .parquet(\"/tmp/category_segment\")\n",
    "\n",
    "customer_ltv.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(\"/tmp/customer_ltv\")\n",
    "\n",
    "top_customers.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(\"/tmp/top_customers\")\n",
    "\n",
    "# 8. Performance Monitoring\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "print(f\"âœ… Job completed in {execution_time:.2f} seconds\")\n",
    "print(f\"ðŸ“Š Processed {customer_count} customers and {order_count} orders\")\n",
    "\n",
    "# Performance metrics\n",
    "print(\"\\nðŸ“ˆ Performance Metrics:\")\n",
    "print(f\"- Execution time: {execution_time:.2f}s\")\n",
    "print(f\"- Data processed: {order_count} orders\")\n",
    "print(f\"- Throughput: {order_count/execution_time:.0f} orders/second\")\n",
    "\n",
    "# Show sample results\n",
    "print(\"\\nðŸŽ¯ Sample Results:\")\n",
    "country_insights.show(5)\n",
    "customer_ltv.orderBy(desc(\"total_amount\")).show(5)\n",
    "\n",
    "# Cleanup\n",
    "spark.catalog.clearCache()\n",
    "print(\"\\nðŸ§¹ Cache cleared, job finished!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
