{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83d\udcdd Word Count Algorithm\n\n",
    "Classic MapReduce word count implementation in PySpark - the \"Hello World\" of big data processing.\n\n",
    "## \ud83c\udfaf Overview\n\n",
    "Word count is the quintessential big data algorithm that demonstrates:\n\n",
    "- \u2705 **Distributed processing** across multiple nodes\n",
    "- \u2705 **MapReduce paradigm** - Map \u2192 Shuffle \u2192 Reduce\n",
    "- \u2705 **Key-value pair transformations**\n",
    "- \u2705 **Scalable aggregation** patterns\n\n",
    "**Why Word Count Matters:** It forms the foundation for text analytics, search indexing, and many NLP applications.\n\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \u2699\ufe0f PySpark Setup\n\n",
    "Initialize Spark for word count operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"WordCount_Algorithm\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(\"Ready for word count operations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcdd Prepare Input Data\n\n",
    "Create sample text data for word count demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample text data\n",
    "sample_text = [\n",
    "    \"crazy crazy fox jumped\",\n",
    "    \"crazy fox jumped\",\n",
    "    \"fox is fast\",\n",
    "    \"fox is smart\",\n",
    "    \"dog is smart\",\n",
    "    \"big data is powerful\",\n",
    "    \"spark processes big data\",\n",
    "    \"machine learning with spark\"\n",
    "]\n",
    "\n",
    "# Write to file\n",
    "with open(\"wordcount_data.txt\", \"w\") as f:\n",
    "    for line in sample_text:\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "print(\"Sample data created:\")\n",
    "with open(\"wordcount_data.txt\", \"r\") as f:\n",
    "    content = f.read()\n",
    "    print(content)\n",
    "    print(f\"\\nTotal lines: {len(content.splitlines())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udd04 Classic RDD Word Count Implementation\n\n",
    "The traditional MapReduce word count using RDD transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load text file as RDD\n",
    "text_rdd = sc.textFile(\"wordcount_data.txt\")\n",
    "\n",
    "print(f\"Loaded {text_rdd.count()} lines from file\")\n",
    "print(\"Sample lines:\")\n",
    "for line in text_rdd.take(3):\n",
    "    print(f\"  '{line}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Split lines into words (FLATMAP)\n",
    "words_rdd = text_rdd.flatMap(lambda line: line.split())\n",
    "print(f\"After flatMap (splitting): {words_rdd.count()} words\")\n",
    "print(\"Sample words:\")\n",
    "for word in words_rdd.take(10):\n",
    "    print(f\"  '{word}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create key-value pairs (MAP)\n",
    "word_pairs_rdd = words_rdd.map(lambda word: (word, 1))\n",
    "print(\"\\nAfter map (key-value pairs):\")\n",
    "for pair in word_pairs_rdd.take(10):\n",
    "    print(f\"  {pair}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Aggregate counts (REDUCE BY KEY)\n",
    "word_counts_rdd = word_pairs_rdd.reduceByKey(lambda a, b: a + b)\n",
    "print(\"\\nAfter reduceByKey (aggregated counts):\")\n",
    "for pair in word_counts_rdd.take(10):\n",
    "    print(f\"  {pair}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Collect and display final results\n",
    "final_results = word_counts_rdd.collect()\n",
    "\n",
    "print(\"\\n\ud83c\udfaf FINAL WORD COUNT RESULTS (RDD Approach):\")\n",
    "print(\"=\" * 50)\n",
    "for word, count in sorted(final_results):\n",
    "    print(f\"{word:15}: {count}\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total unique words: {len(final_results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcca DataFrame API Word Count\n\n",
    "Modern approach using PySpark DataFrame API for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, explode, col\n",
    "\n",
    "# Read as DataFrame\n",
    "df = spark.read.text(\"wordcount_data.txt\")\n",
    "\n",
    "print(\"DataFrame loaded:\")\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame word count pipeline\n",
    "word_counts_df = df \\\n",
    "    .select(split(col(\"value\"), \" \").alias(\"words\")) \\\n",
    "    .select(explode(col(\"words\")).alias(\"word\")) \\\n",
    "    .filter(col(\"word\") != \"\") \\\n",
    "    .groupBy(\"word\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc(), col(\"word\"))\n",
    "\n",
    "print(\"\\n\ud83c\udfaf FINAL WORD COUNT RESULTS (DataFrame Approach):\")\n",
    "print(\"=\" * 50)\n",
    "word_counts_df.show(20, truncate=False)\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \u26a1 Performance Comparison\n\n",
    "Compare RDD vs DataFrame approaches for word count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Create larger dataset for meaningful comparison\n",
    "large_text = sample_text * 100  # Repeat 100 times\n",
    "with open(\"large_wordcount_data.txt\", \"w\") as f:\n",
    "    for line in large_text:\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "print(f\"Created larger dataset: {len(large_text)} lines\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: RDD Approach\n",
    "print(\"=== RDD Word Count ===\")\n",
    "start_time = time.time()\n",
    "\n",
    "rdd_result = sc.textFile(\"large_wordcount_data.txt\") \\\n",
    "    .flatMap(lambda line: line.split()) \\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .count()  # Just count, don't collect all\n",
    "\n",
    "rdd_time = time.time() - start_time\n",
    "print(f\"RDD approach time: {rdd_time:.3f} seconds\")\n",
    "print(f\"Unique words found: {rdd_result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: DataFrame Approach\n",
    "print(\"\\n=== DataFrame Word Count ===\")\n",
    "start_time = time.time()\n",
    "\n",
    "df_result = spark.read.text(\"large_wordcount_data.txt\") \\\n",
    "    .select(split(col(\"value\"), \" \").alias(\"words\")) \\\n",
    "    .select(explode(col(\"words\")).alias(\"word\")) \\\n",
    "    .filter(col(\"word\") != \"\") \\\n",
    "    .groupBy(\"word\") \\\n",
    "    .count()\n",
    "\n",
    "df_count = df_result.count()\n",
    "df_time = time.time() - start_time\n",
    "print(f\"DataFrame approach time: {df_time:.3f} seconds\")\n",
    "print(f\"Unique words found: {df_count}\")\n",
    "\n",
    "print(f\"\\nPerformance comparison:\")\n",
    "if rdd_time < df_time:\n",
    "    print(f\"RDD was {df_time/rdd_time:.2f}x faster\")\n",
    "else:\n",
    "    print(f\"DataFrame was {rdd_time/df_time:.2f}x faster\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udd27 Advanced Word Count Techniques\n\n",
    "Handle edge cases and improve word count quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced word count with text cleaning\n",
    "import re\n",
    "\n",
    "def clean_word(word):\n",
    "    \"\"\"Clean and normalize words\"\"\"\n",
    "    # Convert to lowercase\n",
    "    word = word.lower()\n",
    "    # Remove punctuation\n",
    "    word = re.sub(r'[^a-zA-Z]', '', word)\n",
    "    return word\n",
    "\n",
    "# Advanced word count with cleaning\n",
    "cleaned_word_counts = sc.textFile(\"wordcount_data.txt\") \\\n",
    "    .flatMap(lambda line: line.split()) \\\n",
    "    .map(lambda word: (clean_word(word), 1)) \\\n",
    "    .filter(lambda pair: len(pair[0]) > 0) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .sortBy(lambda pair: pair[1], ascending=False)\n",
    "\n",
    "print(\"\\n\ud83e\uddf9 CLEANED WORD COUNT RESULTS:\")\n",
    "print(\"=\" * 50)\n",
    "for word, count in cleaned_word_counts.collect():\n",
    "    print(f\"{word:15}: {count}\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Applied: lowercase conversion, punctuation removal, empty word filtering, sorting by frequency\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop words filtering\n",
    "stop_words = {\"is\", \"the\", \"a\", \"an\", \"and\", \"or\", \"but\", \"in\", \"on\", \"at\", \"to\", \"for\", \"of\", \"with\", \"by\"}\n",
    "\n",
    "filtered_word_counts = sc.textFile(\"wordcount_data.txt\") \\\n",
    "    .flatMap(lambda line: line.split()) \\\n",
    "    .map(lambda word: clean_word(word)) \\\n",
    "    .filter(lambda word: len(word) > 0 and word not in stop_words) \\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .sortBy(lambda pair: pair[1], ascending=False)\n",
    "\n",
    "print(\"\\n\ud83d\udeab FILTERED WORD COUNT (No Stop Words):\")\n",
    "print(\"=\" * 50)\n",
    "for word, count in filtered_word_counts.take(10):  # Top 10\n",
    "    print(f\"{word:15}: {count}\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Removed {len(stop_words)} stop words from analysis\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udfaf Interview Questions & Key Takeaways\n\n",
    "### Common Interview Questions:\n",
    "1. **Explain the word count algorithm in MapReduce terms**\n",
    "2. **What's the difference between RDD and DataFrame word count?**\n",
    "3. **How would you handle very large text files?**\n",
    "4. **What are the performance implications of `collect()`?**\n\n",
    "### Key Takeaways:\n",
    "- \u2705 **Word count demonstrates core MapReduce principles**\n",
    "- \u2705 **flatMap()** splits lines into words (1:N transformation)\n",
    "- \u2705 **map()** creates key-value pairs (1:1 transformation)\n",
    "- \u2705 **reduceByKey()** aggregates by key (N:1 transformation)\n",
    "- \u2705 **DataFrames provide higher-level APIs** with optimization\n",
    "- \u2705 **Always consider data size** before using `collect()`\n",
    "- \u2705 **Text preprocessing** (cleaning, stop words) improves quality\n\n",
    "### Real-World Applications:\n",
    "- **Search engine indexing**\n",
    "- **Document classification**\n",
    "- **Sentiment analysis**\n",
    "- **Topic modeling**\n",
    "- **Spam detection**\n\n",
    "---\n\n",
    "**\ud83d\ude80 Word count is just the beginning! These patterns apply to countless big data analytics problems.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}