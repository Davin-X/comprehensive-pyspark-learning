{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interview Challenge 6: ML Pipeline for Customer Churn Prediction\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "Build an end-to-end machine learning pipeline to predict customer churn using PySpark MLlib. The pipeline should handle feature engineering, model training, evaluation, and deployment considerations.\n",
    "\n",
    "## Dataset Description\n",
    "\n",
    "**Customer Data Schema:**\n",
    "- `customer_id` (string) - Unique customer identifier\n",
    "- `age` (integer) - Customer age\n",
    "- `gender` (string) - 'M' or 'F'\n",
    "- `tenure_months` (integer) - How long customer has been with company\n",
    "- `monthly_charges` (double) - Monthly billing amount\n",
    "- `total_charges` (double) - Total amount paid so far\n",
    "- `contract_type` (string) - 'Month-to-month', 'One year', 'Two year'\n",
    "- `internet_service` (string) - 'DSL', 'Fiber optic', 'No'\n",
    "- `payment_method` (string) - Payment type\n",
    "- `churn` (integer) - Target: 1 if churned, 0 if not\n",
    "\n",
    "## Tasks\n",
    "\n",
    "1. **Data Exploration & Cleaning**\n",
    "   - Handle missing values appropriately\n",
    "   - Remove or impute invalid data\n",
    "   - Check for data imbalances\n",
    "\n",
    "2. **Feature Engineering**\n",
    "   - Encode categorical variables\n",
    "   - Create derived features (e.g., charges per tenure)\n",
    "   - Handle numerical scaling\n",
    "   - Create interaction features\n",
    "\n",
    "3. **Model Development**\n",
    "   - Split data into train/validation/test sets\n",
    "   - Build pipeline with preprocessing + model\n",
    "   - Try multiple algorithms (Logistic Regression, Random Forest, GBT)\n",
    "   - Use cross-validation for hyperparameter tuning\n",
    "\n",
    "4. **Model Evaluation**\n",
    "   - Evaluate using appropriate metrics for imbalanced data\n",
    "   - Plot ROC curves and confusion matrices\n",
    "   - Compare model performance\n",
    "   - Analyze feature importance\n",
    "\n",
    "5. **Production Considerations**\n",
    "   - Handle model serialization\n",
    "   - Implement prediction pipeline\n",
    "   - Consider model monitoring and retraining\n",
    "\n",
    "## Technical Requirements\n",
    "- Use PySpark MLlib Pipeline API\n",
    "- Implement proper train/val/test splits\n",
    "- Handle class imbalance\n",
    "- Use appropriate evaluation metrics\n",
    "- Include feature selection/engineering\n",
    "- Document your approach and reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ Try It Yourself\n",
    "\n",
    "Build an end-to-end ML pipeline for churn prediction. Start with data exploration, then implement feature engineering, model training, and evaluation.\n",
    "\n",
    "**Steps to follow:**\n",
    "1. Explore the data and check for imbalances\n",
    "2. Create feature engineering pipeline (indexers, encoders, feature assembly)\n",
    "3. Split data and train multiple models\n",
    "4. Evaluate models and perform hyperparameter tuning\n",
    "5. Analyze results and save the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml.evaluation import *\n",
    "from pyspark.ml.tuning import *\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ChurnPrediction\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample dataset\n",
    "data = [\n",
    "    ('CUST001', 34, 'F', 12, 65.5, 786.0, 'Month-to-month', 'Fiber optic', 'Electronic check', 0),\n",
    "    ('CUST002', 45, 'M', 24, 85.0, 2040.0, 'One year', 'DSL', 'Credit card', 0),\n",
    "    ('CUST003', 29, 'F', 3, 55.0, 165.0, 'Month-to-month', 'DSL', 'Bank transfer', 1),\n",
    "    ('CUST004', 52, 'M', 48, 95.0, 4560.0, 'Two year', 'Fiber optic', 'Credit card', 0),\n",
    "    ('CUST005', 38, 'F', 6, 75.0, 450.0, 'Month-to-month', 'No', 'Electronic check', 1),\n",
    "    ('CUST006', 41, 'M', 18, 80.0, 1440.0, 'One year', 'Fiber optic', 'Bank transfer', 0),\n",
    "    ('CUST007', 27, 'F', 1, 45.0, 45.0, 'Month-to-month', 'DSL', 'Electronic check', 1),\n",
    "    ('CUST008', 63, 'M', 60, 110.0, 6600.0, 'Two year', 'Fiber optic', 'Credit card', 0),\n",
    "    ('CUST009', 35, 'F', 9, 70.0, 630.0, 'Month-to-month', 'Fiber optic', 'Electronic check', 0),\n",
    "    ('CUST010', 48, 'M', 36, 90.0, 3240.0, 'Two year', 'DSL', 'Bank transfer', 1)\n",
    "]\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField('customer_id', StringType(), True),\n",
    "    StructField('age', IntegerType(), True),\n",
    "    StructField('gender', StringType(), True),\n",
    "    StructField('tenure_months', IntegerType(), True),\n",
    "    StructField('monthly_charges', DoubleType(), True),\n",
    "    StructField('total_charges', DoubleType(), True),\n",
    "    StructField('contract_type', StringType(), True),\n",
    "    StructField('internet_service', StringType(), True),\n",
    "    StructField('payment_method', StringType(), True),\n",
    "    StructField('churn', IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()\n",
    "\n",
    "# === YOUR SOLUTION GOES HERE ===\n",
    "# Implement the complete ML pipeline\n",
    "\n",
    "# Step 1: Data Exploration\n",
    "# Check class distribution for imbalance\n",
    "\n",
    "# Step 2: Feature Engineering Pipeline\n",
    "# Create indexers for categorical variables\n",
    "# Add derived features\n",
    "# Create feature assembler\n",
    "\n",
    "# Step 3: Train/Test Split\n",
    "# Split data appropriately\n",
    "\n",
    "# Step 4: Model Training\n",
    "# Try Logistic Regression, Random Forest, Gradient Boosted Trees\n",
    "# Create pipelines for each model\n",
    "\n",
    "# Step 5: Model Evaluation\n",
    "# Evaluate using appropriate metrics for imbalanced data\n",
    "\n",
    "# Step 6: Hyperparameter Tuning\n",
    "# Use CrossValidator for best model selection\n",
    "\n",
    "# Step 7: Final Evaluation & Model Saving\n",
    "# Generate confusion matrix, feature importance\n",
    "# Save the best model\n",
    "\n",
    "print(\"Implement your churn prediction pipeline above!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… Solution Reference\n",
    "\n",
    "**âš ï¸ Only refer to this after attempting the problem yourself!**\n",
    "\n",
    "Here's a complete ML pipeline solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION IMPLEMENTATION\n",
    "\n",
    "# 1. Data Exploration\n",
    "print(\"1. Data Exploration:\")\n",
    "df.printSchema()\n",
    "print(f\"Total records: {df.count()}\")\n",
    "\n",
    "# Check class distribution\n",
    "print(\"Class distribution:\")\n",
    "df.groupBy('churn').count().show()\n",
    "\n",
    "# 2. Feature Engineering\n",
    "print(\"\\n2. Feature Engineering:\")\n",
    "\n",
    "# Create indexers for categorical variables\n",
    "gender_indexer = StringIndexer(inputCol='gender', outputCol='gender_index', handleInvalid='keep')\n",
    "contract_indexer = StringIndexer(inputCol='contract_type', outputCol='contract_index', handleInvalid='keep')\n",
    "internet_indexer = StringIndexer(inputCol='internet_service', outputCol='internet_index', handleInvalid='keep')\n",
    "payment_indexer = StringIndexer(inputCol='payment_method', outputCol='payment_index', handleInvalid='keep')\n",
    "\n",
    "# Create derived features\n",
    "df_features = df.withColumn('charges_per_tenure', col('total_charges') / col('tenure_months')) \\\n",
    "               .fillna(0, ['charges_per_tenure'])\n",
    "\n",
    "# One-hot encoding for categorical variables\n",
    "encoder = OneHotEncoder(\n",
    "    inputCols=['contract_index', 'internet_index', 'payment_index'],\n",
    "    outputCols=['contract_vec', 'internet_vec', 'payment_vec']\n",
    ")\n",
    "\n",
    "# Assemble all features\n",
    "feature_cols = ['age', 'tenure_months', 'monthly_charges', 'total_charges', \n",
    "                'charges_per_tenure', 'gender_index', 'contract_vec', 'internet_vec', 'payment_vec']\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
    "\n",
    "# 3. Model Development\n",
    "print(\"\\n3. Model Development:\")\n",
    "\n",
    "# Split data\n",
    "train_df, test_df = df_features.randomSplit([0.8, 0.2], seed=42)\n",
    "print(f\"Train: {train_df.count()}, Test: {test_df.count()}\")\n",
    "\n",
    "# Define models\n",
    "lr = LogisticRegression(featuresCol='features', labelCol='churn', weightCol='class_weights')\n",
    "rf = RandomForestClassifier(featuresCol='features', labelCol='churn', numTrees=50)\n",
    "gbt = GBTClassifier(featuresCol='features', labelCol='churn', maxIter=20)\n",
    "\n",
    "# Handle class imbalance with weights\n",
    "train_with_weights = train_df.withColumn(\n",
    "    'class_weights',\n",
    "    when(col('churn') == 1, 2.0).otherwise(1.0)  # Give more weight to minority class\n",
    ")\n",
    "\n",
    "# Create pipelines\n",
    "lr_pipeline = Pipeline(stages=[gender_indexer, contract_indexer, internet_indexer, \n",
    "                              payment_indexer, encoder, assembler, lr])\n",
    "\n",
    "rf_pipeline = Pipeline(stages=[gender_indexer, contract_indexer, internet_indexer,\n",
    "                              payment_indexer, encoder, assembler, rf])\n",
    "\n",
    "gbt_pipeline = Pipeline(stages=[gender_indexer, contract_indexer, internet_indexer,\n",
    "                               payment_indexer, encoder, assembler, gbt])\n",
    "\n",
    "# 4. Model Evaluation\n",
    "print(\"\\n4. Model Evaluation:\")\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='churn', metricName='areaUnderROC')\n",
    "evaluator_pr = BinaryClassificationEvaluator(labelCol='churn', metricName='areaUnderPR')\n",
    "\n",
    "# Train and evaluate Logistic Regression\n",
    "lr_model = lr_pipeline.fit(train_with_weights)\n",
    "lr_predictions = lr_model.transform(test_df)\n",
    "lr_auc = evaluator.evaluate(lr_predictions)\n",
    "lr_auc_pr = evaluator_pr.evaluate(lr_predictions)\n",
    "\n",
    "print(f\"Logistic Regression - AUC: {lr_auc:.3f}, AUC-PR: {lr_auc_pr:.3f}\")\n",
    "\n",
    "# Train and evaluate Random Forest\n",
    "rf_model = rf_pipeline.fit(train_df)\n",
    "rf_predictions = rf_model.transform(test_df)\n",
    "rf_auc = evaluator.evaluate(rf_predictions)\n",
    "rf_auc_pr = evaluator_pr.evaluate(rf_predictions)\n",
    "\n",
    "print(f\"Random Forest - AUC: {rf_auc:.3f}, AUC-PR: {rf_auc_pr:.3f}\")\n",
    "\n",
    "# Train and evaluate Gradient Boosted Trees\n",
    "gbt_model = gbt_pipeline.fit(train_df)\n",
    "gbt_predictions = gbt_model.transform(test_df)\n",
    "gbt_auc = evaluator.evaluate(gbt_predictions)\n",
    "gbt_auc_pr = evaluator_pr.evaluate(gbt_predictions)\n",
    "\n",
    "print(f\"GBT - AUC: {gbt_auc:.3f}, AUC-PR: {gbt_auc_pr:.3f}\")\n",
    "\n",
    "# 5. Hyperparameter Tuning for Best Model (GBT)\n",
    "print(\"\\n5. Hyperparameter Tuning:\")\n",
    "\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(gbt.maxIter, [10, 20, 30]) \\\n",
    "    .addGrid(gbt.maxDepth, [3, 5, 7]) \\\n",
    "    .build()\n",
    "\n",
    "cv = CrossValidator(\n",
    "    estimator=gbt_pipeline,\n",
    "    estimatorParamMaps=param_grid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "cv_model = cv.fit(train_df)\n",
    "best_model = cv_model.bestModel\n",
    "\n",
    "# 6. Final Evaluation\n",
    "print(\"\\n6. Final Evaluation:\")\n",
    "\n",
    "final_predictions = best_model.transform(test_df)\n",
    "\n",
    "# Confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "final_predictions.select('churn', 'prediction').groupBy('churn', 'prediction').count().show()\n",
    "\n",
    "# Calculate precision, recall, F1\n",
    "tp = final_predictions.filter((col('churn') == 1) & (col('prediction') == 1)).count()\n",
    "fp = final_predictions.filter((col('churn') == 0) & (col('prediction') == 1)).count()\n",
    "tn = final_predictions.filter((col('churn') == 0) & (col('prediction') == 0)).count()\n",
    "fn = final_predictions.filter((col('churn') == 1) & (col('prediction') == 0)).count()\n",
    "\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"F1-Score: {f1:.3f}\")\n",
    "\n",
    "# Feature importance (for tree-based models)\n",
    "print(\"\\nFeature Importance:\")\n",
    "feature_importance = best_model.stages[-1].featureImportances\n",
    "for i, importance in enumerate(feature_importance):\n",
    "    print(f\"{feature_cols[i]}: {importance:.4f}\")\n",
    "\n",
    "# 7. Save Model\n",
    "print(\"\\n7. Saving Model:\")\n",
    "best_model.save('churn_prediction_model')\n",
    "print(\"Model saved successfully!\")\n",
    "\n",
    "print(\"\\nâœ… Churn prediction pipeline completed!\")\n",
    "print(\"Key Learnings:\")\n",
    "print(\"- Handle class imbalance with weighted training\")\n",
    "print(\"- Use AUC-PR for imbalanced classification evaluation\")\n",
    "print(\"- Feature engineering is crucial for model performance\")\n",
    "print(\"- Cross-validation helps prevent overfitting\")\n",
    "print(\"- Consider business metrics (precision vs recall) for model selection\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
