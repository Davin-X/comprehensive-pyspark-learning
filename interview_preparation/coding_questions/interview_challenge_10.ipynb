{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interview Challenge 10: Custom UDFs, Joins & Data Skew Handling\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "You are tasked with building a complex analytics pipeline that requires custom business logic, multiple data source joins, and handling data skew issues. This challenge focuses on advanced PySpark operations that require deep understanding of the framework.\n",
    "\n",
    "## Dataset Description\n",
    "\n",
    "**Users Table:**\n",
    "- `user_id` (string) - Unique user identifier\n",
    "- `registration_date` (date) - When user registered\n",
    "- `country` (string) - User country\n",
    "- `subscription_tier` (string) - Free, Basic, Premium, Enterprise\n",
    "- `last_login` (timestamp) - Last login timestamp\n",
    "\n",
    "**Events Table:**\n",
    "- `event_id` (string) - Unique event identifier\n",
    "- `user_id` (string) - User who triggered event\n",
    "- `event_type` (string) - login, purchase, feature_use, support_ticket\n",
    "- `event_timestamp` (timestamp) - When event occurred\n",
    "- `metadata` (string) - JSON string with additional data\n",
    "- `value` (double) - Numeric value associated with event\n",
    "\n",
    "**Products Table:**\n",
    "- `product_id` (string) - Unique product identifier\n",
    "- `name` (string) - Product name\n",
    "- `category` (string) - Product category\n",
    "- `price` (double) - Product price\n",
    "- `created_date` (date) - When product was created\n",
    "\n",
    "## Tasks\n",
    "\n",
    "1. **Custom UDF Implementation**\n",
    "   - Create UDF to parse JSON metadata and extract specific fields\n",
    "   - Implement business logic UDF for user segmentation\n",
    "   - Build custom aggregation UDF for complex metrics\n",
    "   - Handle null values and edge cases in UDFs\n",
    "\n",
    "2. **Complex Join Operations**\n",
    "   - Perform multi-table joins (users → events → products)\n",
    "   - Implement different join strategies (broadcast, sort-merge, shuffle-hash)\n",
    "   - Handle join conditions with custom logic\n",
    "   - Optimize join performance\n",
    "\n",
    "3. **Data Skew Resolution**\n",
    "   - Identify skewed data distributions\n",
    "   - Implement salting techniques for skewed joins\n",
    "   - Use custom partitioning to balance data\n",
    "   - Apply broadcast joins for small tables\n",
    "\n",
    "4. **Advanced Analytics**\n",
    "   - Calculate user lifetime value with complex business rules\n",
    "   - Implement cohort analysis\n",
    "   - Build product recommendation logic\n",
    "   - Generate predictive features\n",
    "\n",
    "5. **Performance Optimization**\n",
    "   - Implement proper caching strategies\n",
    "   - Use appropriate data structures\n",
    "   - Optimize memory usage\n",
    "   - Monitor and tune performance\n",
    "\n",
    "## Technical Requirements\n",
    "- Implement custom UDFs using both SQL and DataFrame API\n",
    "- Demonstrate different join strategies and their use cases\n",
    "- Handle data skew with practical solutions\n",
    "- Optimize for large-scale data processing\n",
    "- Include proper error handling and validation\n",
    "- Document performance considerations\n",
    "\n",
    "## Key Concepts to Demonstrate\n",
    "- UDF registration and usage\n",
    "- Join optimization techniques\n",
    "- Data skew detection and resolution\n",
    "- Memory management\n",
    "- Custom business logic implementation\n",
    "- Performance monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Window\n",
    "import json\n",
    "import hashlib\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Create Spark session with optimizations\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AdvancedUDFsJoinsChallenge\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"50MB\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "users_data = [\n",
    "    ('user001', '2020-01-15', 'US', 'Premium', '2023-12-15 10:30:00'),\n",
    "    ('user002', '2020-03-20', 'UK', 'Basic', '2023-12-14 15:45:00'),\n",
    "    ('user003', '2020-02-10', 'US', 'Enterprise', '2023-12-15 09:15:00'),\n",
    "    ('user004', '2020-04-05', 'DE', 'Free', '2023-12-13 14:20:00'),\n",
    "    ('user005', '2020-01-30', 'US', 'Premium', '2023-12-15 11:00:00')\n",
    "]\n",
    "\n",
    "events_data = [\n",
    "    ('evt001', 'user001', 'purchase', '2023-12-15 10:35:00', '{\"product_id\": \"prod001\", \"quantity\": 2}', 299.98),\n",
    "    ('evt002', 'user001', 'login', '2023-12-15 10:30:00', '{}', 0.0),\n",
    "    ('evt003', 'user002', 'feature_use', '2023-12-14 15:50:00', '{\"feature\": \"analytics\", \"duration\": 25}', 0.0),\n",
    "    ('evt004', 'user003', 'purchase', '2023-12-15 09:20:00', '{\"product_id\": \"prod002\", \"quantity\": 1}', 149.99),\n",
    "    ('evt005', 'user001', 'support_ticket', '2023-12-15 11:15:00', '{\"category\": \"billing\", \"priority\": \"high\"}', 0.0),\n",
    "    ('evt006', 'user004', 'login', '2023-12-13 14:20:00', '{}', 0.0),\n",
    "    ('evt007', 'user005', 'purchase', '2023-12-15 11:05:00', '{\"product_id\": \"prod003\", \"quantity\": 3}', 449.97)\n",
    "]\n",
    "\n",
    "products_data = [\n",
    "    ('prod001', 'Laptop Pro', 'Electronics', 149.99, '2020-01-01'),\n",
    "    ('prod002', 'Wireless Headphones', 'Electronics', 149.99, '2020-02-15'),\n",
    "    ('prod003', 'Office Chair', 'Furniture', 149.99, '2020-03-10')\n",
    "]\n",
    "\n",
    "# Define schemas\n",
    "users_schema = StructType([\n",
    "    StructField('user_id', StringType(), True),\n",
    "    StructField('registration_date', StringType(), True),\n",
    "    StructField('country', StringType(), True),\n",
    "    StructField('subscription_tier', StringType(), True),\n",
    "    StructField('last_login', StringType(), True)\n",
    "])\n",
    "\n",
    "events_schema = StructType([\n",
    "    StructField('event_id', StringType(), True),\n",
    "    StructField('user_id', StringType(), True),\n",
    "    StructField('event_type', StringType(), True),\n",
    "    StructField('event_timestamp', StringType(), True),\n",
    "    StructField('metadata', StringType(), True),\n",
    "    StructField('value', DoubleType(), True)\n",
    "])\n",
    "\n",
    "products_schema = StructType([\n",
    "    StructField('product_id', StringType(), True),\n",
    "    StructField('name', StringType(), True),\n",
    "    StructField('category', StringType(), True),\n",
    "    StructField('price', DoubleType(), True),\n",
    "    StructField('created_date', StringType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrames\n",
    "users_df = spark.createDataFrame(users_data, users_schema)\n",
    "events_df = spark.createDataFrame(events_data, events_schema)\n",
    "products_df = spark.createDataFrame(products_data, products_schema)\n",
    "\n",
    "# Convert date strings to proper types\n",
    "users_df = users_df \\\n",
    "    .withColumn('registration_date', to_date('registration_date')) \\\n",
    "    .withColumn('last_login', to_timestamp('last_login'))\n",
    "\n",
    "events_df = events_df.withColumn('event_timestamp', to_timestamp('event_timestamp'))\n",
    "products_df = products_df.withColumn('created_date', to_date('created_date'))\n",
    "\n",
    "print(\"Data loaded successfully!\")\n",
    "\n",
    "# === YOUR SOLUTION GOES HERE ===\n",
    "# Implement advanced UDFs, joins, and skew handling\n",
    "\n",
    "# Task 1: Custom UDFs\n",
    "print(\"\\n1. Implementing Custom UDFs:\")\n",
    "\n",
    "# UDF to parse JSON metadata and extract product_id\n",
    "def extract_product_id(metadata):\n",
    "    \"\"\"Extract product_id from JSON metadata\"\"\"\n",
    "    if not metadata or metadata == '{}':\n",
    "        return None\n",
    "    try:\n",
    "        data = json.loads(metadata)\n",
    "        return data.get('product_id')\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        return None\n",
    "\n",
    "# Register UDF\n",
    "extract_product_udf = udf(extract_product_id, StringType())\n",
    "spark.udf.register(\"extract_product_id\", extract_product_id, StringType())\n",
    "\n",
    "# UDF for user segmentation based on activity\n",
    "def segment_user(activity_score, subscription_tier):\n",
    "    \"\"\"Segment users based on activity and subscription\"\"\"\n",
    "    if subscription_tier == 'Enterprise':\n",
    "        return 'Enterprise'\n",
    "    elif activity_score > 100:\n",
    "        return 'Power_User'\n",
    "    elif activity_score > 50:\n",
    "        return 'Regular'\n",
    "    elif activity_score > 10:\n",
    "        return 'Occasional'\n",
    "    else:\n",
    "        return 'Inactive'\n",
    "\n",
    "segment_udf = udf(segment_user, StringType())\n",
    "\n",
    "# Task 2: Enhanced events with extracted data\n",
    "enhanced_events = events_df.withColumn(\n",
    "    'product_id', extract_product_udf('metadata')\n",
    ")\n",
    "\n",
    "print(\"Enhanced events with extracted product IDs:\")\n",
    "enhanced_events.select('event_id', 'user_id', 'event_type', 'product_id', 'value').show()\n",
    "\n",
    "# Task 3: Complex Joins\n",
    "print(\"\\n2. Performing Complex Joins:\")\n",
    "\n",
    "# Multi-table join: users -> events -> products\n",
    "# First, check data sizes for join strategy\n",
    "users_count = users_df.count()\n",
    "events_count = enhanced_events.count()\n",
    "products_count = products_df.count()\n",
    "\n",
    "print(f\"Data sizes - Users: {users_count}, Events: {events_count}, Products: {products_count}\")\n",
    "\n",
    "# Join users with events (likely large tables)\n",
    "user_events = users_df.join(enhanced_events, 'user_id', 'left')\n",
    "print(f\"User-events join result: {user_events.count()} rows\")\n",
    "\n",
    "# Join with products (products table is small, use broadcast)\n",
    "complete_data = user_events.join(\n",
    "    broadcast(products_df),\n",
    "    user_events.product_id == products_df.product_id,\n",
    "    'left'\n",
    ").drop(products_df.product_id)\n",
    "\n",
    "print(f\"Complete joined data: {complete_data.count()} rows\")\n",
    "complete_data.select('user_id', 'country', 'event_type', 'name', 'category', 'value').show(10)\n",
    "\n",
    "# Task 4: Data Skew Handling\n",
    "print(\"\\n3. Handling Data Skew:\")\n",
    "\n",
    "# Check for skew in user events\n",
    "user_event_counts = enhanced_events.groupBy('user_id').count().orderBy(desc('count'))\n",
    "print(\"Top users by event count (checking for skew):\")\n",
    "user_event_counts.show(5)\n",
    "\n",
    "# Implement salting for skewed joins\n",
    "def add_salt(column, num_buckets=10):\n",
    "    \"\"\"Add salt to a column for skew handling\"\"\"\n",
    "    return concat(col(column), lit('_'), (hash(col(column)) % num_buckets).cast('string'))\n",
    "\n",
    "# If we had severe skew, we would use:\n",
    "# salted_users = users_df.withColumn('salted_user_id', add_salt('user_id'))\n",
    "# salted_events = enhanced_events.withColumn('salted_user_id', add_salt('user_id'))\n",
    "# balanced_join = salted_users.join(salted_events, 'salted_user_id', 'left')\n",
    "\n",
    "# Task 5: Advanced Analytics with Custom Logic\n",
    "print(\"\\n4. Advanced Analytics:\")\n",
    "\n",
    "# Calculate user activity scores\n",
    "user_activity = enhanced_events.groupBy('user_id').agg(\n",
    "    count('event_id').alias('total_events'),\n",
    "    sum(when(col('event_type') == 'purchase', col('value')).otherwise(0)).alias('total_spent'),\n",
    "    countDistinct(when(col('event_type') == 'login', col('event_timestamp').cast('date'))).alias('active_days'),\n",
    "    max('event_timestamp').alias('last_activity')\n",
    ")\n",
    "\n",
    "# Calculate activity score (custom business logic)\n",
    "user_activity = user_activity.withColumn(\n",
    "    'activity_score',\n",
    "    col('total_events') * 10 + \n",
    "    col('total_spent') * 0.1 + \n",
    "    col('active_days') * 5\n",
    ")\n",
    "\n",
    "# Apply user segmentation\n",
    "user_segments = user_activity.withColumn(\n",
    "    'segment', segment_udf('activity_score', lit('Premium'))  # Using default tier for demo\n",
    ")\n",
    "\n",
    "print(\"User segmentation results:\")\n",
    "user_segments.select('user_id', 'total_events', 'total_spent', 'activity_score', 'segment').show()\n",
    "\n",
    "# Task 6: Cohort Analysis\n",
    "print(\"\\n5. Cohort Analysis:\")\n",
    "\n",
    "# Join with user registration data\n",
    "cohort_data = user_segments.join(\n",
    "    users_df.select('user_id', 'registration_date', 'subscription_tier'),\n",
    "    'user_id'\n",
    ").withColumn(\n",
    "    'cohort_month', date_format('registration_date', 'yyyy-MM')\n",
    ").withColumn(\n",
    "    'activity_month', date_format('last_activity', 'yyyy-MM')\n",
    ")\n",
    "\n",
    "# Calculate cohort retention\n",
    "cohort_retention = cohort_data.groupBy('cohort_month', 'activity_month').agg(\n",
    "    countDistinct('user_id').alias('active_users'),\n",
    "    avg('activity_score').alias('avg_activity_score')\n",
    ").orderBy('cohort_month', 'activity_month')\n",
    "\n",
    "print(\"Cohort analysis results:\")\n",
    "cohort_retention.show(10)\n",
    "\n",
    "# Task 7: Performance Optimization\n",
    "print(\"\\n6. Performance Optimizations:\")\n",
    "\n",
    "# Cache frequently used DataFrames\n",
    "users_df.cache()\n",
    "enhanced_events.cache()\n",
    "\n",
    "# Force cache materialization\n",
    "users_cached = users_df.count()\n",
    "events_cached = enhanced_events.count()\n",
    "\n",
    "print(f\"Cached {users_cached} users and {events_cached} events\")\n",
    "\n",
    "# Use appropriate partitioning for large datasets\n",
    "# In production, you would repartition based on expected query patterns:\n",
    "# optimized_data = complete_data.repartition('country', 'event_date')\n",
    "\n",
    "# Final results summary\n",
    "print(\"\\n=== FINAL RESULTS ===\")\n",
    "print(f\"Total users processed: {users_df.count()}\")\n",
    "print(f\"Total events processed: {enhanced_events.count()}\")\n",
    "print(f\"Total products: {products_df.count()}\")\n",
    "print(f\"Joined records: {complete_data.count()}\")\n",
    "\n",
    "# Show sample of final enriched data\n",
    "print(\"\\nSample enriched user events:\")\n",
    "complete_data.select(\n",
    "    'user_id', 'country', 'subscription_tier', \n",
    "    'event_type', 'name', 'category', 'value', 'event_timestamp'\n",
    ").orderBy('user_id', 'event_timestamp').show(10)\n",
    "\n",
    "# Cleanup\n",
    "spark.catalog.clearCache()\n",
    "print(\"\\nCache cleared, processing complete!\")\n",
    "\n",
    "print(\"\\n✅ Advanced UDFs and Joins Challenge completed!\")\n",
    "print(\"Key Learnings:\")\n",
    "print(\"- UDFs enable custom business logic in distributed processing\")\n",
    "print(\"- Join strategies (broadcast vs sort-merge) depend on data sizes\")\n",
    "print(\"- Salting helps handle data skew in distributed joins\")\n",
    "print(\"- Caching optimizes repeated access to intermediate results\")\n",
    "print(\"- Custom partitioning improves query performance\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
