{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¯ Advanced Business Logic - Interview Scenarios\n",
    "\n",
    "## **Module Overview**\n",
    "**Master complex business logic implementation with advanced PySpark operations for enterprise-level data processing.**\n",
    "\n",
    "**ğŸ¯ Difficulty**: ğŸ”´ Advanced\n",
    "**â±ï¸ Estimated Time**: 3-4 hours\n",
    "**ğŸ“ Learning Focus**: Complex business logic and enterprise patterns\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ What You Will Learn\n",
    "\n",
    "By completing this module, you will master:\n",
    "- âœ… **Data reconciliation** and comparison techniques\n",
    "- âœ… **Complex hierarchical processing** (parent-child relationships)\n",
    "- âœ… **Advanced filtering** and conditional logic\n",
    "- âœ… **Business rule implementation** with PySpark\n",
    "- âœ… **Data quality analysis** and null handling\n",
    "- âœ… **Array and collection operations**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Advanced Business Logic\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ Scenario 1: Data Reconciliation\n",
    "\n",
    "## ğŸ“‹ Problem Statement\n",
    "\n",
    "### **ğŸ¯ Interview Question:**\n",
    "**\"Implement data reconciliation solution in PySpark\"**\n",
    "\n",
    "### **ğŸ’¼ Business Context:**\n",
    "Compare source and target datasets for data quality - advanced requirement for enterprise data processing.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 1: Data Reconciliation (Source vs Target comparison)\n",
    "\n",
    "source_data = [(1, \"A\"), (2, \"B\"), (3, \"C\"), (4, \"D\")]\n",
    "target_data = [(1, \"A\"), (2, \"B\"), (4, \"X\"), (5, \"F\")]\n",
    "\n",
    "df_source = spark.createDataFrame(source_data, [\"id\", \"name\"])\n",
    "df_target = spark.createDataFrame(target_data, [\"id1\", \"name1\"])\n",
    "\n",
    "print(\"Source Data:\")\n",
    "df_source.show()\n",
    "print(\"Target Data:\")\n",
    "df_target.show()\n",
    "\n",
    "# Method 1: SQL Approach\n",
    "df_source.createOrReplaceTempView(\"sourcetab\")\n",
    "df_target.createOrReplaceTempView(\"targettab\")\n",
    "\n",
    "print(\"=== SQL Approach ===\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT COALESCE(s.id, t.id1) AS id,\n",
    "           CASE\n",
    "               WHEN s.name IS NULL THEN 'new in target'\n",
    "               WHEN t.name1 IS NULL THEN 'new in source'\n",
    "               WHEN s.name != t.name1 THEN 'mismatch'\n",
    "               ELSE 'match'\n",
    "           END AS status\n",
    "    FROM sourcetab s\n",
    "    FULL OUTER JOIN targettab t ON s.id = t.id1\n",
    "    WHERE s.name != t.name1 OR s.name IS NULL OR t.name1 IS NULL\n",
    "\"\"\").show()\n",
    "\n",
    "# Method 2: DSL Approach\n",
    "print(\"=== DSL Approach ===\")\n",
    "df_joined = df_source.join(df_target, df_source[\"id\"] == df_target[\"id1\"], \"outer\")\n",
    "\n",
    "# Filter mismatched and null records\n",
    "df_filtered = df_joined.filter(\n",
    "    (col(\"name\") != col(\"name1\")) | col(\"name\").isNull() | col(\"name1\").isNull()\n",
    ")\n",
    "\n",
    "# Coalesce IDs and add status column\n",
    "df_result = df_filtered.withColumn(\"id\", coalesce(col(\"id\"), col(\"id1\")))\\\n",
    "    .drop(\"id1\")\\\n",
    "    .withColumn(\"status\",\n",
    "        when(col(\"name\").isNull(), \"new in target\")\n",
    "        .when(col(\"name1\").isNull(), \"new in source\")\n",
    "        .when(col(\"name\") != col(\"name1\"), \"mismatch\")\n",
    "        .otherwise(\"match\")\n",
    "    ).drop(\"name\", \"name1\")\n",
    "\n",
    "df_result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ Scenario 2: Salary Increment Analysis\n",
    "\n",
    "## ğŸ“‹ Problem Statement\n",
    "\n",
    "### **ğŸ¯ Interview Question:**\n",
    "**\"Implement salary increment analysis solution in PySpark\"**\n",
    "\n",
    "### **ğŸ’¼ Business Context:**\n",
    "Calculate year-over-year salary changes - advanced requirement for enterprise data processing.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 2: Calculate salary increments over time\n",
    "\n",
    "salary_data = [\n",
    "    (1, 60000, 2018), (1, 70000, 2019), (1, 80000, 2020),\n",
    "    (2, 60000, 2018), (2, 65000, 2019), (2, 65000, 2020),\n",
    "    (3, 60000, 2018), (3, 65000, 2019)\n",
    "]\n",
    "\n",
    "df_salaries = spark.createDataFrame(salary_data, [\"empid\", \"salary\", \"year\"])\n",
    "df_salaries.show()\n",
    "\n",
    "# Calculate year-over-year salary differences\n",
    "window_spec = Window.partitionBy(\"empid\").orderBy(\"year\")\n",
    "\n",
    "df_with_lag = df_salaries.withColumn(\"prev_salary\", lag(\"salary\", 1).over(window_spec))\n",
    "df_with_lag.show()\n",
    "\n",
    "df_increments = df_with_lag.withColumn(\n",
    "    \"salary_increment\",\n",
    "    when(col(\"prev_salary\").isNull(), 0)\n",
    "    .otherwise(col(\"salary\") - col(\"prev_salary\"))\n",
    ").drop(\"prev_salary\")\n",
    "\n",
    "df_increments.orderBy(\"empid\", \"year\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ Scenario 3: Hierarchical Data Processing\n",
    "\n",
    "## ğŸ“‹ Problem Statement\n",
    "\n",
    "### **ğŸ¯ Interview Question:**\n",
    "**\"Implement hierarchical data processing solution in PySpark\"**\n",
    "\n",
    "### **ğŸ’¼ Business Context:**\n",
    "Build parent-child relationships - advanced requirement for enterprise data processing.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 3: Build parent-child hierarchy\n",
    "\n",
    "hierarchy_data = [\n",
    "    (\"A\", \"AA\"), (\"B\", \"BB\"), (\"C\", \"CC\"),\n",
    "    (\"AA\", \"AAA\"), (\"BB\", \"BBB\"), (\"CC\", \"CCC\")\n",
    "]\n",
    "\n",
    "df_hierarchy = spark.createDataFrame(hierarchy_data, [\"child\", \"parent\"])\n",
    "df_hierarchy.show()\n",
    "\n",
    "# Self-join to find child-parent-grandparent relationships\n",
    "df_joined = df_hierarchy.alias(\"child_table\")\\\n",
    "    .join(df_hierarchy.alias(\"parent_table\"),\n",
    "         col(\"child_table.child\") == col(\"parent_table.parent\"),\n",
    "         \"inner\")\\\n",
    "    .select(\n",
    "        col(\"child_table.child\").alias(\"child\"),\n",
    "        col(\"child_table.parent\").alias(\"parent\"),\n",
    "        col(\"parent_table.child\").alias(\"grandparent\")\n",
    "    )\n",
    "\n",
    "df_joined.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ Scenario 4: Conditional Filtering\n",
    "\n",
    "## ğŸ“‹ Problem Statement\n",
    "\n",
    "### **ğŸ¯ Interview Question:**\n",
    "**\"Implement conditional filtering solution in PySpark\"**\n",
    "\n",
    "### **ğŸ’¼ Business Context:**\n",
    "Remove specific values based on complex conditions - advanced requirement for enterprise data processing.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 4: Remove maximum value from dataset\n",
    "\n",
    "data1 = [(1,), (2,), (3,)]\n",
    "data2 = [(1,), (2,), (3,), (4,), (5,)]\n",
    "\n",
    "df1 = spark.createDataFrame(data1, [\"col\"])\n",
    "df2 = spark.createDataFrame(data2, [\"col1\"])\n",
    "\n",
    "df1.show()\n",
    "df2.show()\n",
    "\n",
    "# Find max value from df1\n",
    "max_value = df1.agg(max(\"col\")).collect()[0][0]\n",
    "print(f\"Maximum value to remove: {max_value}\")\n",
    "\n",
    "# Outer join and filter out the max value\n",
    "df_joined = df1.join(df2, df1[\"col\"] == df2[\"col1\"], \"outer\")\\\n",
    "    .drop(\"col\")\n",
    "\n",
    "df_filtered = df_joined.filter(col(\"col1\") != max_value).orderBy(\"col1\")\n",
    "df_filtered.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ Scenario 5: Department Analytics\n",
    "\n",
    "## ğŸ“‹ Problem Statement\n",
    "\n",
    "### **ğŸ¯ Interview Question:**\n",
    "**\"Implement department analytics solution in PySpark\"**\n",
    "\n",
    "### **ğŸ’¼ Business Context:**\n",
    "Find second highest values by category - advanced requirement for enterprise data processing.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 5: Find second highest salary by department\n",
    "\n",
    "emp_dept_data = [\n",
    "    (1, \"A\", \"A\", 1000000), (2, \"B\", \"A\", 2500000),\n",
    "    (3, \"C\", \"G\", 500000), (4, \"D\", \"G\", 800000),\n",
    "    (5, \"E\", \"W\", 9000000), (6, \"F\", \"W\", 2000000)\n",
    "]\n",
    "\n",
    "dept_data = [(\"A\", \"AZURE\"), (\"G\", \"GCP\"), (\"W\", \"AWS\")]\n",
    "\n",
    "df_employees = spark.createDataFrame(emp_dept_data, [\"emp_id\", \"name\", \"dept_id\", \"salary\"])\n",
    "df_departments = spark.createDataFrame(dept_data, [\"dept_id1\", \"dept_name\"])\n",
    "\n",
    "df_employees.show()\n",
    "df_departments.show()\n",
    "\n",
    "# Join employees with departments\n",
    "df_emp_dept = df_employees.join(df_departments,\n",
    "                               df_employees[\"dept_id\"] == df_departments[\"dept_id1\"],\n",
    "                               \"inner\")\\\n",
    "    .drop(\"dept_id1\")\n",
    "\n",
    "df_emp_dept.show()\n",
    "\n",
    "# Find second highest salary per department\n",
    "window_spec = Window.partitionBy(\"dept_id\").orderBy(col(\"salary\").desc())\n",
    "\n",
    "df_ranked = df_emp_dept.withColumn(\"rank\", dense_rank().over(window_spec))\n",
    "df_ranked.show()\n",
    "\n",
    "df_second_highest = df_ranked.filter(col(\"rank\") == 2)\\\n",
    "    .drop(\"rank\")\\\n",
    "    .select(\"emp_id\", \"name\", \"dept_name\", \"salary\")\n",
    "\n",
    "df_second_highest.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ Scenario 6: Data Concatenation\n",
    "\n",
    "## ğŸ“‹ Problem Statement\n",
    "\n",
    "### **ğŸ¯ Interview Question:**\n",
    "**\"Implement data concatenation solution in PySpark\"**\n",
    "\n",
    "### **ğŸ’¼ Business Context:**\n",
    "Combine multiple columns and explode results - advanced requirement for enterprise data processing.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 6: Explode concatenated columns\n",
    "\n",
    "multi_col_data = [(\"m1\", \"m1,m2\", \"m1,m2,m3\", \"m1,m2,m3,m4\")]\n",
    "\n",
    "df_multi = spark.createDataFrame(multi_col_data, [\"col1\", \"col2\", \"col3\", \"col4\"])\n",
    "df_multi.show()\n",
    "\n",
    "# Concatenate all columns with separator\n",
    "df_concat = df_multi.withColumn(\n",
    "    \"concatenated\",\n",
    "    concat_ws(\"-\", \"col1\", \"col2\", \"col3\", \"col4\")\n",
    ").drop(\"col1\", \"col2\", \"col3\", \"col4\")\n",
    "\n",
    "df_concat.show()\n",
    "\n",
    "# Explode the concatenated string back to individual rows\n",
    "df_exploded = df_concat.selectExpr(\"explode(split(concatenated, '-')) as value\")\n",
    "df_exploded.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ Scenario 7: Rating Visualization\n",
    "\n",
    "## ğŸ“‹ Problem Statement\n",
    "\n",
    "### **ğŸ¯ Interview Question:**\n",
    "**\"Implement rating visualization solution in PySpark\"**\n",
    "\n",
    "### **ğŸ’¼ Business Context:**\n",
    "Create star ratings from numeric values - advanced requirement for enterprise data processing.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 7: Create star rating visualization\n",
    "\n",
    "# Sample food rating data (this would be defined earlier in the notebook)\n",
    "food_data = [(1, \"Pizza\", 4), (2, \"Burger\", 5), (3, \"Pasta\", 3)]\n",
    "df_food = spark.createDataFrame(food_data, [\"food_id\", \"food_item\", \"rating\"])\n",
    "\n",
    "df_food.show()\n",
    "\n",
    "# Create star rating string\n",
    "df_star_rating = df_food.withColumn(\n",
    "    \"stars\",\n",
    "    expr(\"repeat('*', rating)\")\n",
    ")\n",
    "\n",
    "df_star_rating.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ Scenario 8: Family Vacation Planning\n",
    "\n",
    "## ğŸ“‹ Problem Statement\n",
    "\n",
    "### **ğŸ¯ Interview Question:**\n",
    "**\"Implement family vacation planning solution in PySpark\"**\n",
    "\n",
    "### **ğŸ’¼ Business Context:**\n",
    "Complex multi-criteria matching - advanced requirement for enterprise data processing.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 8: Find suitable countries for family vacation\n",
    "\n",
    "# Sample family and country data\n",
    "family_data = [(\"Smith\", 5), (\"Johnson\", 3), (\"Brown\", 7)]\n",
    "country_data = [(\"USA\", 2, 6), (\"Canada\", 1, 4), (\"Mexico\", 3, 8), (\"UK\", 1, 3)]\n",
    "\n",
    "df_families = spark.createDataFrame(family_data, [\"name\", \"family_size\"])\n",
    "df_countries = spark.createDataFrame(country_data, [\"country\", \"min_size\", \"max_size\"])\n",
    "\n",
    "df_families.show()\n",
    "df_countries.show()\n",
    "\n",
    "# Join families with suitable countries\n",
    "df_family_countries = df_families.join(\n",
    "    df_countries,\n",
    "    (df_families[\"family_size\"] >= df_countries[\"min_size\"]) &\n",
    "    (df_families[\"family_size\"] <= df_countries[\"max_size\"]),\n",
    "    \"inner\"\n",
    ").select(df_families[\"name\"], df_families[\"family_size\"],\n",
    "         df_countries[\"country\"], \"min_size\", \"max_size\")\n",
    "\n",
    "df_family_countries.show()\n",
    "\n",
    "# Count countries per family\n",
    "df_country_count = df_family_countries.groupBy(df_families[\"name\"])\\\n",
    "    .agg(count(\"*\").alias(\"num_countries\"))\n",
    "\n",
    "df_country_count.show()\n",
    "\n",
    "# Find family with maximum country options\n",
    "window_spec = Window.orderBy(col(\"num_countries\").desc())\n",
    "df_ranked = df_country_count.withColumn(\"rank\", row_number().over(window_spec))\n",
    "\n",
    "df_top_family = df_ranked.filter(col(\"rank\") == 1).drop(\"rank\")\n",
    "df_top_family.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ Scenario 9: Data Quality Assessment\n",
    "\n",
    "## ğŸ“‹ Problem Statement\n",
    "\n",
    "### **ğŸ¯ Interview Question:**\n",
    "**\"Implement data quality assessment solution in PySpark\"**\n",
    "\n",
    "### **ğŸ’¼ Business Context:**\n",
    "Comprehensive null analysis and imputation - advanced requirement for enterprise data processing.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 9: Data Quality Analysis\n",
    "\n",
    "# Sample student data with nulls\n",
    "student_data = [\n",
    "    (1, \"John\", None, \"NY\"),\n",
    "    (2, \"Jane\", 25, \"CA\"),\n",
    "    (3, \"Bob\", 30, None),\n",
    "    (4, None, 22, \"TX\")\n",
    "]\n",
    "\n",
    "df_students = spark.createDataFrame(student_data, [\"id\", \"name\", \"age\", \"city\"])\n",
    "df_students.show()\n",
    "\n",
    "# Count null values per column\n",
    "null_counts = df_students.select([\n",
    "    sum(col(c).isNull().cast(\"int\")).alias(c) for c in df_students.columns\n",
    "])\n",
    "\n",
    "null_counts.show()\n",
    "\n",
    "# Remove rows with null ages\n",
    "df_clean = df_students.filter(col(\"age\").isNotNull())\n",
    "df_clean.show()\n",
    "\n",
    "# Add city data and fill missing ages with mean\n",
    "city_data = [(1, \"seatle\", 82), (2, \"london\", 75), (3, \"banglore\", 60), (4, \"boston\", 90)]\n",
    "df_cities = spark.createDataFrame(city_data, [\"id\", \"city\", \"code\"])\n",
    "\n",
    "df_enriched = df_clean.join(df_cities, \"id\", \"inner\")\\\n",
    "    .select(\"id\", \"name\", \"age\", \"city\", \"code\")\n",
    "\n",
    "# Calculate mean age\n",
    "mean_age = df_enriched.select(round(mean(\"age\"))).collect()[0][0]\n",
    "print(f\"Mean age for null filling: {mean_age}\")\n",
    "\n",
    "# Fill null ages with mean\n",
    "df_filled = df_enriched.na.fill({\"age\": mean_age})\n",
    "df_filled.show()\n",
    "\n",
    "# Filter adults (18+)\n",
    "df_adults = df_filled.filter(col(\"age\") >= 18)\n",
    "df_adults.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ Scenario 10: Sales Data Aggregation\n",
    "\n",
    "## ğŸ“‹ Problem Statement\n",
    "\n",
    "### **ğŸ¯ Interview Question:**\n",
    "**\"Implement sales data aggregation solution in PySpark\"**\n",
    "\n",
    "### **ğŸ’¼ Business Context:**\n",
    "Group and aggregate with collections - advanced requirement for enterprise data processing.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 10: Product sales analysis\n",
    "\n",
    "# Sample sales data\n",
    "sales_data = [\n",
    "    (\"2023-01-01\", \"Apple\"),\n",
    "    (\"2023-01-01\", \"Banana\"),\n",
    "    (\"2023-01-02\", \"Apple\"),\n",
    "    (\"2023-01-02\", \"Orange\")\n",
    "]\n",
    "\n",
    "df_sales = spark.createDataFrame(sales_data, [\"sell_date\", \"product\"])\n",
    "df_sales.show()\n",
    "\n",
    "# Group by date and collect products\n",
    "df_daily_products = df_sales.groupBy(\"sell_date\")\\\n",
    "    .agg(\n",
    "        collect_set(\"product\").alias(\"products\"),\n",
    "        size(collect_set(\"product\")).alias(\"unique_products\")\n",
    "    )\n",
    "\n",
    "df_daily_products.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ‰ Module Complete!\n",
    "\n",
    "## **ğŸ† Congratulations!**\n",
    "\n",
    "You have successfully completed the **Advanced Business Logic** interview scenarios module!\n",
    "\n",
    "### **ğŸ“Š What You Mastered:**\n",
    "- **Enterprise-level business logic** implementation\n",
    "- **Complex data reconciliation** techniques\n",
    "- **Advanced analytical operations**\n",
    "- **Data quality and validation** strategies\n",
    "- **Hierarchical and relational** data processing\n",
    "\n",
    "### **ğŸš€ Next Steps:**\n",
    "ğŸ¯ **You are now ready for PySpark interviews!**\n",
    "\n",
    "Consider exploring:\n",
    "- **Performance Tuning** for large-scale operations\n",
    "- **Streaming Analytics** with Spark Structured Streaming\n",
    "- **Machine Learning** pipelines with MLlib\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Learning & Best of Luck in Your Interviews! ğŸš€**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
