{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¯ Basic Operations - Interview Scenarios\n",
    "\n",
    "## **Module Overview**\n",
    "**Master fundamental PySpark operations through hands-on interview scenarios covering joins, filtering, and data transformations.**\n",
    "\n",
    "**ğŸ¯ Difficulty**: ğŸŸ¢ Beginner\n",
    "**â±ï¸ Estimated Time**: 1-2 hours\n",
    "**ğŸ“ Learning Focus**: Hands-on interview preparation\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ What You Will Learn\n",
    "\n",
    "By completing this module, you will be able to:\n",
    "- âœ… **Solve real interview problems** with PySpark\n",
    "- âœ… **Compare SQL vs DataFrame DSL** approaches\n",
    "- âœ… **Understand performance implications** of different solutions\n",
    "- âœ… **Apply best practices** for production code\n",
    "- âœ… **Debug and optimize** PySpark operations\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¯ Basic Operations - Interview Scenarios\n",
    "\n",
    "## **Module Overview**\n",
    "**Master fundamental PySpark operations through hands-on interview scenarios covering joins, filtering, and data transformations.**\n",
    "\n",
    "**ğŸ¯ Difficulty**: ğŸŸ¢ Beginner  \n",
    "**â±ï¸ Estimated Time**: 1-2 hours  \n",
    "**ğŸ“ Learning Focus**: Hands-on interview scenario practice\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Basic Operations\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 1: Find employees with equal salaries\n",
    "\n",
    "employee_data = [\n",
    "    (\"001\", \"Monika\", \"Arora\", 100000, \"2014-02-20 09:00:00\", \"HR\"),\n",
    "    (\"002\", \"Niharika\", \"Verma\", 300000, \"2014-06-11 09:00:00\", \"Admin\"),\n",
    "    (\"003\", \"Vishal\", \"Singhal\", 300000, \"2014-02-20 09:00:00\", \"HR\"),\n",
    "    (\"004\", \"Amitabh\", \"Singh\", 500000, \"2014-02-20 09:00:00\", \"Admin\")\n",
    "]\n",
    "\n",
    "df_employees = spark.createDataFrame(employee_data, [\"workerid\", \"firstname\", \"lastname\", \"salary\", \"joiningdate\", \"depart\"])\n",
    "df_employees.show()\n",
    "\n",
    "# Method 1: SQL Approach\n",
    "df_employees.createOrReplaceTempView(\"workers\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT a.workerid, a.firstname, a.lastname, a.salary, a.joiningdate, a.depart\n",
    "    FROM workers a, workers b\n",
    "    WHERE a.salary = b.salary\n",
    "    AND a.workerid != b.workerid\n",
    "    ORDER BY salary\n",
    "\"\"\")\\\n",
    "    .dropDuplicates()\\\n",
    "    .show()\n",
    "\n",
    "# Method 2: DataFrame DSL Approach\n",
    "result_dsl = df_employees.alias(\"a\")\\\n",
    "    .join(df_employees.alias(\"b\"),\n",
    "         (col(\"a.salary\") == col(\"b.salary\")) & (col(\"a.workerid\") != col(\"b.workerid\")),\n",
    "         \"inner\")\\\n",
    "    .select(\"a.workerid\", \"a.firstname\", \"a.lastname\", \"a.salary\", \"a.joiningdate\", \"a.depart\")\\\n",
    "    .distinct()\\\n",
    "    .orderBy(\"salary\")\n",
    "\n",
    "result_dsl.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 2: Track order status changes\n",
    "\n",
    "order_data = [\n",
    "    (1, \"1-Jan\", \"Ordered\"),\n",
    "    (1, \"2-Jan\", \"dispatched\"),\n",
    "    (1, \"3-Jan\", \"dispatched\"),\n",
    "    (1, \"4-Jan\", \"Shipped\"),\n",
    "    (2, \"1-Jan\", \"Ordered\"),\n",
    "    (2, \"2-Jan\", \"dispatched\")\n",
    "]\n",
    "\n",
    "df_orders = spark.createDataFrame(order_data, [\"orderid\", \"statusdate\", \"status\"])\n",
    "df_orders.show()\n",
    "\n",
    "# Method 1: SQL Approach - Find dispatched orders that have been ordered\n",
    "df_orders.createOrReplaceTempView(\"ordertab\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT * FROM ordertab\n",
    "    WHERE status = \"dispatched\"\n",
    "    AND orderid IN (\n",
    "        SELECT orderid FROM ordertab WHERE status = \"Ordered\"\n",
    "    )\n",
    "\"\"\")\\\n",
    "    .show()\n",
    "\n",
    "# Method 2: DataFrame DSL Approach\n",
    "ordered_orders = df_orders.filter(col(\"status\") == \"Ordered\")\\\n",
    "                              .select(\"orderid\")\\\n",
    "                              .distinct()\n",
    "\n",
    "result_dsl = df_orders.join(ordered_orders, \"orderid\", \"inner\")\\\n",
    "                      .filter(col(\"status\") == \"dispatched\")\n",
    "\n",
    "result_dsl.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ Scenario 1: Find Employees with Equal Salaries\n",
    "\n",
    "## ğŸ“‹ Problem Statement\n",
    "\n",
    "### **ğŸ¯ Interview Question:**\n",
    "**\"Write a query to find employees who earn the same salary as their colleagues.\"**\n",
    "\n",
    "### **ğŸ’¼ Business Context:**\n",
    "HR Analytics team needs to identify employees earning identical salaries for:\n",
    "- **Compensation Analysis**: Understanding salary distribution patterns\n",
    "- **Budget Planning**: Identifying compensation clusters\n",
    "- **Equity Review**: Ensuring fair pay practices\n",
    "\n",
    "### **ğŸ“Š Input Data:**\n",
    "\n",
    "\n",
    "### **ğŸ¯ Expected Output:**\n",
    "Employees with matching salaries (excluding self-matches).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ What You Will Learn\n",
    "\n",
    "By completing this module, you will be able to:\n",
    "- âœ… **Solve real interview problems** with PySpark\n",
    "- âœ… **Compare SQL vs DataFrame DSL** approaches\n",
    "- âœ… **Understand performance implications** of different solutions\n",
    "- âœ… **Apply best practices** for production code\n",
    "- âœ… **Debug and troubleshoot** PySpark operations\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¡ Solution Explanation\n",
    "\n",
    "### **ğŸ” Approach Analysis:**\n",
    "This scenario demonstrates fundamental **self-join operations** that are frequently asked in PySpark interviews.\n",
    "\n",
    "### **âš¡ Performance Considerations:**\n",
    "- **SQL vs DSL**: Both approaches have similar performance for small datasets\n",
    "- **Distinct**: Prevents duplicate pairs (A-B, B-A)\n",
    "- **Where clause**: Excludes self-comparisons\n",
    "\n",
    "### **ğŸ¯ Key Learning Points:**\n",
    "- Self-joins for comparing rows within the same table\n",
    "- Cartesian product avoidance techniques\n",
    "- Duplicate elimination strategies\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ Scenario 2: Track Order Status Changes\n",
    "\n",
    "## ğŸ“‹ Problem Statement\n",
    "\n",
    "### **ğŸ¯ Interview Question:**\n",
    "**\"Find orders that have been dispatched but were previously ordered.\"**\n",
    "\n",
    "### **ğŸ’¼ Business Context:**\n",
    "E-commerce Operations team needs to track order status transitions for:\n",
    "- **Order Fulfillment Monitoring**: Ensuring proper status flow\n",
    "- **Customer Experience**: Tracking delivery progress\n",
    "- **Operations Analytics**: Understanding fulfillment efficiency\n",
    "\n",
    "### **ğŸ“Š Input Data:**\n",
    "\n",
    "\n",
    "### **ğŸ¯ Expected Output:**\n",
    "Orders that have both \"Ordered\" and \"dispatched\" status.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“– How to Use This Notebook\n",
    "\n",
    "### **ğŸ” For Each Scenario:**\n",
    "1. **Read the problem statement** and understand requirements\n",
    "2. **Analyze the input data** and expected output\n",
    "3. **Try to solve it yourself** before looking at solutions\n",
    "4. **Compare your approach** with provided solutions\n",
    "5. **Understand the explanations** and best practices\n",
    "\n",
    "### **ğŸ’¡ Learning Tips:**\n",
    "- **Time yourself**: Real interviews have time constraints\n",
    "- **Multiple approaches**: Notice SQL vs DSL differences\n",
    "- **Performance matters**: Consider scalability\n",
    "- **Code quality**: Follow production standards\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¡ Solution Explanation\n",
    "\n",
    "### **ğŸ” Approach Analysis:**\n",
    "This scenario tests understanding of **conditional logic and subqueries** in PySpark.\n",
    "\n",
    "### **âš¡ Performance Considerations:**\n",
    "- **Subqueries**: Can be expensive on large datasets\n",
    "- **Joins**: More efficient for complex filtering\n",
    "- **Distinct**: Prevents duplicate order IDs\n",
    "\n",
    "### **ğŸ¯ Key Learning Points:**\n",
    "- Subquery usage in SQL vs join approaches\n",
    "- Conditional filtering techniques\n",
    "- Business rule implementation\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ‰ Module Complete!\n",
    "\n",
    "## **ğŸ† Congratulations!**\n",
    "\n",
    "You have successfully completed the **Basic Operations** interview scenarios module!\n",
    "\n",
    "### **ğŸ“Š What You Mastered:**\n",
    "- **Self-joins and filtering** for data analysis\n",
    "- **Conditional logic** for business rules\n",
    "- **Basic data transformations** and aggregations\n",
    "- **SQL vs DSL comparison** for different approaches\n",
    "- **Performance considerations** for large datasets\n",
    "\n",
    "### **ğŸš€ Next Steps:**\n",
    "Ready for more advanced scenarios? Try the **Aggregations & Analytics** module!\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Learning! ğŸ¯**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
