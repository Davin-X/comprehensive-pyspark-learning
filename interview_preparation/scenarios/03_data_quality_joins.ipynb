{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¯ Data Quality & Joins - Interview Scenarios\n",
    "\n",
    "## **Module Overview**\n",
    "**Master data quality handling, complex joins, and advanced transformations for robust PySpark applications.**\n",
    "\n",
    "**ğŸ¯ Difficulty**: ğŸŸ¡ Intermediate\n",
    "**â±ï¸ Estimated Time**: 2-3 hours\n",
    "**ğŸ“ Learning Focus**: Data quality and complex join operations\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ What You Will Learn\n",
    "\n",
    "By completing this module, you will master:\n",
    "- âœ… **Complex join operations** (self-joins, multi-table joins)\n",
    "- âœ… **Data quality handling** (nulls, duplicates, malformed data)\n",
    "- âœ… **Advanced transformations** (explode, flatten, pivot)\n",
    "- âœ… **JSON processing** and nested structures\n",
    "- âœ… **Error handling** and data validation\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Data Quality Joins\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ Scenario 1: Remove Duplicate Records\n",
    "\n",
    "## ğŸ“‹ Problem Statement\n",
    "\n",
    "### **ğŸ¯ Interview Question:**\n",
    "**\"Implement remove duplicate records in PySpark\"**\n",
    "\n",
    "### **ğŸ’¼ Business Context:**\n",
    "Handle duplicate data and ensure data uniqueness - common requirement in data engineering roles.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 1: Remove duplicate employees by name\n",
    "\n",
    "employee_data = [\n",
    "    (1, \"Jhon\", \"Testing\", 5000),\n",
    "    (2, \"Tim\", \"Development\", 6000),\n",
    "    (3, \"Jhon\", \"Development\", 5000),\n",
    "    (4, \"Sky\", \"Production\", 8000)\n",
    "]\n",
    "\n",
    "df_employees = spark.createDataFrame(employee_data, [\"id\", \"name\", \"dept\", \"salary\"])\n",
    "df_employees.show()\n",
    "\n",
    "# Remove duplicates based on name, keeping first occurrence\n",
    "df_unique = df_employees.dropDuplicates([\"name\"]).orderBy(\"id\")\n",
    "df_unique.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ Scenario 2: Complex Join Operations\n",
    "\n",
    "## ğŸ“‹ Problem Statement\n",
    "\n",
    "### **ğŸ¯ Interview Question:**\n",
    "**\"Implement complex join operations in PySpark\"**\n",
    "\n",
    "### **ğŸ’¼ Business Context:**\n",
    "Master different types of joins and their use cases - common requirement in data engineering roles.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 2: Outer join employee details\n",
    "\n",
    "emp_personal = [\n",
    "    (1, \"Tim\", 24, \"Kerala\", \"India\"),\n",
    "    (2, \"Asman\", 26, \"Kerala\", \"India\")\n",
    "]\n",
    "\n",
    "emp_address = [\n",
    "    (1, \"Tim\", 24, \"Comcity\"),\n",
    "    (2, \"Asman\", 26, \"bimcity\")\n",
    "]\n",
    "\n",
    "df_personal = spark.createDataFrame(emp_personal, [\"emp_id\", \"name\", \"age\", \"state\", \"country\"])\n",
    "df_address = spark.createDataFrame(emp_address, [\"emp_id\", \"name\", \"age\", \"address\"])\n",
    "\n",
    "df_personal.show()\n",
    "df_address.show()\n",
    "\n",
    "# Outer join on multiple columns\n",
    "df_joined = df_personal.join(df_address, [\"emp_id\", \"name\", \"age\"], \"outer\")\n",
    "df_joined.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ Scenario 3: Word Processing with UDFs\n",
    "\n",
    "## ğŸ“‹ Problem Statement\n",
    "\n",
    "### **ğŸ¯ Interview Question:**\n",
    "**\"Implement word processing with udfs in PySpark\"**\n",
    "\n",
    "### **ğŸ’¼ Business Context:**\n",
    "Implement custom functions for text processing - common requirement in data engineering roles.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 3: Reverse words in sentence using UDF\n",
    "\n",
    "sentence_data = [(\"The Social Dilemma\",)]\n",
    "\n",
    "df_input = spark.createDataFrame(sentence_data, [\"word\"])\n",
    "df_input.show()\n",
    "\n",
    "def reverse_sentence(sentence):\n",
    "    \"\"\"Reverse each word in the sentence\"\"\"\n",
    "    return \" \".join([word[::-1] for word in sentence.split(\" \")])\n",
    "\n",
    "# Register UDF\n",
    "reverse_udf = udf(reverse_sentence, StringType())\n",
    "\n",
    "# Apply UDF\n",
    "df_output = df_input.withColumn(\"reversed_word\", reverse_udf(\"word\")).drop(\"word\")\n",
    "df_output.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ Scenario 4: JSON Data Flattening\n",
    "\n",
    "## ğŸ“‹ Problem Statement\n",
    "\n",
    "### **ğŸ¯ Interview Question:**\n",
    "**\"Implement json data flattening in PySpark\"**\n",
    "\n",
    "### **ğŸ’¼ Business Context:**\n",
    "Transform nested JSON structures to flat tables - common requirement in data engineering roles.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 4: Flatten nested JSON data\n",
    "\n",
    "import json\n",
    "\n",
    "json_data = {\n",
    "    \"code\": 1234,\n",
    "    \"commentCount\": 5,\n",
    "    \"id\": 1,\n",
    "    \"name\": \"John Doe\",\n",
    "    \"likeDislike\": {\n",
    "        \"dislikes\": 10,\n",
    "        \"likes\": 20,\n",
    "        \"userAction\": 1\n",
    "    },\n",
    "    \"multiMedia\": [\n",
    "        {\n",
    "            \"id\": 1001,\n",
    "            \"name\": \"Media name\",\n",
    "            \"url\": \"https://example.com/media1\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": 1002,\n",
    "            \"name\": \"Another media name\",\n",
    "            \"url\": \"https://example.com/media2\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create DataFrame from JSON\n",
    "df_json = spark.read.json(spark.sparkContext.parallelize([json.dumps(json_data)]))\n",
    "df_json.printSchema()\n",
    "\n",
    "# Flatten nested structures\n",
    "df_flattened = df_json.withColumn(\"multiMedia\", explode(col(\"multiMedia\")))\\\n",
    "    .withColumn(\"dislikes\", col(\"likeDislike.dislikes\"))\\\n",
    "    .withColumn(\"likes\", col(\"likeDislike.likes\"))\\\n",
    "    .withColumn(\"userAction\", col(\"likeDislike.userAction\"))\\\n",
    "    .withColumn(\"media_id\", col(\"multiMedia.id\"))\\\n",
    "    .withColumn(\"media_name\", col(\"multiMedia.name\"))\\\n",
    "    .withColumn(\"media_url\", col(\"multiMedia.url\"))\\\n",
    "    .drop(\"likeDislike\", \"multiMedia\")\n",
    "\n",
    "df_flattened.printSchema()\n",
    "df_flattened.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ Scenario 5: Advanced JSON Restructuring\n",
    "\n",
    "## ğŸ“‹ Problem Statement\n",
    "\n",
    "### **ğŸ¯ Interview Question:**\n",
    "**\"Implement advanced json restructuring in PySpark\"**\n",
    "\n",
    "### **ğŸ’¼ Business Context:**\n",
    "Create complex nested data structures - common requirement in data engineering roles.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 5: Restructure complex JSON data\n",
    "\n",
    "json_list = [\n",
    "    {\n",
    "        \"id\": 1, \"name\": \"John\", \"likes\": 10, \"dislikes\": 2,\n",
    "        \"createAt\": \"2023-05-30T10:30:00\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 2, \"name\": \"Jane\", \"likes\": 15, \"dislikes\": 1,\n",
    "        \"createAt\": \"2023-05-31T11:00:00\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create DataFrame from JSON list\n",
    "df_json_list = spark.read.json(spark.sparkContext.parallelize([json.dumps(item) for item in json_list]))\n",
    "df_json_list.printSchema()\n",
    "df_json_list.show()\n",
    "\n",
    "# Restructure with nested structures\n",
    "df_restructured = df_json_list.select(\n",
    "    col(\"id\"),\n",
    "    col(\"name\"),\n",
    "    struct(col(\"dislikes\"), col(\"likes\"), col(\"userAction\")).alias(\"likeDislike\"),\n",
    "    array(struct(col(\"createAt\"), col(\"name\"), col(\"id\"))).alias(\"multiMedia\")\n",
    ")\n",
    "\n",
    "df_restructured.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ Scenario 6: Round Trip Calculations\n",
    "\n",
    "## ğŸ“‹ Problem Statement\n",
    "\n",
    "### **ğŸ¯ Interview Question:**\n",
    "**\"Implement round trip calculations in PySpark\"**\n",
    "\n",
    "### **ğŸ’¼ Business Context:**\n",
    "Complex join operations for route analysis - common requirement in data engineering roles.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 6: Calculate round trip distances\n",
    "\n",
    "trip_data = [\n",
    "    (\"SEA\", \"SF\", 300),\n",
    "    (\"CHI\", \"SEA\", 2000),\n",
    "    (\"SF\", \"SEA\", 300),\n",
    "    (\"SEA\", \"CHI\", 2000),\n",
    "    (\"SEA\", \"LND\", 500),\n",
    "    (\"LND\", \"SEA\", 500),\n",
    "    (\"LND\", \"CHI\", 1000),\n",
    "    (\"CHI\", \"NDL\", 180)\n",
    "]\n",
    "\n",
    "df_trips = spark.createDataFrame(trip_data, [\"from\", \"to\", \"dist\"])\n",
    "df_trips.show()\n",
    "\n",
    "# Method 1: SQL Approach\n",
    "df_trips.createOrReplaceTempView(\"trip\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT r1.\"from\", r1.\"to\", (r1.dist + r2.dist) AS roundtrip_dist\n",
    "    FROM trip r1\n",
    "    JOIN trip r2 ON r1.\"from\" = r2.\"to\" AND r1.\"to\" = r2.\"from\"\n",
    "    WHERE r1.\"from\" < r1.\"to\"\n",
    "\"\"\").show()\n",
    "\n",
    "# Method 2: DSL Approach\n",
    "df_roundtrip = df_trips.alias(\"r1\").join(\n",
    "    df_trips.alias(\"r2\"),\n",
    "    (col(\"r1.from\") == col(\"r2.to\")) & (col(\"r1.to\") == col(\"r2.from\")),\n",
    "    \"inner\"\n",
    ").where(col(\"r1.from\") < col(\"r1.to\"))\\\n",
    " .select(col(\"r1.from\"), col(\"r1.to\"), (col(\"r1.dist\") + col(\"r2.dist\")).alias(\"roundtrip_dist\"))\n",
    "\n",
    "df_roundtrip.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ Scenario 7: Running Totals with Window Functions\n",
    "\n",
    "## ğŸ“‹ Problem Statement\n",
    "\n",
    "### **ğŸ¯ Interview Question:**\n",
    "**\"Implement running totals with window functions in PySpark\"**\n",
    "\n",
    "### **ğŸ’¼ Business Context:**\n",
    "Calculate cumulative sums over partitions - common requirement in data engineering roles.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 7: Calculate running totals\n",
    "\n",
    "order_data = [\n",
    "    (1, \"26-May\", 100),\n",
    "    (1, \"27-May\", 200),\n",
    "    (1, \"28-May\", 300),\n",
    "    (2, \"29-May\", 400),\n",
    "    (3, \"30-May\", 500),\n",
    "    (3, \"31-May\", 600)\n",
    "]\n",
    "\n",
    "df_orders = spark.createDataFrame(order_data, [\"pid\", \"date\", \"price\"])\n",
    "df_orders.show()\n",
    "\n",
    "# Method 1: SQL Approach\n",
    "df_orders.createOrReplaceTempView(\"ordertab\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT pid, date, price,\n",
    "           SUM(price) OVER (PARTITION BY pid ORDER BY price) AS running_total\n",
    "    FROM ordertab\n",
    "\"\"\").show()\n",
    "\n",
    "# Method 2: DSL Approach\n",
    "window_spec = Window.partitionBy(\"pid\").orderBy(\"price\")\n",
    "df_with_running_total = df_orders.withColumn(\"running_total\", sum(\"price\").over(window_spec))\n",
    "df_with_running_total.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ Scenario 8: Customer Analysis\n",
    "\n",
    "## ğŸ“‹ Problem Statement\n",
    "\n",
    "### **ğŸ¯ Interview Question:**\n",
    "**\"Implement customer analysis in PySpark\"**\n",
    "\n",
    "### **ğŸ’¼ Business Context:**\n",
    "Multi-table joins for business intelligence - common requirement in data engineering roles.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 8: Find customers who bought all products\n",
    "\n",
    "purchase_data = [\n",
    "    (1, 5), (2, 6), (3, 5), (3, 6), (1, 6)\n",
    "]\n",
    "\n",
    "product_data = [(5,), (6,)]\n",
    "\n",
    "df_purchases = spark.createDataFrame(purchase_data, [\"customer_id\", \"product_key\"])\n",
    "df_products = spark.createDataFrame(product_data, [\"product_key\"])\n",
    "\n",
    "df_purchases.show()\n",
    "df_products.show()\n",
    "\n",
    "# Find customers who bought products (excluding customer 2)\n",
    "df_customer_products = df_purchases.join(df_products, \"product_key\", \"inner\")\\\n",
    "    .select(\"customer_id\", \"product_key\")\\\n",
    "    .distinct()\\\n",
    "    .where(col(\"customer_id\") != 2)\n",
    "\n",
    "df_customer_products.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ Scenario 9: Page Visit Analysis\n",
    "\n",
    "## ğŸ“‹ Problem Statement\n",
    "\n",
    "### **ğŸ¯ Interview Question:**\n",
    "**\"Implement page visit analysis in PySpark\"**\n",
    "\n",
    "### **ğŸ’¼ Business Context:**\n",
    "Array operations and collections - common requirement in data engineering roles.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 9: Collect user page visits\n",
    "\n",
    "page_visit_data = [\n",
    "    (1, \"home\"),\n",
    "    (1, \"products\"),\n",
    "    (1, \"checkout\"),\n",
    "    (1, \"confirmation\"),\n",
    "    (2, \"home\"),\n",
    "    (2, \"products\"),\n",
    "    (2, \"cart\"),\n",
    "    (2, \"checkout\"),\n",
    "    (2, \"confirmation\"),\n",
    "    (2, \"home\"),\n",
    "    (2, \"products\")\n",
    "]\n",
    "\n",
    "df_page_visits = spark.createDataFrame(page_visit_data, [\"userid\", \"page\"])\n",
    "df_page_visits.show()\n",
    "\n",
    "# Method 1: SQL Approach\n",
    "df_page_visits.createOrReplaceTempView(\"pagetab\")\n",
    "spark.sql(\"SELECT userid, COLLECT_LIST(page) AS pages FROM pagetab GROUP BY userid\").show()\n",
    "\n",
    "# Method 2: DSL Approac\n",
    "df_user_pages = df_page_visits.groupBy(\"userid\")\\\n",
    "    .agg(collect_list(\"page\").alias(\"pages\"))\n",
    "\n",
    "df_user_pages.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ Scenario 10: CSV Error Handling\n",
    "\n",
    "## ğŸ“‹ Problem Statement\n",
    "\n",
    "### **ğŸ¯ Interview Question:**\n",
    "**\"Implement csv error handling in PySpark\"**\n",
    "\n",
    "### **ğŸ’¼ Business Context:**\n",
    "Process malformed CSV data with error tolerance - common requirement in data engineering roles.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 10: Handle CSV with malformed records\n",
    "\n",
    "csv_data = \"\"\"emp_no,emp_name,dep\n",
    "101,Murugan,HealthCare\n",
    "Invalid Entry,Description: Bad Record Entry\n",
    "102,Kannan,Finance\n",
    "103,Mani,IT\"\"\"\n",
    "\n",
    "# Create DataFrame from inline CSV with error handling\n",
    "lines = csv_data.splitlines()\n",
    "rdd = spark.sparkContext.parallelize(lines)\n",
    "df_csv = spark.read.csv(rdd, header=True, mode=\"DROPMALFORMED\")\n",
    "df_csv.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ‰ Module Complete!\n",
    "\n",
    "## **ğŸ† Congratulations!**\n",
    "\n",
    "You have successfully completed the **Data Quality & Joins** interview scenarios module!\n",
    "\n",
    "### **ğŸ“Š What You Mastered:**\n",
    "- **Complex join strategies** for multi-table operations\n",
    "- **Data quality assurance** and error handling\n",
    "- **JSON processing** and nested data structures\n",
    "- **Advanced transformations** for data pipelines\n",
    "\n",
    "### **ğŸš€ Next Steps:**\n",
    "Ready for complex business logic? Try the **Advanced Business Logic** module!\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
