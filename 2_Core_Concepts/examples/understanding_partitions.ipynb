{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Partitions in PySpark\n",
    "\nPartitions are the fundamental unit of parallelism in PySpark - understanding them is crucial for performance optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are Partitions?\n",
    "\n**Partitions** are chunks of data that are distributed across the cluster. Each partition:\n",
    "- Is processed by one task\n",
    "- Can be on a different node\n",
    "- Enables parallel processing\n",
    "- Affects performance and memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder \\n",
    "    .appName(\"UnderstandingPartitions\") \\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "print(f\"Default parallelism: {sc.defaultParallelism}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating RDDs with Different Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RDD with default partitions\n",
    "numbers_default = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "print(f\"Default partitions: {numbers_default.getNumPartitions()}\")\n",
    "print(f\"Data: {numbers_default.collect()}\")\n",
    "\n",
    "# Create RDD with specific number of partitions\n",
    "numbers_3_parts = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 3)\n",
    "print(f\"3 partitions: {numbers_3_parts.getNumPartitions()}\")\n",
    "print(f\"Data: {numbers_3_parts.collect()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition Distribution\n",
    "\nLet's see how data is distributed across partitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to see partition contents\n",
    "def print_partition_data(partition_id, data):\n",
    "    print(f\"Partition {partition_id}: {list(data)}\")\n",
    "    return data\n",
    "\n",
    "# Map partitions to see distribution\n",
    "numbers_3_parts.mapPartitions(lambda x: [list(x)]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better way to inspect partitions\n",
    "def inspect_partition(partition_index, data):\n",
    "    data_list = list(data)\n",
    "    print(f\"Partition {partition_index}: {data_list} (size: {len(data_list)})\")\n",
    "    return data_list\n",
    "\n",
    "# Use foreachPartition to inspect\n",
    "print(\"Partition inspection:\")\n",
    "numbers_3_parts.foreachPartition(lambda x: print(f\"Partition content: {list(x)}\"))\n",
    "\n",
    "# To see results, we need to use a different approach\n",
    "partitioned_data = numbers_3_parts.mapPartitionsWithIndex(inspect_partition).collect()\n",
    "print(\"\nCollected results show partition processing order\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impact of Partition Count\n",
    "\nDifferent partition counts affect performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Create larger dataset\n",
    "large_data = list(range(1, 10001))  # 1 to 10000\n",
    "\n",
    "# Test with different partition counts\n",
    "for num_partitions in [1, 4, 8, 16]:\n",
    "    rdd = sc.parallelize(large_data, num_partitions)\n",
    "    start_time = time.time()\n",
    "    count = rdd.map(lambda x: x * 2).reduce(lambda a, b: a + b)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"Partitions: {num_partitions}, Time: {end_time - start_time:.3f}s, Result: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repartitioning Operations\n",
    "\nChanging partition count affects data distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with 2 partitions\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 2)\n",
    "print(f\"Original partitions: {rdd.getNumPartitions()}\")\n",
    "\n",
    "# repartition() - full shuffle\n",
    "repartitioned = rdd.repartition(4)\n",
    "print(f\"After repartition(4): {repartitioned.getNumPartitions()}\")\n",
    "\n",
    "# coalesce() - reduce partitions without full shuffle\n",
    "coalesced = repartitioned.coalesce(2)\n",
    "print(f\"After coalesce(2): {coalesced.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitioning Best Practices\n",
    "\n### When to increase partitions:\n",
    "- Large datasets\n",
    "- CPU-intensive operations\n",
    "- Available cluster resources\n",
    "\n### When to decrease partitions:\n",
    "- Small datasets\n",
    "- I/O intensive operations\n",
    "- Memory constraints\n",
    "\n### Optimal partition size:\n",
    "- 100-200MB per partition (uncompressed)\n",
    "- 2-4x number of cores\n",
    "- Balance between parallelism and overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Optimal partitioning\n",
    "large_dataset = list(range(1, 100001))  # 100k elements\n",
    "\n",
    "# Calculate optimal partitions based on data size\n",
    "data_size_mb = len(large_dataset) * 8 / (1024 * 1024)  # Rough estimate\n",
    "optimal_partitions = max(2, int(data_size_mb / 128))  # ~128MB per partition\n",
    "\n",
    "print(f\"Data size estimate: {data_size_mb:.1f}MB\")\n",
    "print(f\"Suggested partitions: {optimal_partitions}\")\n",
    "\n",
    "# Create RDD with optimal partitioning\n",
    "optimal_rdd = sc.parallelize(large_dataset, optimal_partitions)\n",
    "print(f\"Actual partitions: {optimal_rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n1. **Partitions = Parallelism**: More partitions = more parallel tasks\n",
    "2. **Size Matters**: Too small = overhead, too large = memory issues\n",
    "3. **Shuffle Operations**: repartition() causes full shuffle, coalesce() is more efficient\n",
    "4. **Monitor Performance**: Use Spark UI to understand partition distribution\n",
    "5. **Tune for Workload**: Different workloads need different partition strategies\n",
    "\n**Remember**: Understanding partitions is key to optimizing PySpark performance!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}