{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ¯ Spark SQL Basics: SQL Interface for DataFrames\n",
    "\n",
    "**Time to complete:** 35 minutes  \n",
    "**Difficulty:** Intermediate  \n",
    "**Prerequisites:** DataFrame basics, SQL knowledge\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will master:\n",
    "- âœ… **Spark SQL fundamentals** - SQL queries on DataFrames\n",
    "- âœ… **Creating and using views** - Temporary and global views\n",
    "- âœ… **SQL functions** - Built-in SQL functions and operations\n",
    "- âœ… **Complex queries** - Joins, subqueries, CTEs\n",
    "- âœ… **Integration patterns** - SQL + DataFrame API mixing\n",
    "- âœ… **Performance considerations** - When to use SQL vs DataFrames\n",
    "\n",
    "**Spark SQL brings familiar SQL syntax to distributed data processing!**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ” Understanding Spark SQL\n",
    "\n",
    "**Spark SQL** provides a SQL interface to work with structured data in Spark. It allows you to:\n",
    "\n",
    "- **Write SQL queries** on DataFrames and RDDs\n",
    "- **Create databases and tables** in Spark\n",
    "- **Use familiar SQL syntax** for data manipulation\n",
    "- **Mix SQL and DataFrame API** in the same application\n",
    "- **Access data sources** through SQL\n",
    "\n",
    "### Architecture:\n",
    "```\n",
    "SQL Query â†’ Catalyst Optimizer â†’ Logical Plan â†’ Physical Plan â†’ Execution\n",
    "```\n",
    "\n",
    "**Spark SQL uses the same execution engine as DataFrame API - they're interchangeable!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, lit, round as round_func\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark_SQL_Basics\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"âœ… Spark ready - Version: {spark.version}\")\n",
    "\n",
    "# Enable Spark SQL\n",
    "spark.sql(\"SET spark.sql.adaptive.enabled = true\")\n",
    "print(\"Spark SQL optimizations enabled\")\n",
    "\n",
    "# Create sample datasets\n",
    "employees_data = [\n",
    "    (1, \"Alice\", \"Engineering\", 75000, \"2023-01-15\"),\n",
    "    (2, \"Bob\", \"Sales\", 65000, \"2023-02-20\"),\n",
    "    (3, \"Charlie\", \"Engineering\", 85000, \"2023-03-10\"),\n",
    "    (4, \"Diana\", \"HR\", 55000, \"2023-04-05\"),\n",
    "    (5, \"Eve\", \"Sales\", 70000, \"2023-05-12\"),\n",
    "    (6, \"Frank\", \"Finance\", 80000, \"2023-06-18\"),\n",
    "    (7, \"Grace\", \"Engineering\", 78000, \"2023-07-22\"),\n",
    "    (8, \"Henry\", \"Sales\", 72000, \"2023-08-30\")\n",
    "]\n",
    "\n",
    "departments_data = [\n",
    "    (\"Engineering\", \"Building software\", \"New York\", 10),\n",
    "    (\"Sales\", \"Selling products\", \"Chicago\", 8),\n",
    "    (\"HR\", \"Managing people\", \"Boston\", 3),\n",
    "    (\"Finance\", \"Financial management\", \"New York\", 5),\n",
    "    (\"Marketing\", \"Brand promotion\", \"Los Angeles\", 6)\n",
    "]\n",
    "\n",
    "projects_data = [\n",
    "    (101, \"Website Redesign\", \"Engineering\", 3, \"2023-12-01\"),\n",
    "    (102, \"Sales CRM\", \"Sales\", 2, \"2023-11-15\"),\n",
    "    (103, \"Talent Acquisition\", \"HR\", 1, \"2023-10-20\"),\n",
    "    (104, \"Budget Planning\", \"Finance\", 2, \"2023-09-10\"),\n",
    "    (105, \"Brand Campaign\", \"Marketing\", 4, \"2023-08-25\")\n",
    "]\n",
    "\n",
    "# Create DataFrames\n",
    "employees_df = spark.createDataFrame(employees_data, \n",
    "    [\"emp_id\", \"name\", \"department\", \"salary\", \"hire_date\"])\n",
    "\n",
    "departments_df = spark.createDataFrame(departments_data, \n",
    "    [\"dept_name\", \"description\", \"location\", \"team_size\"])\n",
    "\n",
    "projects_df = spark.createDataFrame(projects_data, \n",
    "    [\"project_id\", \"project_name\", \"department\", \"priority\", \"deadline\"])\n",
    "\n",
    "print(\"ðŸ“Š Sample DataFrames:\")\n",
    "print(\"\\nEmployees:\")\n",
    "employees_df.show()\n",
    "\n",
    "print(\"Departments:\")\n",
    "departments_df.show()\n",
    "\n",
    "print(\"Projects:\")\n",
    "projects_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Creating Views and Running SQL Queries\n",
    "\n",
    "### Creating Temporary Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporary views for SQL queries\n",
    "print(\"ðŸŽ¯ CREATING VIEWS AND SQL QUERIES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create temporary views (available in current session)\n",
    "employees_df.createOrReplaceTempView(\"employees\")\n",
    "departments_df.createOrReplaceTempView(\"departments\")\n",
    "projects_df.createOrReplaceTempView(\"projects\")\n",
    "\n",
    "print(\"âœ… Temporary views created\")\n",
    "\n",
    "# List available tables\n",
    "tables_df = spark.sql(\"SHOW TABLES\")\n",
    "print(\"\\nAvailable tables:\")\n",
    "tables_df.show()\n",
    "\n",
    "# Basic SQL SELECT query\n",
    "basic_query = spark.sql(\"\"\"\n",
    "    SELECT emp_id, name, department, salary\n",
    "    FROM employees\n",
    "    WHERE salary > 70000\n",
    "    ORDER BY salary DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nBasic SQL query (salary > 70k):\")\n",
    "basic_query.show()\n",
    "\n",
    "# SQL with aggregations\n",
    "agg_query = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        department,\n",
    "        COUNT(*) as employee_count,\n",
    "        AVG(salary) as avg_salary,\n",
    "        MAX(salary) as max_salary,\n",
    "        MIN(salary) as min_salary\n",
    "    FROM employees\n",
    "    GROUP BY department\n",
    "    ORDER BY avg_salary DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nSQL aggregation by department:\")\n",
    "agg_query.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL Functions and Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL functions and expressions\n",
    "print(\"ðŸ§® SQL FUNCTIONS AND EXPRESSIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# String functions\n",
    "string_functions = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        name,\n",
    "        UPPER(name) as upper_name,\n",
    "        LOWER(name) as lower_name,\n",
    "        LENGTH(name) as name_length,\n",
    "        SUBSTRING(name, 1, 3) as first_3_chars,\n",
    "        CONCAT(name, ' (', department, ')') as full_name\n",
    "    FROM employees\n",
    "\"\"\")\n",
    "\n",
    "print(\"String functions:\")\n",
    "string_functions.show()\n",
    "\n",
    "# Mathematical and conditional functions\n",
    "math_functions = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        name,\n",
    "        salary,\n",
    "        ROUND(salary * 0.10, 2) as bonus,\n",
    "        salary * 1.10 as salary_with_bonus,\n",
    "        CASE \n",
    "            WHEN salary >= 80000 THEN 'High'\n",
    "            WHEN salary >= 60000 THEN 'Medium'\n",
    "            ELSE 'Entry'\n",
    "        END as salary_tier,\n",
    "        YEAR(hire_date) as hire_year,\n",
    "        MONTH(hire_date) as hire_month\n",
    "    FROM employees\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nMathematical and conditional functions:\")\n",
    "math_functions.show()\n",
    "\n",
    "# Window functions in SQL\n",
    "window_functions = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        name,\n",
    "        department,\n",
    "        salary,\n",
    "        ROW_NUMBER() OVER (PARTITION BY department ORDER BY salary DESC) as dept_rank,\n",
    "        RANK() OVER (ORDER BY salary DESC) as overall_rank,\n",
    "        SUM(salary) OVER (PARTITION BY department) as dept_total_salary,\n",
    "        AVG(salary) OVER (PARTITION BY department) as dept_avg_salary\n",
    "    FROM employees\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nWindow functions in SQL:\")\n",
    "window_functions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”— SQL Joins\n",
    "\n",
    "### Inner, Left, and Other Join Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL joins\n",
    "print(\"ðŸ”— SQL JOINS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Inner join\n",
    "inner_join = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        e.name,\n",
    "        e.department,\n",
    "        e.salary,\n",
    "        d.description,\n",
    "        d.location,\n",
    "        d.team_size\n",
    "    FROM employees e\n",
    "    INNER JOIN departments d ON e.department = d.dept_name\n",
    "    ORDER BY e.salary DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"Inner join (employees + departments):\")\n",
    "inner_join.show()\n",
    "\n",
    "# Left join\n",
    "left_join = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        e.name,\n",
    "        e.department as emp_dept,\n",
    "        e.salary,\n",
    "        d.dept_name,\n",
    "        d.location,\n",
    "        d.team_size\n",
    "    FROM employees e\n",
    "    LEFT JOIN departments d ON e.department = d.dept_name\n",
    "    ORDER BY e.name\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nLeft join (all employees, matching departments):\")\n",
    "left_join.show()\n",
    "\n",
    "# Find employees without departments\n",
    "orphaned_employees = spark.sql(\"\"\"\n",
    "    SELECT e.name, e.department, e.salary\n",
    "    FROM employees e\n",
    "    LEFT JOIN departments d ON e.department = d.dept_name\n",
    "    WHERE d.dept_name IS NULL\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nEmployees without departments:\")\n",
    "orphaned_employees.show()\n",
    "\n",
    "# Right join\n",
    "right_join = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        e.name,\n",
    "        d.dept_name,\n",
    "        d.description,\n",
    "        d.location\n",
    "    FROM employees e\n",
    "    RIGHT JOIN departments d ON e.department = d.dept_name\n",
    "    ORDER BY d.dept_name\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nRight join (all departments, matching employees):\")\n",
    "right_join.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complex Multi-Table Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex multi-table joins\n",
    "print(\"ðŸ”€ COMPLEX MULTI-TABLE JOINS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Three-way join\n",
    "three_way_join = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        e.name as employee_name,\n",
    "        e.salary,\n",
    "        d.dept_name,\n",
    "        d.location,\n",
    "        p.project_name,\n",
    "        p.priority,\n",
    "        p.deadline\n",
    "    FROM employees e\n",
    "    INNER JOIN departments d ON e.department = d.dept_name\n",
    "    LEFT JOIN projects p ON e.department = p.department\n",
    "    ORDER BY e.salary DESC, p.priority ASC\n",
    "\"\"\")\n",
    "\n",
    "print(\"Three-way join (employees â†’ departments â†’ projects):\")\n",
    "three_way_join.show()\n",
    "\n",
    "# Subqueries\n",
    "subquery_example = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        dept_name,\n",
    "        location,\n",
    "        team_size,\n",
    "        avg_salary\n",
    "    FROM (\n",
    "        SELECT \n",
    "            department,\n",
    "            AVG(salary) as avg_salary,\n",
    "            COUNT(*) as emp_count\n",
    "        FROM employees\n",
    "        GROUP BY department\n",
    "    ) dept_stats\n",
    "    INNER JOIN departments d ON dept_stats.department = d.dept_name\n",
    "    WHERE dept_stats.avg_salary > 65000\n",
    "    ORDER BY dept_stats.avg_salary DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nSubquery example (departments with high avg salary):\")\n",
    "subquery_example.show()\n",
    "\n",
    "# CTE (Common Table Expression)\n",
    "cte_example = spark.sql(\"\"\"\n",
    "    WITH dept_stats AS (\n",
    "        SELECT \n",
    "            department,\n",
    "            COUNT(*) as employee_count,\n",
    "            AVG(salary) as avg_salary,\n",
    "            MAX(salary) as max_salary\n",
    "        FROM employees\n",
    "        GROUP BY department\n",
    "    ),\n",
    "    high_performers AS (\n",
    "        SELECT \n",
    "            name,\n",
    "            department,\n",
    "            salary\n",
    "        FROM employees\n",
    "        WHERE salary > 70000\n",
    "    )\n",
    "    SELECT \n",
    "        ds.department,\n",
    "        ds.employee_count,\n",
    "        ROUND(ds.avg_salary, 2) as avg_salary,\n",
    "        hp.name as top_employee,\n",
    "        hp.salary as top_salary\n",
    "    FROM dept_stats ds\n",
    "    LEFT JOIN high_performers hp ON ds.department = hp.department\n",
    "    ORDER BY ds.avg_salary DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nCTE example (department stats with top performers):\")\n",
    "cte_example.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¨ Mixing SQL and DataFrame API\n",
    "\n",
    "### SQL + DataFrame Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixing SQL and DataFrame API\n",
    "print(\"ðŸŽ¨ MIXING SQL AND DATAFRAME API\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create DataFrame using SQL, then use DataFrame API\n",
    "sql_result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        department,\n",
    "        name,\n",
    "        salary,\n",
    "        ROW_NUMBER() OVER (PARTITION BY department ORDER BY salary DESC) as dept_rank\n",
    "    FROM employees\n",
    "\"\"\")\n",
    "\n",
    "# Continue with DataFrame API\n",
    "mixed_result = sql_result \\\n",
    "    .filter(col(\"dept_rank\") <= 2) \\\n",
    "    .withColumn(\"salary_category\", \n",
    "                when(col(\"salary\") >= 75000, \"High\")\n",
    "                .otherwise(\"Regular\")) \\\n",
    "    .groupBy(\"department\", \"salary_category\") \\\n",
    "    .agg(F.count(\"name\").alias(\"count\"), F.avg(\"salary\").alias(\"avg_salary\")) \\\n",
    "    .orderBy(\"department\", \"salary_category\")\n",
    "\n",
    "print(\"SQL query + DataFrame API processing:\")\n",
    "mixed_result.show()\n",
    "\n",
    "# Create DataFrame with API, register as view, then query with SQL\n",
    "api_df = employees_df \\\n",
    "    .withColumn(\"experience_level\", \n",
    "                when(col(\"salary\") >= 80000, \"Senior\")\n",
    "                .when(col(\"salary\") >= 60000, \"Mid\")\n",
    "                .otherwise(\"Junior\")) \\\n",
    "    .withColumn(\"annual_bonus\", col(\"salary\") * 0.15)\n",
    "\n",
    "# Register as temporary view\n",
    "api_df.createOrReplaceTempView(\"enriched_employees\")\n",
    "\n",
    "# Query with SQL\n",
    "sql_on_api_df = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        experience_level,\n",
    "        COUNT(*) as count,\n",
    "        ROUND(AVG(salary), 2) as avg_salary,\n",
    "        ROUND(AVG(annual_bonus), 2) as avg_bonus,\n",
    "        ROUND(SUM(salary + annual_bonus), 2) as total_compensation\n",
    "    FROM enriched_employees\n",
    "    GROUP BY experience_level\n",
    "    ORDER BY avg_salary DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nDataFrame API processing + SQL aggregation:\")\n",
    "sql_on_api_df.show()\n",
    "\n",
    "# Dynamic SQL generation\n",
    "def generate_dynamic_sql(dept_filter=None):\n",
    "    base_query = \"\"\"\n",
    "        SELECT department, COUNT(*) as count, AVG(salary) as avg_salary\n",
    "        FROM employees\n",
    "    \"\"\"\n",
    "    \n",
    "    if dept_filter:\n",
    "        query = f\"{base_query} WHERE department = '{dept_filter}' GROUP BY department\"\n",
    "    else:\n",
    "        query = f\"{base_query} GROUP BY department\"\n",
    "    \n",
    "    return spark.sql(query)\n",
    "\n",
    "print(\"\\nDynamic SQL generation:\")\n",
    "generate_dynamic_sql().show()\n",
    "print(\"\\nFiltered for Engineering:\")\n",
    "generate_dynamic_sql(\"Engineering\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Advanced SQL Features\n",
    "\n",
    "### Complex Queries and Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced SQL features\n",
    "print(\"ðŸ“Š ADVANCED SQL FEATURES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Complex analytics query\n",
    "analytics_query = spark.sql(\"\"\"\n",
    "    WITH employee_stats AS (\n",
    "        SELECT \n",
    "            department,\n",
    "            COUNT(*) as emp_count,\n",
    "            AVG(salary) as avg_salary,\n",
    "            STDDEV(salary) as salary_stddev,\n",
    "            MIN(salary) as min_salary,\n",
    "            MAX(salary) as max_salary\n",
    "        FROM employees\n",
    "        GROUP BY department\n",
    "    ),\n",
    "    department_projects AS (\n",
    "        SELECT \n",
    "            department,\n",
    "            COUNT(*) as project_count,\n",
    "            AVG(priority) as avg_priority,\n",
    "            MAX(deadline) as latest_deadline\n",
    "        FROM projects\n",
    "        GROUP BY department\n",
    "    )\n",
    "    SELECT \n",
    "        COALESCE(es.department, dp.department) as department,\n",
    "        es.emp_count,\n",
    "        ROUND(es.avg_salary, 2) as avg_salary,\n",
    "        ROUND(es.salary_stddev, 2) as salary_variation,\n",
    "        dp.project_count,\n",
    "        ROUND(dp.avg_priority, 1) as avg_project_priority,\n",
    "        dp.latest_deadline,\n",
    "        CASE \n",
    "            WHEN es.avg_salary > 75000 THEN 'High Cost'\n",
    "            WHEN es.avg_salary > 60000 THEN 'Medium Cost'\n",
    "            ELSE 'Low Cost'\n",
    "        END as cost_category,\n",
    "        CASE \n",
    "            WHEN dp.project_count > 0 THEN 'Active'\n",
    "            ELSE 'Inactive'\n",
    "        END as project_status\n",
    "    FROM employee_stats es\n",
    "    FULL OUTER JOIN department_projects dp ON es.department = dp.department\n",
    "    ORDER BY es.avg_salary DESC NULLS LAST\n",
    "\"\"\")\n",
    "\n",
    "print(\"Advanced analytics query (employees + projects):\")\n",
    "analytics_query.show(truncate=False)\n",
    "\n",
    "# Pivot-like operations\n",
    "pivot_query = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        department,\n",
    "        COUNT(CASE WHEN salary >= 80000 THEN 1 END) as high_earners,\n",
    "        COUNT(CASE WHEN salary BETWEEN 60000 AND 79999 THEN 1 END) as mid_earners,\n",
    "        COUNT(CASE WHEN salary < 60000 THEN 1 END) as entry_earners,\n",
    "        ROUND(AVG(CASE WHEN salary >= 80000 THEN salary END), 2) as avg_high_salary,\n",
    "        ROUND(AVG(CASE WHEN salary < 60000 THEN salary END), 2) as avg_entry_salary\n",
    "    FROM employees\n",
    "    GROUP BY department\n",
    "    ORDER BY department\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nPivot-style analysis by salary ranges:\")\n",
    "pivot_query.show()\n",
    "\n",
    "# Time-based analytics\n",
    "time_analysis = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        YEAR(hire_date) as hire_year,\n",
    "        MONTH(hire_date) as hire_month,\n",
    "        COUNT(*) as hires,\n",
    "        ROUND(AVG(salary), 2) as avg_salary,\n",
    "        COUNT(DISTINCT department) as departments_hiring,\n",
    "        SUM(COUNT(*)) OVER (ORDER BY YEAR(hire_date), MONTH(hire_date)) as cumulative_hires\n",
    "    FROM employees\n",
    "    GROUP BY YEAR(hire_date), MONTH(hire_date)\n",
    "    ORDER BY hire_year, hire_month\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nTime-based hiring analytics:\")\n",
    "time_analysis.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš¡ Performance Considerations\n",
    "\n",
    "### SQL vs DataFrame API Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance considerations\n",
    "print(\"âš¡ SQL VS DATAFRAME API PERFORMANCE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create larger dataset for performance testing\n",
    "large_data = [\n",
    "    (i, f\"Employee_{i}\", f\"Dept_{(i%10)+1}\", 50000 + (i % 50000), f\"2023-{(i%12)+1:02d}-01\")\n",
    "    for i in range(50000)\n",
    "]\n",
    "\n",
    "large_df = spark.createDataFrame(large_data, \n",
    "    [\"emp_id\", \"name\", \"department\", \"salary\", \"hire_date\"])\n",
    "large_df.createOrReplaceTempView(\"large_employees\")\n",
    "\n",
    "print(f\"Large dataset: {large_df.count():,} rows\")\n",
    "\n",
    "# Test 1: Simple filtering\n",
    "print(\"\\n=== Test 1: Simple Filtering ===\")\n",
    "\n",
    "# SQL approach\n",
    "import time\n",
    "start_time = time.time()\n",
    "sql_result = spark.sql(\"\"\"\n",
    "    SELECT * FROM large_employees \n",
    "    WHERE salary > 75000 AND department = 'Dept_1'\n",
    "\"\"\")\n",
    "sql_count = sql_result.count()\n",
    "sql_time = time.time() - start_time\n",
    "\n",
    "# DataFrame API approach\n",
    "start_time = time.time()\n",
    "df_result = large_df.filter((col(\"salary\") > 75000) & (col(\"department\") == \"Dept_1\"))\n",
    "df_count = df_result.count()\n",
    "df_time = time.time() - start_time\n",
    "\n",
    "print(f\"SQL approach: {sql_time:.3f} seconds ({sql_count} rows)\")\n",
    "print(f\"DataFrame API: {df_time:.3f} seconds ({df_count} rows)\")\n",
    "print(f\"Results match: {sql_count == df_count}\")\n",
    "\n",
    "# Test 2: Complex aggregation\n",
    "print(\"\\n=== Test 2: Complex Aggregation ===\")\n",
    "\n",
    "# SQL approach\n",
    "start_time = time.time()\n",
    "sql_agg = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        department,\n",
    "        COUNT(*) as emp_count,\n",
    "        AVG(salary) as avg_salary,\n",
    "        MAX(salary) as max_salary,\n",
    "        COUNT(CASE WHEN salary > 75000 THEN 1 END) as high_earners\n",
    "    FROM large_employees\n",
    "    GROUP BY department\n",
    "    HAVING COUNT(*) > 4000\n",
    "    ORDER BY avg_salary DESC\n",
    "\"\"\")\n",
    "sql_agg_count = sql_agg.count()\n",
    "sql_agg_time = time.time() - start_time\n",
    "\n",
    "# DataFrame API approach\n",
    "start_time = time.time()\n",
    "df_agg = large_df.groupBy(\"department\").agg(\n",
    "    F.count(\"*\").alias(\"emp_count\"),\n",
    "    F.avg(\"salary\").alias(\"avg_salary\"),\n",
    "    F.max(\"salary\").alias(\"max_salary\"),\n",
    "    F.sum(when(col(\"salary\") > 75000, 1).otherwise(0)).alias(\"high_earners\")\n",
    ").filter(col(\"emp_count\") > 4000).orderBy(col(\"avg_salary\").desc())\n",
    "df_agg_count = df_agg.count()\n",
    "df_agg_time = time.time() - start_time\n",
    "\n",
    "print(f\"SQL aggregation: {sql_agg_time:.3f} seconds ({sql_agg_count} groups)\")\n",
    "print(f\"DataFrame agg: {df_agg_time:.3f} seconds ({df_agg_count} groups)\")\n",
    "\n",
    "# Performance insights\n",
    "print(\"\\nðŸ“Š PERFORMANCE INSIGHTS:\")\n",
    "print(\"- SQL and DataFrame API use the same execution engine\")\n",
    "print(\"- Performance differences are usually negligible\")\n",
    "print(\"- Choose based on team preference and use case\")\n",
    "print(\"- SQL is often more readable for complex joins\")\n",
    "print(\"- DataFrame API is better for programmatic operations\")\n",
    "print(\"- Catalyst optimizer handles both equally well\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš¨ Common Mistakes and Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common mistakes and best practices\n",
    "print(\"ðŸš¨ COMMON MISTAKES AND BEST PRACTICES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Mistake 1: Not creating views before querying\n",
    "print(\"âŒ Mistake: Querying DataFrames without views\")\n",
    "try:\n",
    "    # This won't work - employees_df is not a table\n",
    "    bad_query = spark.sql(\"SELECT * FROM employees_df\")\n",
    "    bad_query.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)[:80]}...\")\n",
    "\n",
    "print(\"\\nâœ… Solution: Create views first\")\n",
    "employees_df.createOrReplaceTempView(\"employees_table\")\n",
    "good_query = spark.sql(\"SELECT * FROM employees_table LIMIT 3\")\n",
    "good_query.show()\n",
    "\n",
    "# Mistake 2: SQL injection vulnerability\n",
    "print(\"\\nâŒ Mistake: SQL injection in dynamic queries\")\n",
    "def unsafe_query(user_input):\n",
    "    # DON'T DO THIS - vulnerable to SQL injection\n",
    "    return spark.sql(f\"SELECT * FROM employees WHERE name = '{user_input}'\")\n",
    "\n",
    "print(\"\\nâœ… Solution: Use parameterized queries or DataFrame API\")\n",
    "def safe_query(name_filter):\n",
    "    return employees_df.filter(col(\"name\") == name_filter)\n",
    "\n",
    "safe_query(\"Alice\").show()\n",
    "\n",
    "# Mistake 3: Forgetting NULL handling in SQL\n",
    "print(\"\\nâŒ Mistake: NULL handling in SQL\")\n",
    "null_query = spark.sql(\"\"\"\n",
    "    SELECT department, AVG(salary) as avg_salary\n",
    "    FROM employees\n",
    "    GROUP BY department\n",
    "\"\"\")\n",
    "print(\"Query works but NULLs are ignored in AVG:\")\n",
    "null_query.show()\n",
    "\n",
    "# Mistake 4: Performance - not using EXPLAIN\n",
    "print(\"\\nâŒ Mistake: Not checking query plans\")\n",
    "print(\"Always use EXPLAIN to understand execution plans:\")\n",
    "\n",
    "explain_query = spark.sql(\"\"\"\n",
    "    SELECT e.name, d.location\n",
    "    FROM employees e\n",
    "    INNER JOIN departments d ON e.department = d.dept_name\n",
    "    WHERE e.salary > 60000\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nQuery execution plan:\")\n",
    "explain_query.explain(mode=\"formatted\")\n",
    "\n",
    "# Best practices summary\n",
    "print(\"\\nðŸŽ¯ SQL BEST PRACTICES:\")\n",
    "print(\"1. Create views before complex queries\")\n",
    "print(\"2. Use CTEs for complex multi-step logic\")\n",
    "print(\"3. Always check execution plans with EXPLAIN\")\n",
    "print(\"4. Handle NULLs explicitly in aggregations\")\n",
    "print(\"5. Use appropriate join types for your use case\")\n",
    "print(\"6. Mix SQL and DataFrame API when beneficial\")\n",
    "print(\"7. Validate query results against expectations\")\n",
    "print(\"8. Use descriptive aliases for complex queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Key Takeaways\n",
    "\n",
    "### What You Learned:\n",
    "- âœ… **`spark.sql()`** - Execute SQL queries on DataFrames\n",
    "- âœ… **`createOrReplaceTempView()`** - Register DataFrames as SQL tables\n",
    "- âœ… **SQL functions** - String, math, date, and aggregate functions\n",
    "- âœ… **Joins in SQL** - INNER, LEFT, RIGHT, FULL OUTER joins\n",
    "- âœ… **Complex queries** - CTEs, subqueries, window functions\n",
    "- âœ… **SQL + DataFrame mixing** - Best of both worlds\n",
    "- âœ… **Performance optimization** - SQL vs DataFrame API comparison\n",
    "\n",
    "### SQL Capabilities in Spark:\n",
    "- ðŸ”¸ **Standard SQL** - SELECT, FROM, WHERE, GROUP BY, HAVING, ORDER BY\n",
    "- ðŸ”¸ **Joins** - All major join types (INNER, LEFT, RIGHT, FULL)\n",
    "- ðŸ”¸ **Subqueries** - Nested queries and CTEs\n",
    "- ðŸ”¸ **Window functions** - ROW_NUMBER, RANK, SUM OVER, etc.\n",
    "- ðŸ”¸ **Built-in functions** - 200+ SQL functions available\n",
    "- ðŸ”¸ **User-defined functions** - Custom SQL functions\n",
    "\n",
    "### When to Use SQL vs DataFrame API:\n",
    "- ðŸ”¸ **Use SQL when**: Team prefers SQL, complex joins, reporting queries\n",
    "- ðŸ”¸ **Use DataFrame API when**: Programmatic logic, complex transformations, UDFs\n",
    "- ðŸ”¸ **Mix both when**: SQL for data access, DataFrame for processing\n",
    "\n",
    "### Performance Notes:\n",
    "- ðŸ”¸ **Same execution engine** - SQL and DataFrame API performance is identical\n",
    "- ðŸ”¸ **Catalyst optimization** - Both benefit from the same query optimizer\n",
    "- ðŸ”¸ **Choose based on preference** - Use what your team knows best\n",
    "\n",
    "### Common Patterns:\n",
    "- ðŸ”¸ `spark.sql(\"SELECT ... FROM table\")` - Basic SQL query\n",
    "- ðŸ”¸ `df.createOrReplaceTempView(\"table\")` - Register DataFrame as table\n",
    "- ðŸ”¸ `spark.sql(\"CREATE TEMP VIEW view_name AS SELECT ...\")` - Create views\n",
    "- ðŸ”¸ `WITH cte_name AS (...) SELECT ... FROM cte_name` - CTE pattern\n",
    "- ðŸ”¸ `spark.sql(query).toPandas()` - Convert results to Pandas\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸš€ Next Steps\n",
    "\n",
    "Now that you master Spark SQL basics, you're ready for:\n",
    "\n",
    "1. **Temporary vs Global Views** - View management and persistence\n",
    "2. **SQL vs DataFrame API** - When to use each approach\n",
    "3. **SQL Optimization** - Advanced query optimization techniques\n",
    "4. **Integration Patterns** - SQL in production Spark applications\n",
    "\n",
    "**Spark SQL brings the power of SQL to distributed computing!**\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸŽ‰ Congratulations! You now have SQL superpowers in Spark!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
