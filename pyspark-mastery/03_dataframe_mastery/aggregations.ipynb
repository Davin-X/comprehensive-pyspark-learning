{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä DataFrame Aggregations: GroupBy & Statistics\n",
    "\n",
    "**Time to complete:** 35 minutes  \n",
    "**Difficulty:** Intermediate  \n",
    "**Prerequisites:** DataFrame basics, column expressions\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will master:\n",
    "- ‚úÖ **`groupBy()`** - Group data by columns\n",
    "- ‚úÖ **`agg()`** - Apply aggregation functions\n",
    "- ‚úÖ **Statistical functions** - sum, avg, min, max, count\n",
    "- ‚úÖ **Multiple aggregations** - Combine multiple stats\n",
    "- ‚úÖ **Custom aggregations** - User-defined aggregation logic\n",
    "- ‚úÖ **Performance optimization** - Efficient grouping strategies\n",
    "\n",
    "**Aggregations are the heart of data analysis in Spark!**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Understanding DataFrame Aggregations\n",
    "\n",
    "**Aggregations** combine multiple rows into summary statistics. They're essential for:\n",
    "\n",
    "- **Business Intelligence**: Sales by region, revenue by product\n",
    "- **Data Analysis**: Average age by department, count by category\n",
    "- **Reporting**: Statistical summaries and KPIs\n",
    "- **Data Quality**: Checking distributions and outliers\n",
    "\n",
    "### Aggregation Flow:\n",
    "```\n",
    "Raw Data ‚Üí Group By ‚Üí Aggregate Functions ‚Üí Summary Results\n",
    "```\n",
    "\n",
    "**GroupBy operations are wide transformations** - they cause data shuffling!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, avg, min, max, count, stddev, variance\n",
    "from pyspark.sql.functions import countDistinct, approx_count_distinct, first, last\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataFrame_Aggregations\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"‚úÖ Spark ready - Version: {spark.version}\")\n",
    "\n",
    "# Create comprehensive sales data\n",
    "sales_data = [\n",
    "    (\"Alice\", \"North\", \"Electronics\", 1200, \"2023-01\"),\n",
    "    (\"Bob\", \"South\", \"Electronics\", 800, \"2023-01\"),\n",
    "    (\"Charlie\", \"North\", \"Clothing\", 150, \"2023-01\"),\n",
    "    (\"Diana\", \"East\", \"Electronics\", 950, \"2023-01\"),\n",
    "    (\"Eve\", \"South\", \"Clothing\", 200, \"2023-01\"),\n",
    "    (\"Frank\", \"North\", \"Electronics\", 1300, \"2023-02\"),\n",
    "    (\"Grace\", \"East\", \"Clothing\", 300, \"2023-02\"),\n",
    "    (\"Henry\", \"South\", \"Electronics\", 1100, \"2023-02\"),\n",
    "    (\"Ivy\", \"North\", \"Books\", 75, \"2023-02\"),\n",
    "    (\"Jack\", \"East\", \"Books\", 125, \"2023-02\")\n",
    "]\n",
    "\n",
    "sales_df = spark.createDataFrame(\n",
    "    sales_data, \n",
    "    [\"salesperson\", \"region\", \"category\", \"amount\", \"month\"]\n",
    ")\n",
    "\n",
    "print(\"üìä Sales Dataset:\")\n",
    "sales_df.show()\n",
    "print(f\"Total records: {sales_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Basic GroupBy Operations\n",
    "\n",
    "### Single Column Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic aggregations\n",
    "print(\"üéØ BASIC GROUPBY OPERATIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Group by region and calculate total sales\n",
    "region_sales = sales_df.groupBy(\"region\").sum(\"amount\")\n",
    "print(\"Total sales by region:\")\n",
    "region_sales.show()\n",
    "\n",
    "# Group by category and count transactions\n",
    "category_count = sales_df.groupBy(\"category\").count()\n",
    "print(\"\\nTransaction count by category:\")\n",
    "category_count.show()\n",
    "\n",
    "# Group by salesperson and get average sale\n",
    "salesperson_avg = sales_df.groupBy(\"salesperson\").avg(\"amount\")\n",
    "print(\"\\nAverage sale amount by salesperson:\")\n",
    "salesperson_avg.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Column Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by multiple columns\n",
    "print(\"üîÄ MULTIPLE COLUMN GROUPING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Group by region and category\n",
    "region_category_sales = sales_df.groupBy([\"region\", \"category\"]).sum(\"amount\")\n",
    "print(\"Sales by region and category:\")\n",
    "region_category_sales.show()\n",
    "\n",
    "# Group by month and region\n",
    "monthly_region_sales = sales_df.groupBy([\"month\", \"region\"]).agg(\n",
    "    sum(\"amount\").alias(\"total_sales\"),\n",
    "    count(\"*\").alias(\"transaction_count\"),\n",
    "    avg(\"amount\").alias(\"avg_transaction\")\n",
    ")\n",
    "print(\"\\nMonthly sales by region:\")\n",
    "monthly_region_sales.show()\n",
    "\n",
    "# Sort results for better readability\n",
    "monthly_region_sales.orderBy([\"month\", \"region\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßÆ Advanced Aggregation Functions\n",
    "\n",
    "### Using the agg() Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced aggregations with agg()\n",
    "print(\"üßÆ ADVANCED AGGREGATIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Comprehensive sales analysis by region\n",
    "comprehensive_analysis = sales_df.groupBy(\"region\").agg(\n",
    "    count(\"*\").alias(\"total_transactions\"),\n",
    "    sum(\"amount\").alias(\"total_sales\"),\n",
    "    avg(\"amount\").alias(\"avg_transaction\"),\n",
    "    min(\"amount\").alias(\"smallest_sale\"),\n",
    "    max(\"amount\").alias(\"largest_sale\"),\n",
    "    stddev(\"amount\").alias(\"sales_stddev\"),\n",
    "    countDistinct(\"salesperson\").alias(\"unique_salespeople\"),\n",
    "    countDistinct(\"category\").alias(\"unique_categories\")\n",
    ")\n",
    "\n",
    "print(\"Comprehensive regional analysis:\")\n",
    "comprehensive_analysis.show()\n",
    "\n",
    "# Category performance analysis\n",
    "category_analysis = sales_df.groupBy(\"category\").agg(\n",
    "    sum(\"amount\").alias(\"total_revenue\"),\n",
    "    count(\"*\").alias(\"transaction_count\"),\n",
    "    (sum(\"amount\") / count(\"*\")).alias(\"avg_transaction_size\"),\n",
    "    approx_count_distinct(\"salesperson\").alias(\"unique_sellers\"),\n",
    "    first(\"salesperson\").alias(\"first_seller\"),\n",
    "    last(\"salesperson\").alias(\"last_seller\")\n",
    ")\n",
    "\n",
    "print(\"\\nCategory performance analysis:\")\n",
    "category_analysis.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical aggregations\n",
    "print(\"üìà STATISTICAL AGGREGATIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Statistical summary of sales amounts\n",
    "stats_summary = sales_df.select(\n",
    "    sum(\"amount\").alias(\"total_sales\"),\n",
    "    avg(\"amount\").alias(\"mean_sale\"),\n",
    "    stddev(\"amount\").alias(\"std_deviation\"),\n",
    "    variance(\"amount\").alias(\"variance\"),\n",
    "    min(\"amount\").alias(\"min_sale\"),\n",
    "    max(\"amount\").alias(\"max_sale\"),\n",
    "    (F.percentile_approx(\"amount\", 0.5)).alias(\"median_sale\"),\n",
    "    (F.percentile_approx(\"amount\", 0.25)).alias(\"q1\"),\n",
    "    (F.percentile_approx(\"amount\", 0.75)).alias(\"q3\")\n",
    ")\n",
    "\n",
    "print(\"Overall sales statistics:\")\n",
    "stats_summary.show()\n",
    "\n",
    "# Statistical analysis by category\n",
    "category_stats = sales_df.groupBy(\"category\").agg(\n",
    "    count(\"*\").alias(\"count\"),\n",
    "    avg(\"amount\").alias(\"mean\"),\n",
    "    stddev(\"amount\").alias(\"std\"),\n",
    "    min(\"amount\").alias(\"min\"),\n",
    "    max(\"amount\").alias(\"max\"),\n",
    "    (max(\"amount\") - min(\"amount\")).alias(\"range\"),\n",
    "    (stddev(\"amount\") / avg(\"amount\")).alias(\"coefficient_of_variation\")\n",
    ")\n",
    "\n",
    "print(\"\\nStatistical analysis by category:\")\n",
    "category_stats.show()\n",
    "\n",
    "# Calculate coefficient of variation (CV)\n",
    "# CV < 0.15: Low variability\n",
    "# CV 0.15-0.35: Moderate variability  \n",
    "# CV > 0.35: High variability\n",
    "cv_analysis = category_stats.withColumn(\n",
    "    \"variability_level\",\n",
    "    F.when(F.col(\"coefficient_of_variation\") < 0.15, \"Low\")\n",
    "    .when(F.col(\"coefficient_of_variation\") < 0.35, \"Moderate\")\n",
    "    .otherwise(\"High\")\n",
    ")\n",
    "\n",
    "print(\"\\nVariability analysis:\")\n",
    "cv_analysis.select(\"category\", \"coefficient_of_variation\", \"variability_level\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé® Custom Aggregation Functions\n",
    "\n",
    "### Using aggregateByKey for Complex Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom aggregations\n",
    "print(\"üé® CUSTOM AGGREGATIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Define custom aggregation functions\n",
    "def combine_stats(acc, value):\n",
    "    \"\"\"Combine statistics incrementally\"\"\"\n",
    "    count, total, min_val, max_val, sum_squares = acc\n",
    "    return (\n",
    "        count + 1,\n",
    "        total + value,\n",
    "        min(min_val, value),\n",
    "        max(max_val, value),\n",
    "        sum_squares + (value ** 2)\n",
    "    )\n",
    "\n",
    "def merge_stats(acc1, acc2):\n",
    "    \"\"\"Merge statistics from different partitions\"\"\"\n",
    "    c1, t1, min1, max1, ss1 = acc1\n",
    "    c2, t2, min2, max2, ss2 = acc2\n",
    "    return (\n",
    "        c1 + c2,\n",
    "        t1 + t2,\n",
    "        min(min1, min2),\n",
    "        max(max1, max2),\n",
    "        ss1 + ss2\n",
    "    )\n",
    "\n",
    "# Calculate comprehensive statistics including standard deviation\n",
    "initial_acc = (0, 0, float('inf'), float('-inf'), 0)  # count, sum, min, max, sum_squares\n",
    "\n",
    "custom_stats = sales_df.rdd.map(lambda row: (row.region, row.amount)) \\\n",
    "    .aggregateByKey(initial_acc, combine_stats, merge_stats)\n",
    "\n",
    "# Convert back to DataFrame and calculate final statistics\n",
    "stats_df = custom_stats.mapValues(lambda stats: {\n",
    "    'count': stats[0],\n",
    "    'sum': stats[1],\n",
    "    'min': stats[2],\n",
    "    'max': stats[3],\n",
    "    'mean': stats[1] / stats[0] if stats[0] > 0 else 0,\n",
    "    'variance': (stats[4] / stats[0] - (stats[1] / stats[0]) ** 2) if stats[0] > 1 else 0,\n",
    "    'stddev': ((stats[4] / stats[0] - (stats[1] / stats[0]) ** 2) ** 0.5) if stats[0] > 1 else 0\n",
    "}).toDF([\"region\", \"statistics\"])\n",
    "\n",
    "# Extract statistics into separate columns\n",
    "final_stats = stats_df.select(\n",
    "    \"region\",\n",
    "    F.col(\"statistics.count\").alias(\"transaction_count\"),\n",
    "    F.col(\"statistics.sum\").alias(\"total_sales\"),\n",
    "    F.col(\"statistics.mean\").alias(\"avg_sale\"),\n",
    "    F.col(\"statistics.min\").alias(\"min_sale\"),\n",
    "    F.col(\"statistics.max\").alias(\"max_sale\"),\n",
    "    F.col(\"statistics.stddev\").alias(\"std_dev\")\n",
    ")\n",
    "\n",
    "print(\"Custom aggregation results:\")\n",
    "final_stats.show()\n",
    "\n",
    "# Verify calculations\n",
    "verification = sales_df.groupBy(\"region\").agg(\n",
    "    count(\"*\").alias(\"count\"),\n",
    "    sum(\"amount\").alias(\"sum\"),\n",
    "    avg(\"amount\").alias(\"avg\"),\n",
    "    min(\"amount\").alias(\"min\"),\n",
    "    max(\"amount\").alias(\"max\"),\n",
    "    stddev(\"amount\").alias(\"std\")\n",
    ")\n",
    "\n",
    "print(\"\\nVerification (built-in functions):\")\n",
    "verification.show()\n",
    "\n",
    "print(\"\\n‚úÖ Custom aggregation matches built-in functions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö° Performance Optimization\n",
    "\n",
    "### Efficient Aggregation Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance optimization patterns\n",
    "print(\"‚ö° PERFORMANCE OPTIMIZATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create larger dataset for performance testing\n",
    "large_sales_data = [\n",
    "    (f\"salesperson_{i%50}\", f\"region_{(i%5)+1}\", f\"category_{(i%4)+1}\", \n",
    "     100 + (i % 900), f\"2023-{(i%12)+1:02d}\")\n",
    "    for i in range(50000)\n",
    "]\n",
    "\n",
    "large_df = spark.createDataFrame(large_sales_data, \n",
    "    [\"salesperson\", \"region\", \"category\", \"amount\", \"month\"])\n",
    "\n",
    "print(f\"Large dataset: {large_df.count():,} records\")\n",
    "\n",
    "# Pattern 1: Pre-filter before aggregation\n",
    "print(\"\\nPattern 1: Filter before aggregation\")\n",
    "high_value_sales = large_df.filter(col(\"amount\") > 500)\n",
    "filtered_result = high_value_sales.groupBy(\"region\").sum(\"amount\")\n",
    "print(f\"High-value sales by region: {filtered_result.count()} regions\")\n",
    "\n",
    "# Pattern 2: Use approximate functions for large datasets\n",
    "print(\"\\nPattern 2: Approximate aggregations\")\n",
    "approx_stats = large_df.select(\n",
    "    F.approx_count_distinct(\"salesperson\").alias(\"unique_sellers\"),\n",
    "    F.percentile_approx(\"amount\", 0.5).alias(\"median_amount\"),\n",
    "    F.percentile_approx(\"amount\", [0.25, 0.75]).alias(\"quartiles\")\n",
    ")\n",
    "approx_stats.show()\n",
    "\n",
    "# Pattern 3: Cache intermediate results for multiple aggregations\n",
    "print(\"\\nPattern 3: Cache for multiple operations\")\n",
    "cached_df = large_df.filter(col(\"amount\") > 200).cache()\n",
    "\n",
    "result1 = cached_df.groupBy(\"region\").sum(\"amount\")\n",
    "result2 = cached_df.groupBy(\"category\").avg(\"amount\")\n",
    "result3 = cached_df.agg(count(\"*\"), sum(\"amount\"))\n",
    "\n",
    "print(f\"Multiple aggregations on cached data:\")\n",
    "print(f\"- Regional totals: {result1.count()} regions\")\n",
    "print(f\"- Category averages: {result2.count()} categories\")\n",
    "print(f\"- Overall stats computed\")\n",
    "\n",
    "# Clean up cache\n",
    "cached_df.unpersist()\n",
    "print(\"\\nCache cleared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation Pipeline Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized aggregation pipeline\n",
    "print(\"üîß AGGREGATION PIPELINE OPTIMIZATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Complete business analysis pipeline\n",
    "business_analysis = sales_df \\\n",
    "    .filter(col(\"amount\") > 0) \\\n",
    "    .withColumn(\"revenue_category\",\n",
    "                F.when(col(\"amount\") >= 1000, \"High\")\n",
    "                .when(col(\"amount\") >= 500, \"Medium\")\n",
    "                .otherwise(\"Low\")) \\\n",
    "    .groupBy([\"region\", \"revenue_category\"]) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"transaction_count\"),\n",
    "        sum(\"amount\").alias(\"total_revenue\"),\n",
    "        avg(\"amount\").alias(\"avg_transaction\"),\n",
    "        stddev(\"amount\").alias(\"revenue_stddev\"),\n",
    "        collect_list(\"salesperson\").alias(\"top_sellers\")\n",
    "    ) \\\n",
    "    .withColumn(\"revenue_per_transaction\", col(\"total_revenue\") / col(\"transaction_count\")) \\\n",
    "    .withColumn(\"efficiency_score\", \n",
    "                F.when(col(\"transaction_count\") > 2, \"High\")\n",
    "                .when(col(\"transaction_count\") > 1, \"Medium\")\n",
    "                .otherwise(\"Low\")) \\\n",
    "    .orderBy([\"region\", \"revenue_category\"])\n",
    "\n",
    "print(\"Complete business analysis pipeline:\")\n",
    "business_analysis.show(truncate=False)\n",
    "\n",
    "# Executive summary\n",
    "executive_summary = sales_df.agg(\n",
    "    countDistinct(\"salesperson\").alias(\"total_salespeople\"),\n",
    "    countDistinct(\"region\").alias(\"regions_covered\"),\n",
    "    countDistinct(\"category\").alias(\"product_categories\"),\n",
    "    sum(\"amount\").alias(\"total_revenue\"),\n",
    "    avg(\"amount\").alias(\"avg_transaction\"),\n",
    "    max(\"amount\").alias(\"highest_sale\"),\n",
    "    min(\"amount\").alias(\"lowest_sale\")\n",
    ")\n",
    "\n",
    "print(\"\\nüìä EXECUTIVE SUMMARY\")\n",
    "executive_summary.show()\n",
    "\n",
    "# Monthly trends\n",
    "monthly_trends = sales_df.groupBy(\"month\").agg(\n",
    "    count(\"*\").alias(\"transactions\"),\n",
    "    sum(\"amount\").alias(\"revenue\"),\n",
    "    avg(\"amount\").alias(\"avg_sale\"),\n",
    "    countDistinct(\"salesperson\").alias(\"active_sellers\")\n",
    ").orderBy(\"month\")\n",
    "\n",
    "print(\"\\nüìà MONTHLY TRENDS\")\n",
    "monthly_trends.show()\n",
    "\n",
    "# Performance analysis\n",
    "performance_analysis = sales_df.groupBy(\"salesperson\").agg(\n",
    "    count(\"*\").alias(\"total_sales\"),\n",
    "    sum(\"amount\").alias(\"total_revenue\"),\n",
    "    avg(\"amount\").alias(\"avg_sale_size\"),\n",
    "    max(\"amount\").alias(\"best_sale\"),\n",
    "    countDistinct(\"category\").alias(\"categories_sold\"),\n",
    "    countDistinct(\"region\").alias(\"regions_covered\")\n",
    ").withColumn(\n",
    "    \"productivity_score\", \n",
    "    (col(\"total_sales\") * 0.3 + col(\"total_revenue\") / 1000 * 0.7)\n",
    ").orderBy(col(\"productivity_score\").desc())\n",
    "\n",
    "print(\"\\nüèÜ SALESPERSON PERFORMANCE RANKING\")\n",
    "performance_analysis.select(\n",
    "    \"salesperson\", \"total_sales\", \"total_revenue\", \n",
    "    \"avg_sale_size\", \"productivity_score\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üö® Common Mistakes and Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common mistakes and solutions\n",
    "print(\"üö® COMMON MISTAKES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Mistake 1: Forgetting to collect() results\n",
    "print(\"‚ùå Mistake: Forgetting to collect results\")\n",
    "grouped_data = sales_df.groupBy(\"region\").sum(\"amount\")\n",
    "print(f\"Type: {type(grouped_data)} (DataFrame, not collected results)\")\n",
    "\n",
    "print(\"\\n‚úÖ Solution: Use collect() or show() to see results\")\n",
    "results = grouped_data.collect()\n",
    "print(f\"Collected {len(results)} results\")\n",
    "\n",
    "# Mistake 2: Using wrong column references\n",
    "print(\"\\n‚ùå Mistake: Wrong column references in agg\")\n",
    "try:\n",
    "    bad_agg = sales_df.groupBy(\"region\").agg(sum(\"nonexistent_column\"))\n",
    "    bad_agg.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)[:100]}...\")\n",
    "\n",
    "print(\"\\n‚úÖ Solution: Use correct column names\")\n",
    "correct_agg = sales_df.groupBy(\"region\").agg(sum(\"amount\"))\n",
    "correct_agg.show()\n",
    "\n",
    "# Mistake 3: Performance - aggregating without filtering\n",
    "print(\"\\n‚ùå Mistake: Aggregating massive datasets without filtering\")\n",
    "print(\"Imagine aggregating 1TB of data without any filters...\")\n",
    "\n",
    "print(\"\\n‚úÖ Solution: Filter before aggregating\")\n",
    "optimized_agg = sales_df \\\n",
    "    .filter(col(\"amount\") > 100) \\\n",
    "    .filter(col(\"region\") == \"North\") \\\n",
    "    .groupBy(\"category\") \\\n",
    "    .sum(\"amount\")\n",
    "\n",
    "print(\"Filtered and aggregated efficiently:\")\n",
    "optimized_agg.show()\n",
    "\n",
    "# Mistake 4: Using collect() on large result sets\n",
    "print(\"\\n‚ùå Mistake: collect() on large result sets\")\n",
    "print(\"Can cause OutOfMemoryError on driver\")\n",
    "\n",
    "print(\"\\n‚úÖ Solutions:\")\n",
    "print(\"- Use show(n) for preview\")\n",
    "print(\"- Use take(n) to limit results\")\n",
    "print(\"- Write to file instead of collecting\")\n",
    "print(\"- Use limit() before collect()\")\n",
    "\n",
    "# Safe result collection\n",
    "safe_results = sales_df.groupBy(\"region\").sum(\"amount\").limit(10).collect()\n",
    "print(f\"\\nSafely collected {len(safe_results)} results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Key Takeaways\n",
    "\n",
    "### What You Learned:\n",
    "- ‚úÖ **`groupBy()`** - Group data by one or more columns\n",
    "- ‚úÖ **`agg()`** - Apply aggregation functions to grouped data\n",
    "- ‚úÖ **Statistical functions** - sum, avg, min, max, stddev, variance\n",
    "- ‚úÖ **Multiple aggregations** - Combine multiple stats in one operation\n",
    "- ‚úÖ **Custom aggregations** - Using aggregateByKey for complex logic\n",
    "- ‚úÖ **Performance optimization** - Filter, cache, and pipeline efficiently\n",
    "\n",
    "### Aggregation Types:\n",
    "- üî∏ **Simple aggregations**: `sum()`, `avg()`, `count()`, `min()`, `max()`\n",
    "- üî∏ **Statistical aggregations**: `stddev()`, `variance()`, `percentile_approx()`\n",
    "- üî∏ **Distinct operations**: `countDistinct()`, `approx_count_distinct()`\n",
    "- üî∏ **Custom aggregations**: `aggregateByKey()` with user-defined functions\n",
    "- üî∏ **Collection aggregations**: `collect_list()`, `collect_set()`\n",
    "\n",
    "### Performance Best Practices:\n",
    "- üî∏ **Filter before aggregating** to reduce data volume\n",
    "- üî∏ **Use approximate functions** for large datasets\n",
    "- üî∏ **Cache intermediate results** for multiple operations\n",
    "- üî∏ **Choose appropriate aggregation functions** for your use case\n",
    "- üî∏ **Monitor shuffle operations** in Spark UI\n",
    "\n",
    "### Common Patterns:\n",
    "- üî∏ `df.groupBy(cols).agg(funcs)` - Basic aggregation pattern\n",
    "- üî∏ `df.groupBy(cols).sum/avg/count()` - Simple single-function aggregations\n",
    "- üî∏ `F.percentile_approx(col, percentile)` - Approximate percentiles\n",
    "- üî∏ `F.approx_count_distinct(col)` - Approximate distinct counts\n",
    "- üî∏ `aggregateByKey()` - Custom aggregation logic\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Next Steps\n",
    "\n",
    "Now that you master DataFrame aggregations, you're ready for:\n",
    "\n",
    "1. **Window Functions** - Advanced analytical operations\n",
    "2. **Joins** - Combining multiple DataFrames\n",
    "3. **Complex Data Types** - Arrays, maps, and structs\n",
    "4. **Advanced Analytics** - Time series and trend analysis\n",
    "\n",
    "**Aggregations are the foundation of data analysis in Spark!**\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Congratulations! You now have the power to analyze and summarize data at scale with Spark aggregations!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
