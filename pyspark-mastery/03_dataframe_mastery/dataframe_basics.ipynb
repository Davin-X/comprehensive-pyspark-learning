{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“Š PySpark DataFrame Mastery: Complete Operations Guide\n\n",
    "Master **PySpark DataFrame operations** through hands-on examples covering creation, transformation, analysis, and optimization. This comprehensive tutorial takes you from basic DataFrame manipulation to advanced analytics techniques.\n\n",
    "## ðŸŽ¯ What You'll Learn\n\n",
    "Transform into a PySpark DataFrame expert by mastering:\n\n",
    "### ðŸ—ï¸ **Core DataFrame Operations**\n",
    "- **DataFrame Creation** - Multiple methods (lists, RDDs, external sources)\n",
    "- **Data Inspection** - Schema exploration, data types, and structure analysis\n",
    "- **Column Manipulation** - Selection, transformation, addition, and renaming\n",
    "- **Row Filtering** - Conditional selection and complex filtering logic\n\n",
    "### ðŸ“Š **Advanced Analytics**\n",
    "- **Aggregations & Grouping** - GroupBy operations with statistical functions\n",
    "- **DataFrame Joins** - Inner, outer, and cross joins with best practices\n",
    "- **Window Functions** - Ranking, running totals, and analytical functions\n",
    "- **Spark SQL Integration** - SQL queries and complex analytical workflows\n\n",
    "### âš¡ **Performance & Production**\n",
    "- **Data I/O Operations** - Efficient reading/writing (CSV, JSON, Parquet)\n",
    "- **Performance Optimization** - Caching, broadcast joins, and tuning\n",
    "- **Schema Management** - Type safety and data validation\n",
    "- **UDFs & Extensions** - Custom functions and PySpark extensibility\n\n",
    "---\n\n",
    "## ðŸ“š Tutorial Structure\n\n",
    "Follow this structured learning path:\n\n",
    "1. **âš™ï¸ PySpark Setup** - Optimized configuration for development\n",
    "2. **ðŸ—ï¸ DataFrame Creation** - Multiple creation methods\n",
    "3. **ðŸ“‹ Data Inspection** - Understanding your data structure\n",
    "4. **ðŸŽ¯ Column Operations** - Selection and manipulation\n",
    "5. **ðŸ” Filtering** - Row selection techniques\n",
    "6. **ðŸ“Š Aggregations** - Grouping and statistical analysis\n",
    "7. **ðŸ”— Joins** - Combining DataFrames effectively\n",
    "8. **ðŸ“ˆ Window Functions** - Advanced analytical operations\n",
    "9. **ðŸ—„ï¸ Spark SQL** - SQL-based DataFrame operations\n",
    "10. **ðŸ’¾ Data I/O** - Reading and writing data efficiently\n",
    "11. **âš¡ Performance** - Optimization techniques\n",
    "12. **ðŸ”§ Schema Management** - Data types and validation\n",
    "13. **ðŸŽ¯ UDFs** - Custom function development\n",
    "14. **ðŸ“š Reference** - Quick operations guide\n\n",
    "## ðŸš€ Prerequisites & Setup\n\n",
    "### System Requirements\n",
    "- **Python 3.8+** - Modern Python version\n",
    "- **Java 8/11** - JVM for Spark runtime\n",
    "- **PySpark 3.3+** - Latest stable version\n",
    "- **8GB+ RAM** - Recommended for smooth operation\n\n",
    "### Quick Installation\n",
    "```bash\n",
    "# Install PySpark\n",
    "pip install pyspark\n",
    "\n",
    "# Optional: Install findspark for easier setup\n",
    "pip install findspark\n",
    "```\n\n",
    "### Getting Started\n",
    "1. **Run cells sequentially** - Each section builds on previous concepts\n",
    "2. **Monitor Spark UI** at `http://localhost:4040` for job execution\n",
    "3. **Experiment freely** - Modify code and explore results\n",
    "4. **Check outputs** - Understand what each operation returns\n\n",
    "**ðŸ’¡ Pro Tip:** Focus on understanding the concepts rather than memorizing syntax. The operations follow logical patterns that become intuitive with practice.\n\n",
    "---\n\n",
    "**ðŸŽ¯ Ready to master PySpark DataFrames? Let's begin with optimized setup and configuration!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \u2699\ufe0f PySpark Setup\n\n",
    "Initialize PySpark with optimized configuration for development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark Setup and Configuration\n",
    "\n",
    "# Initialize findspark to locate PySpark\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# Import PySpark components\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, when, avg, sum, count\n",
    "\n",
    "# Create SparkSession with comprehensive configuration\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"PySpark_DataFrame_Basics\")\n",
    "         .master(\"local[*]\")  # Use all available cores\n",
    "         .config(\"spark.sql.adaptive.enabled\", \"true\")  # Enable adaptive query execution\n",
    "         .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")  # Optimize partitions\n",
    "         .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")  # Efficient serialization\n",
    "         .enableHiveSupport()\n",
    "         .getOrCreate()\n",
    "         )\n",
    "\n",
    "# Get SparkContext for RDD operations\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Verify setup\n",
    "print(f\"\u2705 Spark Version: {spark.version}\")\n",
    "print(f\"\u2705 Python Version: {sc.pythonVer}\")\n",
    "print(f\"\u2705 Spark UI: http://localhost:4040\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udfd7\ufe0f DataFrame Creation\n\n",
    "Learn different ways to create DataFrames in PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: From Python lists\n",
    "data = [\n",
    "    (\"Alice\", 25, \"Engineering\", 75000),\n",
    "    (\"Bob\", 30, \"Sales\", 65000),\n",
    "    (\"Charlie\", 35, \"Engineering\", 85000),\n",
    "    (\"Diana\", 28, \"HR\", 55000),\n",
    "    (\"Eve\", 32, \"Sales\", 70000)\n",
    "]\n",
    "\n",
    "columns = [\"name\", \"age\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "print(\"DataFrame from Python lists:\")\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udccb DataFrame Inspection\n\n",
    "Explore DataFrame structure and content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic DataFrame inspection\n",
    "print(f\"Shape: {df.count()} rows \u00d7 {len(df.columns)} columns\")\n",
    "print(f\"Columns: {df.columns}\")\n",
    "print(f\"Data types: {df.dtypes}\")\n",
    "\n",
    "# Display data\n",
    "print(\"First 3 rows:\")\n",
    "df.show(3)\n",
    "\n",
    "print(\"Schema:\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udfaf Column Selection & Manipulation\n\n",
    "Select, rename, and transform DataFrame columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select specific columns\n",
    "df.select(\"name\", \"department\").show()\n",
    "\n",
    "# Select with expressions\n",
    "df.select(\n",
    "    \"name\",\n",
    "    col(\"salary\") * 1.1,\n",
    "    (col(\"age\") + 1).alias(\"age_next_year\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new columns\n",
    "df.withColumn(\"bonus\", col(\"salary\") * 0.1).show()\n",
    "\n",
    "df.withColumn(\"is_senior\", when(col(\"age\") >= 30, \"Yes\").otherwise(\"No\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udd0d Filtering & Conditional Selection\n\n",
    "Filter DataFrame rows based on conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple filtering\n",
    "print(\"Employees over 30:\")\n",
    "df.filter(col(\"age\") > 30).show()\n",
    "\n",
    "print(\"Engineering department:\")\n",
    "df.filter(col(\"department\") == \"Engineering\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex filtering with multiple conditions\n",
    "print(\"High earners in Engineering:\")\n",
    "df.filter(\n",
    "    (col(\"department\") == \"Engineering\") & \n",
    "    (col(\"salary\") > 80000)\n",
    ").show()\n",
    "\n",
    "print(\"Young professionals (25-30):\")\n",
    "df.filter(col(\"age\").between(25, 30)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcca Grouping & Aggregations\n\n",
    "Group data and perform aggregate calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic groupBy and aggregation\n",
    "print(\"Department statistics:\")\n",
    "df.groupBy(\"department\").agg(\n",
    "    count(\"name\").alias(\"employee_count\"),\n",
    "    avg(\"salary\").alias(\"avg_salary\"),\n",
    "    sum(\"salary\").alias(\"total_salary\"),\n",
    "    max(\"age\").alias(\"max_age\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udd17 DataFrame Joins\n\n",
    "Combine DataFrames using different join types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create department lookup table\n",
    "dept_info = spark.createDataFrame([\n",
    "    (\"Engineering\", \"Tech Division\", \"A\"),\n",
    "    (\"Sales\", \"Business Division\", \"B\"),\n",
    "    (\"HR\", \"Support Division\", \"C\")\n",
    "], [\"department\", \"division\", \"building\"])\n",
    "\n",
    "print(\"Department information:\")\n",
    "dept_info.show()\n",
    "\n",
    "# Inner join (default)\n",
    "print(\"Inner join results:\")\n",
    "df.join(dept_info, \"department\", \"inner\").show()\n",
    "\n",
    "# Left outer join\n",
    "print(\"Left outer join results:\")\n",
    "df.join(dept_info, \"department\", \"left_outer\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcc8 Window Functions\n\n",
    "Perform advanced analytics with window functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, rank, dense_rank, lag, lead\n",
    "\n",
    "# Define window specification\n",
    "window_spec = Window.partitionBy(\"department\").orderBy(col(\"salary\").desc())\n",
    "\n",
    "# Row numbering within department\n",
    "print(\"Row numbers by department and salary:\")\n",
    "df.withColumn(\"row_num\", row_number().over(window_spec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\uddc4\ufe0f Spark SQL Integration\n\n",
    "Execute SQL queries on DataFrames using Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register DataFrame as temporary view\n",
    "df.createOrReplaceTempView(\"employees\")\n",
    "dept_info.createOrReplaceTempView(\"departments\")\n",
    "\n",
    "# Execute SQL queries\n",
    "print(\"SQL Query Results:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        e.name,\n",
    "        e.department,\n",
    "        e.salary,\n",
    "        d.division,\n",
    "        CASE \n",
    "            WHEN e.salary > 70000 THEN 'High'\n",
    "            WHEN e.salary > 60000 THEN 'Medium'\n",
    "            ELSE 'Low'\n",
    "        END as salary_tier\n",
    "    FROM employees e\n",
    "    LEFT JOIN departments d ON e.department = d.department\n",
    "    ORDER BY e.salary DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcbe Data I/O Operations\n\n",
    "Read and write data in various formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data for I/O demonstration\n",
    "sample_data = [\n",
    "    {\"id\": 1, \"product\": \"Laptop\", \"price\": 1200.00, \"category\": \"Electronics\"},\n",
    "    {\"id\": 2, \"product\": \"Book\", \"price\": 25.99, \"category\": \"Education\"},\n",
    "    {\"id\": 3, \"product\": \"Chair\", \"price\": 150.50, \"category\": \"Furniture\"}\n",
    "]\n",
    "\n",
    "products_df = spark.createDataFrame(sample_data)\n",
    "products_df.show()\n",
    "\n",
    "# Write to different formats\n",
    "products_df.write.mode(\"overwrite\").csv(\"products.csv\", header=True)\n",
    "products_df.write.mode(\"overwrite\").json(\"products.json\")\n",
    "products_df.write.mode(\"overwrite\").parquet(\"products.parquet\")\n",
    "\n",
    "print(\"Files written successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \u26a1 Performance Optimization\n\n",
    "Learn techniques to optimize PySpark performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caching for repeated operations\n",
    "large_df = df.union(df).union(df)  # Simulate larger dataset\n",
    "\n",
    "# Cache in memory\n",
    "large_df.cache()\n",
    "print(f\"Cached dataset has {large_df.count()} rows\")\n",
    "\n",
    "# Check storage level\n",
    "print(f\"Storage level: {large_df.storageLevel}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udd27 Schema Management\n\n",
    "Handle DataFrame schemas and data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "# Define explicit schema\n",
    "explicit_schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"department\", StringType(), True),\n",
    "    StructField(\"salary\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame with explicit schema\n",
    "explicit_df = spark.createDataFrame(data, explicit_schema)\n",
    "print(\"Explicit schema:\")\n",
    "explicit_df.printSchema()\n",
    "\n",
    "print(\"Data types:\")\n",
    "print(explicit_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udfaf User Defined Functions (UDFs)\n\n",
    "Create custom functions to extend PySpark capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Define UDF for salary categorization\n",
    "def categorize_salary(salary):\n",
    "    if salary > 75000:\n",
    "        return \"Executive\"\n",
    "    elif salary > 65000:\n",
    "        return \"Senior\"\n",
    "    elif salary > 55000:\n",
    "        return \"Mid-level\"\n",
    "    else:\n",
    "        return \"Entry-level\"\n",
    "\n",
    "# Register UDF\n",
    "salary_category_udf = udf(categorize_salary, StringType())\n",
    "\n",
    "# Apply UDF\n",
    "df.withColumn(\"salary_level\", salary_category_udf(col(\"salary\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcda Quick Reference: DataFrame Operations\n\n",
    "### Essential Operations\n",
    "- **`df.select(*cols)`** - Select columns\n",
    "- **`df.filter(condition)`** - Filter rows\n",
    "- **`df.groupBy(*cols).agg(*exprs)`** - Group and aggregate\n",
    "- **`df.join(other, on, how)`** - Join DataFrames\n",
    "- **`df.orderBy(*cols)`** - Sort rows\n",
    "- **`df.withColumn(col, expr)`** - Add/modify column\n",
    "- **`df.show()`** - Display DataFrame\n",
    "- **`df.count()`** - Count rows\n\n",
    "### Common Functions\n",
    "- **`col(\"name\")`** - Reference column\n",
    "- **`when(condition, value).otherwise(other)`** - Conditional logic\n",
    "- **`avg(col)`**, **`sum(col)`**, **`count(col)`** - Aggregations\n",
    "- **`lit(value)`** - Create literal column\n\n",
    "**Next Steps:** Explore advanced topics like machine learning, streaming, and production deployment!\n\n",
    "---\n",
    "**\ud83c\udfaf This notebook covers the complete DataFrame fundamentals you need to become proficient with PySpark!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
