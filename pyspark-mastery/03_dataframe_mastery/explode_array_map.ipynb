{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üí• Explode, Arrays & Maps: Complex Data Types in DataFrames\n",
    "\n",
    "**Time to complete:** 35 minutes  \n",
    "**Difficulty:** Intermediate  \n",
    "**Prerequisites:** DataFrame basics, column expressions\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will master:\n",
    "- ‚úÖ **`explode()`** - Convert arrays to rows\n",
    "- ‚úÖ **Array operations** - Manipulate array columns\n",
    "- ‚úÖ **Map operations** - Work with key-value pairs\n",
    "- ‚úÖ **Struct operations** - Handle nested data\n",
    "- ‚úÖ **JSON data handling** - Parse and process JSON\n",
    "- ‚úÖ **Complex data transformations** - Nested operations\n",
    "\n",
    "**Essential for handling real-world JSON and nested data!**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Understanding Complex Data Types\n",
    "\n",
    "**Spark DataFrames support complex data types beyond simple strings and numbers:**\n",
    "\n",
    "- **Arrays**: Ordered collections `[item1, item2, item3]`\n",
    "- **Maps**: Key-value pairs `{\"key1\": \"value1\", \"key2\": \"value2\"}`\n",
    "- **Structs**: Named tuples with fields `{field1: value1, field2: value2}`\n",
    "\n",
    "### Why Complex Types Matter:\n",
    "- **JSON data** often contains nested structures\n",
    "- **Real-world data** is rarely flat\n",
    "- **Performance** - Avoid expensive joins with pre-joined data\n",
    "- **Flexibility** - Handle variable-length data\n",
    "\n",
    "**`explode()` is your gateway to flattening complex data!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, explode_outer, posexplode, arrays_zip\n",
    "from pyspark.sql.functions import array, array_contains, array_distinct, array_sort\n",
    "from pyspark.sql.functions import map_keys, map_values, map_from_arrays\n",
    "from pyspark.sql.functions import struct, col, when, lit\n",
    "from pyspark.sql.types import ArrayType, MapType, StructType, StructField, StringType, IntegerType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Explode_Arrays_Maps\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"‚úÖ Spark ready - Version: {spark.version}\")\n",
    "\n",
    "# Create sample data with complex types\n",
    "complex_data = [\n",
    "    (\"Alice\", [\"Python\", \"Java\", \"SQL\"], {\"experience\": 5, \"level\": \"Senior\"}, \n",
    "     {\"city\": \"New York\", \"country\": \"USA\"}),\n",
    "    (\"Bob\", [\"Scala\", \"Python\"], {\"experience\": 3, \"level\": \"Mid\"}, \n",
    "     {\"city\": \"London\", \"country\": \"UK\"}),\n",
    "    (\"Charlie\", [\"Java\", \"C++\", \"JavaScript\"], {\"experience\": 7, \"level\": \"Senior\"}, \n",
    "     {\"city\": \"Tokyo\", \"country\": \"Japan\"}),\n",
    "    (\"Diana\", [\"R\", \"Python\", \"SQL\"], {\"experience\": 4, \"level\": \"Mid\"}, \n",
    "     {\"city\": \"Berlin\", \"country\": \"Germany\"})\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(complex_data, \n",
    "    [\"name\", \"skills\", \"profile\", \"location\"])\n",
    "\n",
    "print(\"üìä Complex Data Dataset:\")\n",
    "df.show(truncate=False)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí• Explode Operations\n",
    "\n",
    "### Basic Explode: Arrays to Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode operations\n",
    "print(\"üí• EXPLODE OPERATIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Basic explode - convert array elements to separate rows\n",
    "exploded_df = df.withColumn(\"skill\", explode(\"skills\"))\n",
    "\n",
    "print(\"Original data:\")\n",
    "df.select(\"name\", \"skills\").show(truncate=False)\n",
    "\n",
    "print(\"\\nAfter explode (skills become separate rows):\")\n",
    "exploded_df.select(\"name\", \"skill\").show()\n",
    "\n",
    "print(f\"\\nOriginal rows: {df.count()}\")\n",
    "print(f\"Exploded rows: {exploded_df.count()}\")\n",
    "print(f\"Total skills: {exploded_df.select(\"skill\").distinct().count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explode Outer: Handle Null Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data with null/empty arrays\n",
    "null_data = [\n",
    "    (\"Alice\", [\"Python\", \"Java\"]),\n",
    "    (\"Bob\", []),  # Empty array\n",
    "    (\"Charlie\", None),  # Null array\n",
    "    (\"Diana\", [\"SQL\"])\n",
    "]\n",
    "\n",
    "null_df = spark.createDataFrame(null_data, [\"name\", \"skills\"])\n",
    "print(\"Data with null/empty arrays:\")\n",
    "null_df.show()\n",
    "\n",
    "# explode() fails on null/empty arrays\n",
    "try:\n",
    "    null_df.withColumn(\"skill\", explode(\"skills\")).show()\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå explode() fails: {str(e)[:80]}...\")\n",
    "\n",
    "# explode_outer() handles nulls gracefully\n",
    "print(\"\\n‚úÖ explode_outer() handles nulls:\")\n",
    "null_df.withColumn(\"skill\", explode_outer(\"skills\")).show()\n",
    "\n",
    "# posexplode() includes position index\n",
    "print(\"\\nüìç posexplode() with position:\")\n",
    "null_df.withColumn(\"skill_pos\", posexplode(\"skills\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Explodes and Complex Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex explode scenarios\n",
    "print(\"üîÄ COMPLEX EXPLODE SCENARIOS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create nested array data\n",
    "nested_data = [\n",
    "    (\"Project_A\", [[\"Alice\", \"Bob\"], [\"Charlie\"]], [\"backend\", \"frontend\"]),\n",
    "    (\"Project_B\", [[\"Diana\"], [\"Eve\", \"Frank\"]], [\"database\", \"api\"])\n",
    "]\n",
    "\n",
    "nested_df = spark.createDataFrame(nested_data, \n",
    "    [\"project\", \"teams\", \"components\"])\n",
    "\n",
    "print(\"Nested array data:\")\n",
    "nested_df.show(truncate=False)\n",
    "\n",
    "# Single explode\n",
    "single_explode = nested_df.withColumn(\"team\", explode(\"teams\"))\n",
    "print(\"\\nSingle explode (teams):\")\n",
    "single_explode.select(\"project\", \"team\").show(truncate=False)\n",
    "\n",
    "# Double explode (flatten nested arrays)\n",
    "double_explode = single_explode.withColumn(\"member\", explode(\"team\"))\n",
    "print(\"\\nDouble explode (team members):\")\n",
    "double_explode.select(\"project\", \"member\").show()\n",
    "\n",
    "# Cross product with arrays_zip\n",
    "zipped_df = nested_df.withColumn(\n",
    "    \"team_component\", arrays_zip(\"teams\", \"components\")\n",
    ").withColumn(\n",
    "    \"zipped\", explode(\"team_component\")\n",
    ")\n",
    "\n",
    "print(\"\\nZipped arrays (team ‚Üî component):\")\n",
    "zipped_df.select(\"project\", \"zipped\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Array Operations\n",
    "\n",
    "### Creating and Manipulating Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array operations\n",
    "print(\"üìä ARRAY OPERATIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create arrays from columns\n",
    "array_data = [\n",
    "    (\"Alice\", \"Python\", \"Java\", \"SQL\"),\n",
    "    (\"Bob\", \"Scala\", \"Python\", None),\n",
    "    (\"Charlie\", \"Java\", \"C++\", \"JavaScript\")\n",
    "]\n",
    "\n",
    "array_df = spark.createDataFrame(array_data, \n",
    "    [\"name\", \"skill1\", \"skill2\", \"skill3\"])\n",
    "\n",
    "# Create array column from existing columns\n",
    "array_created = array_df.withColumn(\n",
    "    \"skills_array\", array(\"skill1\", \"skill2\", \"skill3\")\n",
    ")\n",
    "\n",
    "print(\"Created array from columns:\")\n",
    "array_created.select(\"name\", \"skills_array\").show(truncate=False)\n",
    "\n",
    "# Array operations\n",
    "array_ops = array_created.withColumn(\n",
    "    \"array_size\", F.size(\"skills_array\")\n",
    ").withColumn(\n",
    "    \"has_python\", array_contains(\"skills_array\", \"Python\")\n",
    ").withColumn(\n",
    "    \"distinct_skills\", array_distinct(\"skills_array\")\n",
    ").withColumn(\n",
    "    \"sorted_skills\", array_sort(\"skills_array\")\n",
    ").withColumn(\n",
    "    \"first_skill\", col(\"skills_array\")[0]\n",
    ").withColumn(\n",
    "    \"last_two\", F.slice(\"skills_array\", -2, 2)\n",
    ")\n",
    "\n",
    "print(\"\\nArray operations:\")\n",
    "array_ops.select(\n",
    "    \"name\", \"skills_array\", \"array_size\", \"has_python\", \n",
    "    \"distinct_skills\", \"first_skill\", \"last_two\"\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Array Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced array transformations\n",
    "print(\"üîß ADVANCED ARRAY TRANSFORMATIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Transform array elements\n",
    "transformed_arrays = array_created.withColumn(\n",
    "    \"upper_skills\", F.transform(\"skills_array\", lambda x: F.upper(x))\n",
    ").withColumn(\n",
    "    \"skill_lengths\", F.transform(\"skills_array\", lambda x: F.length(x))\n",
    ").withColumn(\n",
    "    \"filtered_skills\", F.filter(\"skills_array\", lambda x: x.isNotNull())\n",
    ").withColumn(\n",
    "    \"skill_exists\", F.exists(\"skills_array\", lambda x: x == \"Python\")\n",
    ")\n",
    "\n",
    "print(\"Array element transformations:\")\n",
    "transformed_arrays.select(\n",
    "    \"name\", \"skills_array\", \"upper_skills\", \"skill_lengths\", \n",
    "    \"filtered_skills\", \"skill_exists\"\n",
    ").show(truncate=False)\n",
    "\n",
    "# Array aggregation\n",
    "array_agg_df = array_created.withColumn(\n",
    "    \"concat_skills\", F.array_join(\"skills_array\", \", \")\n",
    ").withColumn(\n",
    "    \"max_length\", F.array_max(F.transform(\"skills_array\", lambda x: F.length(x)))\n",
    ").withColumn(\n",
    "    \"total_length\", F.aggregate(\n",
    "        F.transform(\"skills_array\", lambda x: F.length(x)),\n",
    "        0,\n",
    "        lambda acc, x: acc + x\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\\nArray aggregations:\")\n",
    "array_agg_df.select(\n",
    "    \"name\", \"concat_skills\", \"max_length\", \"total_length\"\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üó∫Ô∏è Map Operations\n",
    "\n",
    "### Creating and Using Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map operations\n",
    "print(\"üó∫Ô∏è MAP OPERATIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create maps from data\n",
    "map_data = [\n",
    "    (\"Alice\", [\"Python\", \"Java\"], [4, 3]),\n",
    "    (\"Bob\", [\"Scala\", \"Python\"], [2, 4]),\n",
    "    (\"Charlie\", [\"Java\", \"C++\"], [5, 2])\n",
    "]\n",
    "\n",
    "map_df = spark.createDataFrame(map_data, \n",
    "    [\"name\", \"skills\", \"experience_years\"])\n",
    "\n",
    "# Create map from arrays\n",
    "map_created = map_df.withColumn(\n",
    "    \"skill_experience\", map_from_arrays(\"skills\", \"experience_years\")\n",
    ")\n",
    "\n",
    "print(\"Created maps from arrays:\")\n",
    "map_created.select(\"name\", \"skill_experience\").show(truncate=False)\n",
    "\n",
    "# Map operations\n",
    "map_ops = map_created.withColumn(\n",
    "    \"map_keys\", map_keys(\"skill_experience\")\n",
    ").withColumn(\n",
    "    \"map_values\", map_values(\"skill_experience\")\n",
    ").withColumn(\n",
    "    \"map_size\", F.size(\"skill_experience\")\n",
    ").withColumn(\n",
    "    \"python_exp\", col(\"skill_experience\")[\"Python\"]\n",
    ").withColumn(\n",
    "    \"has_java\", F.map_contains_key(\"skill_experience\", \"Java\")\n",
    ")\n",
    "\n",
    "print(\"\\nMap operations:\")\n",
    "map_ops.select(\n",
    "    \"name\", \"skill_experience\", \"map_keys\", \"map_values\", \n",
    "    \"map_size\", \"python_exp\", \"has_java\"\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Struct Operations\n",
    "\n",
    "### Working with Nested Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Struct operations\n",
    "print(\"üèóÔ∏è STRUCT OPERATIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create struct columns\n",
    "struct_df = df.withColumn(\n",
    "    \"profile_struct\", struct(\n",
    "        col(\"profile.experience\").alias(\"years\"),\n",
    "        col(\"profile.level\").alias(\"seniority\")\n",
    "    )\n",
    ").withColumn(\n",
    "    \"location_struct\", struct(\n",
    "        col(\"location.city\").alias(\"city\"),\n",
    "        col(\"location.country\").alias(\"country\")\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Created struct columns:\")\n",
    "struct_df.select(\"name\", \"profile_struct\", \"location_struct\").show(truncate=False)\n",
    "\n",
    "# Access struct fields\n",
    "struct_access = struct_df.withColumn(\n",
    "    \"years_exp\", col(\"profile_struct.years\")\n",
    ").withColumn(\n",
    "    \"seniority\", col(\"profile_struct.seniority\")\n",
    ").withColumn(\n",
    "    \"city\", col(\"location_struct.city\")\n",
    ").withColumn(\n",
    "    \"country\", col(\"location_struct.country\")\n",
    ")\n",
    "\n",
    "print(\"\\nAccessing struct fields:\")\n",
    "struct_access.select(\n",
    "    \"name\", \"years_exp\", \"seniority\", \"city\", \"country\"\n",
    ").show()\n",
    "\n",
    "# Complex struct operations\n",
    "complex_struct = struct_df.withColumn(\n",
    "    \"employee_summary\", struct(\n",
    "        col(\"name\"),\n",
    "        col(\"profile_struct\"),\n",
    "        col(\"location_struct\"),\n",
    "        F.size(\"skills\").alias(\"skill_count\")\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\\nNested struct with summary:\")\n",
    "complex_struct.select(\"employee_summary\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã JSON Data Processing\n",
    "\n",
    "### Parsing JSON Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON processing\n",
    "print(\"üìã JSON DATA PROCESSING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create JSON data\n",
    "json_strings = [\n",
    "    '{\"name\": \"Alice\", \"skills\": [\"Python\", \"Java\"], \"profile\": {\"experience\": 5, \"level\": \"Senior\"}}',\n",
    "    '{\"name\": \"Bob\", \"skills\": [\"Scala\", \"Python\"], \"profile\": {\"experience\": 3, \"level\": \"Mid\"}}',\n",
    "    '{\"name\": \"Charlie\", \"skills\": [\"Java\", \"C++\"], \"profile\": {\"experience\": 7, \"level\": \"Senior\"}}'\n",
    "]\n",
    "\n",
    "json_df = spark.createDataFrame([(json_str,) for json_str in json_strings], [\"json_data\"])\n",
    "\n",
    "print(\"JSON strings:\")\n",
    "json_df.show(truncate=False)\n",
    "\n",
    "# Parse JSON\n",
    "parsed_df = json_df.withColumn(\n",
    "    \"parsed\", F.from_json(\"json_data\", \n",
    "        \"name string, skills array<string>, profile struct<experience int, level string>\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\\nParsed JSON:\")\n",
    "parsed_df.select(\"parsed\").show(truncate=False)\n",
    "\n",
    "# Extract from parsed JSON\n",
    "extracted_df = parsed_df.withColumn(\n",
    "    \"employee_name\", col(\"parsed.name\")\n",
    ").withColumn(\n",
    "    \"skill_list\", col(\"parsed.skills\")\n",
    ").withColumn(\n",
    "    \"experience\", col(\"parsed.profile.experience\")\n",
    ").withColumn(\n",
    "    \"level\", col(\"parsed.profile.level\")\n",
    ")\n",
    "\n",
    "print(\"\\nExtracted JSON fields:\")\n",
    "extracted_df.select(\n",
    "    \"employee_name\", \"skill_list\", \"experience\", \"level\"\n",
    ").show(truncate=False)\n",
    "\n",
    "# Explode JSON arrays\n",
    "final_df = extracted_df.withColumn(\"skill\", explode_outer(\"skill_list\"))\n",
    "print(\"\\nExploded skills from JSON:\")\n",
    "final_df.select(\"employee_name\", \"skill\", \"experience\", \"level\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé® Real-World Use Cases\n",
    "\n",
    "### Processing Nested Customer Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real-world example: Customer analytics\n",
    "print(\"üé® REAL-WORLD USE CASES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Simulate customer data with complex structures\n",
    "customer_data = [\n",
    "    {\n",
    "        \"customer_id\": \"C001\",\n",
    "        \"name\": \"Alice Johnson\",\n",
    "        \"orders\": [\n",
    "            {\"order_id\": \"O001\", \"amount\": 150.00, \"items\": [\"laptop\", \"mouse\"]},\n",
    "            {\"order_id\": \"O002\", \"amount\": 75.50, \"items\": [\"book\"]}\n",
    "        ],\n",
    "        \"preferences\": {\"category\": \"electronics\", \"notifications\": True},\n",
    "        \"addresses\": [\n",
    "            {\"type\": \"home\", \"city\": \"New York\", \"country\": \"USA\"},\n",
    "            {\"type\": \"work\", \"city\": \"Newark\", \"country\": \"USA\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"customer_id\": \"C002\",\n",
    "        \"name\": \"Bob Smith\",\n",
    "        \"orders\": [\n",
    "            {\"order_id\": \"O003\", \"amount\": 200.00, \"items\": [\"phone\", \"case\"]}\n",
    "        ],\n",
    "        \"preferences\": {\"category\": \"mobile\", \"notifications\": False},\n",
    "        \"addresses\": [\n",
    "            {\"type\": \"home\", \"city\": \"London\", \"country\": \"UK\"}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create DataFrame from nested data\n",
    "customer_df = spark.createDataFrame(customer_data)\n",
    "\n",
    "print(\"Customer data with nested structures:\")\n",
    "customer_df.show(truncate=False)\n",
    "customer_df.printSchema()\n",
    "\n",
    "# Extract order details (explode orders)\n",
    "order_details = customer_df.withColumn(\n",
    "    \"order\", explode(\"orders\")\n",
    ").select(\n",
    "    \"customer_id\", \"name\",\n",
    "    col(\"order.order_id\").alias(\"order_id\"),\n",
    "    col(\"order.amount\").alias(\"order_amount\"),\n",
    "    col(\"order.items\").alias(\"order_items\")\n",
    ")\n",
    "\n",
    "print(\"\\nFlattened order details:\")\n",
    "order_details.show()\n",
    "\n",
    "# Extract item-level details (double explode)\n",
    "item_details = order_details.withColumn(\n",
    "    \"item\", explode(\"order_items\")\n",
    ").select(\"customer_id\", \"name\", \"order_id\", \"order_amount\", \"item\")\n",
    "\n",
    "print(\"\\nItem-level details:\")\n",
    "item_details.show()\n",
    "\n",
    "# Customer analytics\n",
    "analytics = customer_df.withColumn(\n",
    "    \"total_orders\", F.size(\"orders\")\n",
    ").withColumn(\n",
    "    \"total_spent\", F.aggregate(\n",
    "        F.transform(\"orders\", lambda x: x.amount),\n",
    "        0.0,\n",
    "        lambda acc, x: acc + x\n",
    "    )\n",
    ").withColumn(\n",
    "    \"avg_order_value\", col(\"total_spent\") / col(\"total_orders\")\n",
    ").withColumn(\n",
    "    \"preferred_category\", col(\"preferences.category\")\n",
    ").withColumn(\n",
    "    \"home_city\", F.filter(\"addresses\", lambda x: x.type == \"home\")[0].city\n",
    ")\n",
    "\n",
    "print(\"\\nCustomer analytics summary:\")\n",
    "analytics.select(\n",
    "    \"customer_id\", \"name\", \"total_orders\", \"total_spent\", \n",
    "    \"avg_order_value\", \"preferred_category\", \"home_city\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üö® Common Mistakes and Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common mistakes\n",
    "print(\"üö® COMMON MISTAKES WITH COMPLEX DATA TYPES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Mistake 1: explode() on null arrays\n",
    "print(\"‚ùå Mistake: explode() on null arrays\")\n",
    "null_array_df = spark.createDataFrame([(\"Alice\", None), (\"Bob\", [\"Java\"])], [\"name\", \"skills\"])\n",
    "try:\n",
    "    null_array_df.withColumn(\"skill\", explode(\"skills\")).show()\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)[:80]}...\")\n",
    "\n",
    "print(\"\\n‚úÖ Solution: Use explode_outer()\")\n",
    "null_array_df.withColumn(\"skill\", explode_outer(\"skills\")).show()\n",
    "\n",
    "# Mistake 2: Accessing non-existent struct fields\n",
    "print(\"\\n‚ùå Mistake: Wrong struct field access\")\n",
    "test_struct = spark.createDataFrame([(\"Alice\", {\"age\": 25, \"city\": \"NY\"})], [\"name\", \"info\"])\n",
    "try:\n",
    "    test_struct.withColumn(\"wrong_field\", col(\"info.nonexistent\")).show()\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)[:80]}...\")\n",
    "\n",
    "print(\"\\n‚úÖ Solution: Check schema first\")\n",
    "test_struct.printSchema()\n",
    "print(\"Correct access:\")\n",
    "test_struct.withColumn(\"age\", col(\"info.age\")).withColumn(\"city\", col(\"info.city\")).show()\n",
    "\n",
    "# Mistake 3: Array index out of bounds\n",
    "print(\"\\n‚ùå Mistake: Array index out of bounds\")\n",
    "array_bounds = spark.createDataFrame([(\"Alice\", [\"Java\"]), (\"Bob\", [\"Python\", \"Scala\"])], [\"name\", \"skills\"])\n",
    "try:\n",
    "    array_bounds.withColumn(\"third_skill\", col(\"skills\")[2]).show()\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)[:80]}...\")\n",
    "\n",
    "print(\"\\n‚úÖ Solution: Check array size first\")\n",
    "array_bounds.withColumn(\"array_size\", F.size(\"skills\")) \\\n",
    "    .withColumn(\"second_skill\", \n",
    "                when(F.size(\"skills\") > 1, col(\"skills\")[1])\n",
    "                .otherwise(\"N/A\")) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Key Takeaways\n",
    "\n",
    "### What You Learned:\n",
    "- ‚úÖ **`explode()` & `explode_outer()`** - Convert arrays to rows\n",
    "- ‚úÖ **Array operations** - create, manipulate, transform arrays\n",
    "- ‚úÖ **Map operations** - work with key-value pairs\n",
    "- ‚úÖ **Struct operations** - handle nested data structures\n",
    "- ‚úÖ **JSON processing** - parse and extract from JSON data\n",
    "- ‚úÖ **Complex transformations** - nested operations and flattening\n",
    "\n",
    "### Essential Functions:\n",
    "- üî∏ `explode(col)` - Array to rows (fails on null)\n",
    "- üî∏ `explode_outer(col)` - Array to rows (handles null)\n",
    "- üî∏ `posexplode(col)` - Array to rows with position\n",
    "- üî∏ `arrays_zip(*cols)` - Zip multiple arrays\n",
    "- üî∏ `array(*cols)` - Create array from columns\n",
    "- üî∏ `array_contains(arr, value)` - Check if array contains value\n",
    "- üî∏ `map_from_arrays(keys, values)` - Create map from arrays\n",
    "- üî∏ `struct(*cols)` - Create struct from columns\n",
    "- üî∏ `from_json(col, schema)` - Parse JSON strings\n",
    "\n",
    "### Performance Considerations:\n",
    "- üî∏ `explode()` can significantly increase row count\n",
    "- üî∏ Complex nested operations may require multiple passes\n",
    "- üî∏ Use `explode_outer()` to avoid null-related failures\n",
    "- üî∏ Consider caching intermediate results for complex pipelines\n",
    "- üî∏ Monitor data skew after explode operations\n",
    "\n",
    "### Common Patterns:\n",
    "- üî∏ **Flattening JSON**: `from_json() + explode() + struct access`\n",
    "- üî∏ **Array filtering**: `filter(array_col, lambda x: condition)`\n",
    "- üî∏ **Map transformations**: `transform(map_col, lambda k,v: operation)`\n",
    "- üî∏ **Nested aggregations**: `aggregate(array_col, initial, merge_func)`\n",
    "- üî∏ **Struct field access**: `col(\"struct_col.field_name\")`\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Next Steps\n",
    "\n",
    "Now that you master complex data types, you're ready for:\n",
    "\n",
    "1. **DataFrame Joins** - Combining multiple DataFrames\n",
    "2. **Spark SQL Integration** - SQL interface for DataFrames\n",
    "3. **Advanced Analytics** - Machine learning and streaming\n",
    "4. **Production Data Processing** - Handling real-world data at scale\n",
    "\n",
    "**Complex data types are essential for modern data processing!**\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Congratulations! You now have the power to wrangle the most complex nested data structures in Spark!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
