{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üì° Broadcast Join: Optimizing Small Table Joins\n",
    "\n",
    "**Time to complete:** 30 minutes  \n",
    "**Difficulty:** Advanced  \n",
    "**Prerequisites:** Basic joins, DataFrame operations\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will master:\n",
    "- ‚úÖ **Broadcast join mechanics** - How it works\n",
    "- ‚úÖ **When to use broadcast joins** - Optimal scenarios\n",
    "- ‚úÖ **Manual broadcast control** - Forcing broadcast behavior\n",
    "- ‚úÖ **Performance monitoring** - Measuring broadcast effectiveness\n",
    "- ‚úÖ **Broadcast join limitations** - When it doesn't work\n",
    "- ‚úÖ **Alternative strategies** - When broadcast isn't feasible\n",
    "\n",
    "**Broadcast joins can be 10-100x faster than shuffle joins!**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Understanding Broadcast Joins\n",
    "\n",
    "**Broadcast join** is Spark's optimization for joining a large table with a small table. Instead of shuffling both tables across the network, it sends the small table to all executors.\n",
    "\n",
    "### How Broadcast Join Works:\n",
    "```\n",
    "Traditional Join:     Broadcast Join:\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ Large Table ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   Shuffle   ‚îÇ\n",
    "‚îÇ (1TB)       ‚îÇ       ‚îÇ   Both      ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ   Tables    ‚îÇ\n",
    "        ‚îÇ             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê             ‚îÇ\n",
    "‚îÇ Small Table ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂   Network Heavy\n",
    "‚îÇ (100MB)     ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "vs\n",
    "\n",
    "Broadcast Join:\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ Large Table ‚îÇ       ‚îÇ Large Table ‚îÇ\n",
    "‚îÇ (1TB)       ‚îÇ       ‚îÇ (1TB)       ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "        ‚îÇ                   ‚îÇ\n",
    "        ‚ñº                   ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ Small Table ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂   ‚îÇSmall Table  ‚îÇ (broadcasted)\n",
    "‚îÇ (100MB)     ‚îÇ       ‚îÇ  in Memory  ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                      Network Light ‚ö°\n",
    "```\n",
    "\n",
    "**Network traffic reduction: 99%+ for typical use cases!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, broadcast\n",
    "import pyspark.sql.functions as F\n",
    "import time\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Broadcast_Joins\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"‚úÖ Spark ready - Version: {spark.version}\")\n",
    "\n",
    "# Check current broadcast threshold\n",
    "threshold = spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")\n",
    "print(f\"Current broadcast threshold: {threshold} bytes\")\n",
    "\n",
    "# Create test datasets\n",
    "# Large table (simulating fact table)\n",
    "large_orders = [\n",
    "    (i, f\"customer_{i%1000}\", f\"product_{(i%100)+1}\", 10 + (i % 90), f\"2023-{(i%12)+1:02d}-01\")\n",
    "    for i in range(50000)\n",
    "]\n",
    "\n",
    "# Medium table (simulating dimension table)\n",
    "medium_products = [\n",
    "    (f\"product_{i}\", f\"Category_{(i%5)+1}\", f\"Brand_{(i%3)+1}\", 20.0 + (i % 80))\n",
    "    for i in range(1, 101)\n",
    "]\n",
    "\n",
    "# Small table (lookup/dimension table)\n",
    "small_categories = [\n",
    "    (\"Category_1\", \"Electronics\", \"High-tech products\"),\n",
    "    (\"Category_2\", \"Clothing\", \"Fashion and apparel\"),\n",
    "    (\"Category_3\", \"Books\", \"Educational materials\"),\n",
    "    (\"Category_4\", \"Home\", \"Household items\"),\n",
    "    (\"Category_5\", \"Sports\", \"Athletic equipment\")\n",
    "]\n",
    "\n",
    "# Create DataFrames\n",
    "orders_df = spark.createDataFrame(large_orders, \n",
    "    [\"order_id\", \"customer_id\", \"product_id\", \"quantity\", \"order_date\"])\n",
    "products_df = spark.createDataFrame(medium_products, \n",
    "    [\"product_id\", \"category_id\", \"brand\", \"base_price\"])\n",
    "categories_df = spark.createDataFrame(small_categories, \n",
    "    [\"category_id\", \"category_name\", \"description\"])\n",
    "\n",
    "print(\"üìä Test Datasets:\")\n",
    "print(f\"Orders: {orders_df.count():,} rows\")\n",
    "print(f\"Products: {products_df.count()} rows\")\n",
    "print(f\"Categories: {categories_df.count()} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö° Automatic Broadcast Detection\n",
    "\n",
    "### Spark's Auto-Broadcast Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate automatic broadcast detection\n",
    "print(\"‚ö° AUTOMATIC BROADCAST DETECTION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check DataFrame sizes (approximate)\n",
    "print(\"DataFrame size estimates:\")\n",
    "print(f\"Orders: ~{orders_df.count() * len(orders_df.columns) * 20:,} bytes\")\n",
    "print(f\"Products: ~{products_df.count() * len(products_df.columns) * 20:,} bytes\")\n",
    "print(f\"Categories: ~{categories_df.count() * len(categories_df.columns) * 20:,} bytes\")\n",
    "print(f\"Broadcast threshold: {int(threshold):,} bytes\")\n",
    "\n",
    "# Join orders with categories (should auto-broadcast)\n",
    "print(\"\\nJoining orders (large) with categories (small):\")\n",
    "orders_with_categories = orders_df.join(\n",
    "    products_df,\n",
    "    \"product_id\",\n",
    "    \"inner\"\n",
    ").join(\n",
    "    categories_df,\n",
    "    \"category_id\",\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "print(f\"Result: {orders_with_categories.count():,} rows\")\n",
    "\n",
    "# Check execution plan for broadcast hints\n",
    "print(\"\\nExecution plan (look for 'BroadcastHashJoin'):\")\n",
    "orders_with_categories.explain(mode=\"formatted\")\n",
    "\n",
    "# Show sample result\n",
    "print(\"\\nSample results:\")\n",
    "orders_with_categories.select(\n",
    "    \"order_id\", \"customer_id\", \"category_name\", \"quantity\", \"order_date\"\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Manual Broadcast Control\n",
    "\n",
    "### Forcing Broadcast Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual broadcast control\n",
    "print(\"üéØ MANUAL BROADCAST CONTROL\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Force broadcast even if above threshold\n",
    "print(\"1. Forcing broadcast with broadcast() function:\")\n",
    "forced_broadcast = orders_df.join(\n",
    "    broadcast(products_df),  # Force broadcast\n",
    "    \"product_id\",\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "print(f\"Forced broadcast result: {forced_broadcast.count()} rows\")\n",
    "print(\"Execution plan:\")\n",
    "forced_broadcast.explain(mode=\"formatted\")\n",
    "\n",
    "# Change broadcast threshold for testing\n",
    "print(\"\\n2. Testing different threshold settings:\")\n",
    "\n",
    "# Very restrictive threshold (only very small tables)\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"1KB\")\n",
    "print(f\"Restrictive threshold: {spark.conf.get('spark.sql.autoBroadcastJoinThreshold')}\")\n",
    "\n",
    "restrictive_join = orders_df.join(products_df, \"product_id\", \"inner\")\n",
    "print(\"With restrictive threshold:\")\n",
    "restrictive_join.explain(mode=\"formatted\")\n",
    "\n",
    "# Very permissive threshold (broadcast larger tables)\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"100MB\")\n",
    "print(f\"\\nPermissive threshold: {spark.conf.get('spark.sql.autoBroadcastJoinThreshold')}\")\n",
    "\n",
    "permissive_join = orders_df.join(products_df, \"product_id\", \"inner\")\n",
    "print(\"With permissive threshold:\")\n",
    "permissive_join.explain(mode=\"formatted\")\n",
    "\n",
    "# Reset to default\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"10MB\")\n",
    "print(f\"\\nReset to default: {spark.conf.get('spark.sql.autoBroadcastJoinThreshold')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Performance Comparison\n",
    "\n",
    "### Broadcast vs Shuffle Join Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison\n",
    "print(\"üöÄ PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create larger test datasets\n",
    "big_orders = [\n",
    "    (i, f\"customer_{i%2000}\", f\"product_{(i%200)+1}\", 1 + (i % 10))\n",
    "    for i in range(100000)\n",
    "]\n",
    "\n",
    "big_orders_df = spark.createDataFrame(big_orders, \n",
    "    [\"order_id\", \"customer_id\", \"product_id\", \"quantity\"])\n",
    "\n",
    "# Small lookup table\n",
    "small_lookup = [\n",
    "    (f\"product_{i}\", f\"Category_{(i%5)+1}\", f\"Supplier_{(i%3)+1}\")\n",
    "    for i in range(1, 201)\n",
    "]\n",
    "\n",
    "small_lookup_df = spark.createDataFrame(small_lookup, \n",
    "    [\"product_id\", \"category\", \"supplier\"])\n",
    "\n",
    "print(f\"Big orders: {big_orders_df.count():,} rows\")\n",
    "print(f\"Small lookup: {small_lookup_df.count()} rows\")\n",
    "\n",
    "# Test 1: Regular join (may or may not broadcast)\n",
    "print(\"\\n=== Test 1: Regular Join ===\")\n",
    "start_time = time.time()\n",
    "regular_join = big_orders_df.join(small_lookup_df, \"product_id\", \"inner\")\n",
    "regular_count = regular_join.count()\n",
    "regular_time = time.time() - start_time\n",
    "\n",
    "print(f\"Regular join: {regular_count:,} rows in {regular_time:.3f} seconds\")\n",
    "\n",
    "# Test 2: Forced broadcast join\n",
    "print(\"\\n=== Test 2: Forced Broadcast Join ===\")\n",
    "start_time = time.time()\n",
    "broadcast_join = big_orders_df.join(broadcast(small_lookup_df), \"product_id\", \"inner\")\n",
    "broadcast_count = broadcast_join.count()\n",
    "broadcast_time = time.time() - start_time\n",
    "\n",
    "print(f\"Broadcast join: {broadcast_count:,} rows in {broadcast_time:.3f} seconds\")\n",
    "\n",
    "# Performance analysis\n",
    "print(f\"\\nüéØ PERFORMANCE ANALYSIS:\")\n",
    "print(f\"Regular join: {regular_time:.3f}s\")\n",
    "print(f\"Broadcast join: {broadcast_time:.3f}s\")\n",
    "\n",
    "if regular_time > 0 and broadcast_time > 0:\n",
    "    speedup = regular_time / broadcast_time\n",
    "    if speedup > 1:\n",
    "        print(f\"Broadcast is {speedup:.1f}x faster!\")\n",
    "    else:\n",
    "        print(f\"Broadcast is {1/speedup:.1f}x slower (threshold effects)\")\n",
    "\n",
    "# Verify results are identical\n",
    "print(f\"\\nResults identical: {regular_count == broadcast_count}\")\n",
    "\n",
    "# Show execution plans\n",
    "print(\"\\nRegular join plan:\")\n",
    "regular_join.explain(mode=\"simple\")\n",
    "\n",
    "print(\"\\nBroadcast join plan:\")\n",
    "broadcast_join.explain(mode=\"simple\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéõÔ∏è Advanced Broadcast Scenarios\n",
    "\n",
    "### Complex Join Conditions with Broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced broadcast scenarios\n",
    "print(\"üéõÔ∏è ADVANCED BROADCAST SCENARIOS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create complex lookup table\n",
    "complex_lookup = [\n",
    "    (\"Electronics\", 500, 1000, \"premium\"),\n",
    "    (\"Clothing\", 50, 200, \"standard\"),\n",
    "    (\"Books\", 10, 100, \"standard\"),\n",
    "    (\"Home\", 100, 500, \"standard\"),\n",
    "    (\"Sports\", 75, 300, \"premium\")\n",
    "]\n",
    "\n",
    "category_rules_df = spark.createDataFrame(complex_lookup, \n",
    "    [\"category\", \"min_price\", \"max_price\", \"tier\"])\n",
    "\n",
    "print(\"Category rules lookup table:\")\n",
    "category_rules_df.show()\n",
    "\n",
    "# Complex broadcast join with multiple conditions\n",
    "complex_broadcast = orders_df.alias(\"o\").join(\n",
    "    broadcast(products_df.alias(\"p\")),\n",
    "    col(\"o.product_id\") == col(\"p.product_id\"),\n",
    "    \"inner\"\n",
    ").join(\n",
    "    broadcast(category_rules_df.alias(\"r\")),\n",
    "    (col(\"p.category_id\") == col(\"r.category\")) &\n",
    "    (col(\"p.base_price\").between(col(\"r.min_price\"), col(\"r.max_price\"))),\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "print(\"\\nComplex broadcast join result:\")\n",
    "complex_broadcast.select(\n",
    "    \"o.order_id\", \"o.customer_id\", \"p.category_id\", \n",
    "    \"p.base_price\", \"r.tier\", \"r.min_price\", \"r.max_price\"\n",
    ").show(10)\n",
    "\n",
    "# Broadcast with aggregation\n",
    "broadcast_agg = orders_df.join(\n",
    "    broadcast(products_df),\n",
    "    \"product_id\",\n",
    "    \"inner\"\n",
    ").groupBy(\"category_id\").agg(\n",
    "    F.sum(\"quantity\").alias(\"total_quantity\"),\n",
    "    F.sum(F.col(\"quantity\") * F.col(\"base_price\")).alias(\"total_revenue\"),\n",
    "    F.avg(\"quantity\").alias(\"avg_quantity\"),\n",
    "    F.count(\"*\").alias(\"order_count\")\n",
    ")\n",
    "\n",
    "print(\"\\nBroadcast join with aggregation:\")\n",
    "broadcast_agg.show()\n",
    "\n",
    "# Check execution plan\n",
    "print(\"\\nExecution plan:\")\n",
    "broadcast_agg.explain(mode=\"formatted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Broadcast Join Limitations\n",
    "\n",
    "### When Broadcast Joins Don't Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcast limitations\n",
    "print(\"‚ö†Ô∏è BROADCAST JOIN LIMITATIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create a 'large' table that exceeds broadcast threshold\n",
    "large_lookup = [\n",
    "    (f\"key_{i}\", f\"data_{i}\", i * 10) \n",
    "    for i in range(10000)\n",
    "]  # This will be > 10MB\n",
    "\n",
    "large_lookup_df = spark.createDataFrame(large_lookup, \n",
    "    [\"lookup_key\", \"lookup_data\", \"lookup_value\"])\n",
    "\n",
    "# Check if it would be broadcasted\n",
    "print(f\"Large lookup table size: {large_lookup_df.count()} rows\")\n",
    "\n",
    "# Try join without broadcast hint\n",
    "no_broadcast_join = big_orders_df.join(\n",
    "    large_lookup_df,  # This is too big for auto-broadcast\n",
    "    big_orders_df[\"customer_id\"] == large_lookup_df[\"lookup_key\"],  # Non-matching keys\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "print(\"\\nJoin with large table (no broadcast hint):\")\n",
    "no_broadcast_join.explain(mode=\"formatted\")\n",
    "\n",
    "# Force broadcast (may cause performance issues)\n",
    "print(\"\\n‚ö†Ô∏è  Forcing broadcast on large table:\")\n",
    "try:\n",
    "    forced_large_broadcast = big_orders_df.join(\n",
    "        broadcast(large_lookup_df),  # Force broadcast large table\n",
    "        big_orders_df[\"customer_id\"] == large_lookup_df[\"lookup_key\"],\n",
    "        \"inner\"\n",
    "    )\n",
    "    print(\"Forced broadcast succeeded (may be slow)\")\n",
    "    print(f\"Result count: {forced_large_broadcast.count()}\")\n",
    "except Exception as e:\n",
    "    print(f\"Forced broadcast failed: {str(e)[:100]}...\")\n",
    "\n",
    "# Limitations summary\n",
    "print(\"\\nüö´ BROADCAST JOIN LIMITATIONS:\")\n",
    "print(\"1. Table size limit (default: 10MB)\")\n",
    "print(\"2. Memory pressure on executors\")\n",
    "print(\"3. Network overhead for very large broadcasts\")\n",
    "print(\"4. Not suitable for both tables being large\")\n",
    "print(\"5. Limited by executor memory\")\n",
    "print(\"6. May cause OOM for executors with limited memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Best Practices & Optimization\n",
    "\n",
    "### Broadcast Join Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best practices\n",
    "print(\"üéØ BROADCAST JOIN BEST PRACTICES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Strategy 1: Pre-filter small tables\n",
    "print(\"1. Pre-filter small tables:\")\n",
    "filtered_categories = categories_df.filter(col(\"category_id\").isin([\"Category_1\", \"Category_2\"]))\n",
    "print(f\"Filtered categories: {filtered_categories.count()} rows\")\n",
    "\n",
    "# Join with filtered broadcast table\n",
    "optimized_join = orders_df.join(\n",
    "    broadcast(filtered_categories),\n",
    "    orders_df[\"product_id\"] == filtered_categories[\"category_id\"],  # Simplified condition\n",
    "    \"inner\"\n",
    ")\n",
    "print(f\"Optimized join result: {optimized_join.count()} rows\")\n",
    "\n",
    "# Strategy 2: Cache broadcast tables\n",
    "print(\"\\n2. Cache frequently used broadcast tables:\")\n",
    "cached_categories = categories_df.cache()\n",
    "print(\"Categories table cached\")\n",
    "\n",
    "# Multiple joins with same cached table\n",
    "join1 = orders_df.join(broadcast(cached_categories), orders_df[\"product_id\"].substr(1, 10) == cached_categories[\"category_id\"], \"left\")\n",
    "join2 = products_df.join(broadcast(cached_categories), products_df[\"category_id\"] == cached_categories[\"category_id\"], \"inner\")\n",
    "\n",
    "print(f\"Multiple joins with cached broadcast table\")\n",
    "print(f\"Join 1: {join1.count()} rows\")\n",
    "print(f\"Join 2: {join2.count()} rows\")\n",
    "\n",
    "# Strategy 3: Monitor broadcast effectiveness\n",
    "print(\"\\n3. Monitor broadcast performance:\")\n",
    "print(\"Check Spark UI for:\")\n",
    "print(\"- BroadcastHashJoin in execution plan\")\n",
    "print(\"- Broadcast exchange size\")\n",
    "print(\"- Task execution times\")\n",
    "print(\"- Memory usage per executor\")\n",
    "\n",
    "# Strategy 4: Adaptive query execution\n",
    "print(\"\\n4. Enable adaptive query execution:\")\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
    "print(\"Adaptive query execution enabled\")\n",
    "print(\"Spark will automatically choose optimal join strategies\")\n",
    "\n",
    "# Strategy 5: Combine with other optimizations\n",
    "print(\"\\n5. Combine broadcast with partitioning:\")\n",
    "partitioned_orders = orders_df.repartition(8, \"customer_id\")\n",
    "broadcast_partitioned = partitioned_orders.join(\n",
    "    broadcast(categories_df),\n",
    "    partitioned_orders[\"customer_id\"].substr(1, 9) == categories_df[\"category_id\"],\n",
    "    \"inner\"\n",
    ")\n",
    "print(f\"Broadcast + partitioning: {broadcast_partitioned.count()} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Monitoring & Debugging\n",
    "\n",
    "### Broadcast Join Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitoring and debugging\n",
    "print(\"üîç BROADCAST JOIN MONITORING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create a test scenario\n",
    "test_orders = orders_df.limit(1000)\n",
    "test_join = test_orders.join(broadcast(categories_df), \n",
    "    test_orders[\"product_id\"].substr(1, 9) == categories_df[\"category_id\"], \"inner\")\n",
    "\n",
    "# 1. Check execution plan\n",
    "print(\"1. Execution Plan Analysis:\")\n",
    "print(\"Look for 'BroadcastHashJoin' and broadcast size:\")\n",
    "test_join.explain(mode=\"formatted\")\n",
    "\n",
    "# 2. Performance metrics\n",
    "print(\"\\n2. Performance Metrics:\")\n",
    "start_time = time.time()\n",
    "result_count = test_join.count()\n",
    "execution_time = time.time() - start_time\n",
    "print(f\"Execution time: {execution_time:.3f} seconds\")\n",
    "print(f\"Result count: {result_count}\")\n",
    "\n",
    "# 3. Memory usage estimation\n",
    "print(\"\\n3. Memory Usage Estimation:\")\n",
    "broadcast_size_bytes = categories_df.count() * len(categories_df.columns) * 50  # Rough estimate\n",
    "broadcast_size_mb = broadcast_size_bytes / (1024 * 1024)\n",
    "print(f\"Estimated broadcast table size: {broadcast_size_mb:.2f} MB\")\n",
    "print(f\"Broadcast threshold: {int(spark.conf.get('spark.sql.autoBroadcastJoinThreshold')) / (1024*1024):.0f} MB\")\n",
    "\n",
    "# 4. Data distribution analysis\n",
    "print(\"\\n4. Data Distribution:\")\n",
    "result_distribution = test_join.groupBy(\"category_id\").count().orderBy(\"count\", ascending=False)\n",
    "print(\"Result distribution by category:\")\n",
    "result_distribution.show()\n",
    "\n",
    "# 5. Common issues detection\n",
    "print(\"\\n5. Common Issues Detection:\")\n",
    "if broadcast_size_mb > 100:\n",
    "    print(\"‚ö†Ô∏è  WARNING: Broadcast table is very large (>100MB)\")\n",
    "    print(\"   Consider: Increasing executor memory or using regular join\")\n",
    "elif execution_time > 30:\n",
    "    print(\"‚ö†Ô∏è  WARNING: Join took longer than 30 seconds\")\n",
    "    print(\"   Consider: Checking data skew or network issues\")\n",
    "else:\n",
    "    print(\"‚úÖ Broadcast join appears to be working efficiently\")\n",
    "\n",
    "# 6. Alternative join strategies for comparison\n",
    "print(\"\\n6. Alternative Strategy Comparison:\")\n",
    "regular_join = test_orders.join(categories_df, \n",
    "    test_orders[\"product_id\"].substr(1, 9) == categories_df[\"category_id\"], \"inner\")\n",
    "print(\"Regular join plan:\")\n",
    "regular_join.explain(mode=\"simple\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Interview Questions & Key Takeaways\n",
    "\n",
    "### Common Interview Questions:\n",
    "1. **What is a broadcast join in Spark?**\n",
    "2. **When should you use broadcast joins?**\n",
    "3. **What's the broadcast join threshold?**\n",
    "4. **How do you force a broadcast join?**\n",
    "5. **What are the limitations of broadcast joins?**\n",
    "\n",
    "### Answers:\n",
    "- **Broadcast join**: Sends small table to all executors to avoid shuffling large table\n",
    "- **When to use**: Joining large table with small lookup table (< 10MB default)\n",
    "- **Threshold**: `spark.sql.autoBroadcastJoinThreshold` (default: 10MB)\n",
    "- **Force broadcast**: Use `broadcast()` function: `df1.join(broadcast(df2), \"key\")`\n",
    "- **Limitations**: Table size, memory pressure, network overhead for large broadcasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "print(\"üßπ CLEANUP\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Reset configurations\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"10MB\")\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "\n",
    "print(\"Configurations reset to defaults\")\n",
    "print(\"Broadcast join demonstration complete!\")\n",
    "\n",
    "print(\"\\nüìö KEY TAKEAWAYS:\")\n",
    "print(\"- Broadcast joins send small tables to all executors\")\n",
    "print(\"- Use for large table + small lookup table scenarios\")\n",
    "print(\"- Can be 10-100x faster than shuffle joins\")\n",
    "print(\"- Monitor execution plans and performance\")\n",
    "print(\"- Consider memory and network limitations\")\n",
    "print(\"- Spark auto-detects, but you can force with broadcast()\")\n",
    "\n",
    "# Note: Spark Session will be cleaned up automatically in Jupyter\n",
    "# In production code, use: spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
