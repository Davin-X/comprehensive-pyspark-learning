{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ”— DataFrame Joins: Inner, Left, Right & Full\n",
    "\n",
    "**Time to complete:** 40 minutes  \n",
    "**Difficulty:** Intermediate  \n",
    "**Prerequisites:** DataFrame basics, aggregations\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will master:\n",
    "- âœ… **Inner Join** - Only matching rows\n",
    "- âœ… **Left Join** - All left rows, matching right rows\n",
    "- âœ… **Right Join** - All right rows, matching left rows\n",
    "- âœ… **Full Outer Join** - All rows from both sides\n",
    "- âœ… **Join conditions** - Equality and non-equality joins\n",
    "- âœ… **Multiple table joins** - Joining more than 2 DataFrames\n",
    "- âœ… **Performance optimization** - Efficient join strategies\n",
    "\n",
    "**Joins are fundamental to relational data processing!**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” Understanding DataFrame Joins\n",
    "\n",
    "**Joins combine data from multiple DataFrames based on common columns or conditions.**\n",
    "\n",
    "### Join Types:\n",
    "- **Inner Join**: Only rows with matching keys in both DataFrames\n",
    "- **Left Join**: All rows from left DataFrame, matching rows from right\n",
    "- **Right Join**: All rows from right DataFrame, matching rows from left\n",
    "- **Full Outer Join**: All rows from both DataFrames\n",
    "- **Cross Join**: Cartesian product of all rows\n",
    "- **Anti Join**: Rows that don't have matches\n",
    "\n",
    "### Join Performance:\n",
    "- **Broadcast Join**: Small table fits in memory\n",
    "- **Shuffle Hash Join**: Large tables, hash-based\n",
    "- **Sort-Merge Join**: Large sorted tables\n",
    "- **Nested Loop Join**: Fallback, usually slow\n",
    "\n",
    "**Joins are expensive operations - they require data shuffling!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, lit, coalesce\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataFrame_Joins\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"âœ… Spark ready - Version: {spark.version}\")\n",
    "\n",
    "# Create employees DataFrame\n",
    "employees_data = [\n",
    "    (1, \"Alice\", \"Engineering\", 75000),\n",
    "    (2, \"Bob\", \"Sales\", 65000),\n",
    "    (3, \"Charlie\", \"Engineering\", 85000),\n",
    "    (4, \"Diana\", \"HR\", 55000),\n",
    "    (5, \"Eve\", \"Sales\", 70000),\n",
    "    (6, \"Frank\", \"Engineering\", 80000)\n",
    "]\n",
    "\n",
    "employees_df = spark.createDataFrame(employees_data, \n",
    "    [\"emp_id\", \"name\", \"department\", \"salary\"])\n",
    "\n",
    "# Create departments DataFrame\n",
    "departments_data = [\n",
    "    (\"Engineering\", \"Building software\", \"New York\"),\n",
    "    (\"Sales\", \"Selling products\", \"Chicago\"),\n",
    "    (\"HR\", \"Managing people\", \"Boston\"),\n",
    "    (\"Marketing\", \"Brand promotion\", \"Los Angeles\"),\n",
    "    (\"Finance\", \"Financial management\", \"New York\")\n",
    "]\n",
    "\n",
    "departments_df = spark.createDataFrame(departments_data, \n",
    "    [\"dept_name\", \"description\", \"location\"])\n",
    "\n",
    "# Create projects DataFrame\n",
    "projects_data = [\n",
    "    (101, \"Website Redesign\", \"Engineering\", 3),\n",
    "    (102, \"Sales CRM\", \"Sales\", 2),\n",
    "    (103, \"Talent Acquisition\", \"HR\", 1),\n",
    "    (104, \"Brand Campaign\", \"Marketing\", 4),\n",
    "    (105, \"Budget Planning\", \"Finance\", 2)\n",
    "]\n",
    "\n",
    "projects_df = spark.createDataFrame(projects_data, \n",
    "    [\"project_id\", \"project_name\", \"department\", \"team_size\"])\n",
    "\n",
    "print(\"ğŸ“Š Sample DataFrames:\")\n",
    "print(\"\\nEmployees:\")\n",
    "employees_df.show()\n",
    "\n",
    "print(\"Departments:\")\n",
    "departments_df.show()\n",
    "\n",
    "print(\"Projects:\")\n",
    "projects_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Inner Join\n",
    "\n",
    "**Inner Join returns only rows that have matching values in both DataFrames.**\n",
    "\n",
    "### Characteristics:\n",
    "- âœ… **Only matching rows** from both sides\n",
    "- âœ… **Most common join type**\n",
    "- âœ… **Reduces data size** - only intersections\n",
    "- âœ… **Symmetric operation**\n",
    "\n",
    "### Use Cases:\n",
    "- Finding related data across tables\n",
    "- Data validation and integrity checks\n",
    "- Creating combined views for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner join examples\n",
    "print(\"ğŸ¯ INNER JOIN EXAMPLES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Join employees with departments\n",
    "emp_dept_inner = employees_df.join(\n",
    "    departments_df,\n",
    "    employees_df[\"department\"] == departments_df[\"dept_name\"],\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "print(\"Inner join - Employees with departments:\")\n",
    "emp_dept_inner.select(\n",
    "    \"name\", \"department\", \"salary\", \"description\", \"location\"\n",
    ").show()\n",
    "\n",
    "print(f\"\\nOriginal employees: {employees_df.count()} rows\")\n",
    "print(f\"Original departments: {departments_df.count()} rows\")\n",
    "print(f\"Inner join result: {emp_dept_inner.count()} rows\")\n",
    "\n",
    "# Join employees with projects (both have department column)\n",
    "emp_projects_inner = employees_df.join(\n",
    "    projects_df,\n",
    "    \"department\",  # Join on common column name\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "print(\"\\nInner join - Employees with projects:\")\n",
    "emp_projects_inner.select(\n",
    "    \"name\", \"department\", \"project_name\", \"team_size\"\n",
    ").show()\n",
    "\n",
    "print(f\"\\nProjects inner join result: {emp_projects_inner.count()} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inner Join Analysis\n",
    "\n",
    "Notice how:\n",
    "- **Frank** (Finance) is excluded because Finance department doesn't exist in departments_df\n",
    "- **Marketing & Finance** departments are excluded because they have no employees\n",
    "- Only **matching pairs** are returned\n",
    "\n",
    "**Inner joins are great for finding complete, related data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## â¬…ï¸ Left Join (Left Outer Join)\n",
    "\n",
    "**Left Join returns all rows from the left DataFrame and matching rows from the right DataFrame.**\n",
    "\n",
    "### Characteristics:\n",
    "- âœ… **All rows from left DataFrame**\n",
    "- âœ… **Matching rows from right DataFrame**\n",
    "- âœ… **NULL values** for non-matching right rows\n",
    "- âœ… **Preserves left data integrity**\n",
    "\n",
    "### Use Cases:\n",
    "- Preserving all customer/order data\n",
    "- Finding missing relationships\n",
    "- Left-side dominant analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left join examples\n",
    "print(\"â¬…ï¸ LEFT JOIN EXAMPLES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Left join employees with departments\n",
    "emp_dept_left = employees_df.join(\n",
    "    departments_df,\n",
    "    employees_df[\"department\"] == departments_df[\"dept_name\"],\n",
    "    \"left\"\n",
    ")\n",
    "\n",
    "print(\"Left join - All employees, matching departments:\")\n",
    "emp_dept_left.select(\n",
    "    \"name\", \"department\", \"salary\", \"description\", \"location\"\n",
    ").show()\n",
    "\n",
    "print(f\"\\nOriginal employees: {employees_df.count()} rows\")\n",
    "print(f\"Left join result: {emp_dept_left.count()} rows\")\n",
    "print(\"(Same as original - all employees included)\")\n",
    "\n",
    "# Find employees without department info\n",
    "orphaned_employees = emp_dept_left.filter(\n",
    "    col(\"dept_name\").isNull()\n",
    ")\n",
    "\n",
    "print(\"\\nEmployees without department info:\")\n",
    "orphaned_employees.select(\"name\", \"department\", \"dept_name\").show()\n",
    "\n",
    "# Left join projects with departments\n",
    "proj_dept_left = projects_df.join(\n",
    "    departments_df,\n",
    "    projects_df[\"department\"] == departments_df[\"dept_name\"],\n",
    "    \"left\"\n",
    ")\n",
    "\n",
    "print(\"\\nLeft join - All projects, matching departments:\")\n",
    "proj_dept_left.select(\n",
    "    \"project_name\", \"department\", \"description\", \"location\"\n",
    ").show()\n",
    "\n",
    "print(\"\\nProjects without department info:\")\n",
    "proj_dept_left.filter(col(\"dept_name\").isNull()).select(\n",
    "    \"project_name\", \"department\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Left Join Analysis\n",
    "\n",
    "Key observations:\n",
    "- **All employees included** (6 rows, same as original)\n",
    "- **Frank** (Finance) has NULL values for department info\n",
    "- **Marketing & Finance** departments are excluded (no employees)\n",
    "- **Preserves left DataFrame integrity**\n",
    "\n",
    "**Left joins are perfect for 'what's missing' analysis.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## â¡ï¸ Right Join (Right Outer Join)\n",
    "\n",
    "**Right Join returns all rows from the right DataFrame and matching rows from the left DataFrame.**\n",
    "\n",
    "### Characteristics:\n",
    "- âœ… **All rows from right DataFrame**\n",
    "- âœ… **Matching rows from left DataFrame**\n",
    "- âœ… **NULL values** for non-matching left rows\n",
    "- âœ… **Symmetric to left join** (just swap DataFrames)\n",
    "\n",
    "### Use Cases:\n",
    "- Preserving all reference/lookup data\n",
    "- Finding unused resources\n",
    "- Right-side dominant analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Right join examples\n",
    "print(\"â¡ï¸ RIGHT JOIN EXAMPLES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Right join employees with departments\n",
    "emp_dept_right = employees_df.join(\n",
    "    departments_df,\n",
    "    employees_df[\"department\"] == departments_df[\"dept_name\"],\n",
    "    \"right\"\n",
    ")\n",
    "\n",
    "print(\"Right join - All departments, matching employees:\")\n",
    "emp_dept_right.select(\n",
    "    \"name\", \"department\", \"dept_name\", \"description\", \"location\"\n",
    ").orderBy(\"dept_name\").show()\n",
    "\n",
    "print(f\"\\nOriginal departments: {departments_df.count()} rows\")\n",
    "print(f\"Right join result: {emp_dept_right.count()} rows\")\n",
    "\n",
    "# Find departments without employees\n",
    "empty_departments = emp_dept_right.filter(\n",
    "    col(\"name\").isNull()\n",
    ")\n",
    "\n",
    "print(\"\\nDepartments without employees:\")\n",
    "empty_departments.select(\"dept_name\", \"description\").show()\n",
    "\n",
    "# Right join projects with employees\n",
    "proj_emp_right = projects_df.join(\n",
    "    employees_df,\n",
    "    projects_df[\"department\"] == employees_df[\"department\"],\n",
    "    \"right\"\n",
    ")\n",
    "\n",
    "print(\"\\nRight join - All employees, matching projects:\")\n",
    "proj_emp_right.select(\n",
    "    \"name\", \"department\", \"project_name\", \"team_size\"\n",
    ").show()\n",
    "\n",
    "print(\"\\nEmployees without projects:\")\n",
    "proj_emp_right.filter(col(\"project_name\").isNull()).select(\n",
    "    \"name\", \"department\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Right Join Analysis\n",
    "\n",
    "Key observations:\n",
    "- **All departments included** (5 rows, same as original)\n",
    "- **Marketing & Finance** departments have NULL employee values\n",
    "- **All employees with projects** are shown\n",
    "- **Preserves right DataFrame integrity**\n",
    "\n",
    "**Right joins help find 'unused capacity' or 'missing assignments'.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”„ Full Outer Join\n",
    "\n",
    "**Full Outer Join returns all rows from both DataFrames, with NULLs where there's no match.**\n",
    "\n",
    "### Characteristics:\n",
    "- âœ… **All rows from both DataFrames**\n",
    "- âœ… **NULL values** for non-matching rows\n",
    "- âœ… **Complete picture** of relationships\n",
    "- âœ… **Largest result set** of all join types\n",
    "\n",
    "### Use Cases:\n",
    "- Complete data reconciliation\n",
    "- Finding all relationships and gaps\n",
    "- Data quality assessment\n",
    "- Comprehensive reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full outer join examples\n",
    "print(\"ğŸ”„ FULL OUTER JOIN EXAMPLES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Full outer join employees with departments\n",
    "emp_dept_full = employees_df.join(\n",
    "    departments_df,\n",
    "    employees_df[\"department\"] == departments_df[\"dept_name\"],\n",
    "    \"full\"\n",
    ")\n",
    "\n",
    "print(\"Full outer join - All employees and all departments:\")\n",
    "emp_dept_full.select(\n",
    "    \"name\", \"department\", \"dept_name\", \"description\", \"location\"\n",
    ").orderBy(\n",
    "    coalesce(col(\"department\"), col(\"dept_name\")).asc()\n",
    ").show()\n",
    "\n",
    "print(f\"\\nOriginal employees: {employees_df.count()} rows\")\n",
    "print(f\"Original departments: {departments_df.count()} rows\")\n",
    "print(f\"Full outer join result: {emp_dept_full.count()} rows\")\n",
    "print(f\"Expected: {employees_df.count() + departments_df.count()} rows\")\n",
    "\n",
    "# Analyze join results\n",
    "matched = emp_dept_full.filter(col(\"name\").isNotNull() & col(\"dept_name\").isNotNull())\n",
    "only_employees = emp_dept_full.filter(col(\"name\").isNotNull() & col(\"dept_name\").isNull())\n",
    "only_departments = emp_dept_full.filter(col(\"name\").isNull() & col(\"dept_name\").isNotNull())\n",
    "\n",
    "print(f\"\\nJoin analysis:\")\n",
    "print(f\"- Matched records: {matched.count()}\")\n",
    "print(f\"- Only in employees: {only_employees.count()}\")\n",
    "print(f\"- Only in departments: {only_departments.count()}\")\n",
    "\n",
    "# Full outer join projects with departments\n",
    "proj_dept_full = projects_df.join(\n",
    "    departments_df,\n",
    "    projects_df[\"department\"] == departments_df[\"dept_name\"],\n",
    "    \"full\"\n",
    ")\n",
    "\n",
    "print(\"\\nFull outer join - All projects and all departments:\")\n",
    "proj_dept_full.select(\n",
    "    \"project_name\", \"department\", \"dept_name\", \"description\"\n",
    ").orderBy(\n",
    "    coalesce(col(\"department\"), col(\"dept_name\")).asc()\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Outer Join Analysis\n",
    "\n",
    "Key observations:\n",
    "- **All data preserved** (6 employees + 5 departments - 3 matches = 8 rows)\n",
    "- **Frank** (Finance) appears with NULL department info\n",
    "- **Marketing & Finance** departments appear with NULL employee info\n",
    "- **Complete reconciliation** of both datasets\n",
    "\n",
    "**Full outer joins give you the complete picture of data relationships.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”— Multiple Table Joins\n",
    "\n",
    "### Joining More Than Two DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple table joins\n",
    "print(\"ğŸ”— MULTIPLE TABLE JOINS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Method 1: Sequential joins\n",
    "emp_dept = employees_df.join(\n",
    "    departments_df,\n",
    "    employees_df[\"department\"] == departments_df[\"dept_name\"],\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "emp_dept_proj = emp_dept.join(\n",
    "    projects_df,\n",
    "    emp_dept[\"department\"] == projects_df[\"department\"],\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "print(\"Three-way join (Employees â†’ Departments â†’ Projects):\")\n",
    "emp_dept_proj.select(\n",
    "    \"name\", \"department\", \"project_name\", \"team_size\", \"location\"\n",
    ").show()\n",
    "\n",
    "# Method 2: Chain joins with different conditions\n",
    "comprehensive_join = employees_df.join(\n",
    "    departments_df,\n",
    "    employees_df[\"department\"] == departments_df[\"dept_name\"],\n",
    "    \"left\"\n",
    ").join(\n",
    "    projects_df,\n",
    "    employees_df[\"department\"] == projects_df[\"department\"],\n",
    "    \"left\"\n",
    ")\n",
    "\n",
    "print(\"\\nComprehensive left join (all employees, matching dept/project):\")\n",
    "comprehensive_join.select(\n",
    "    \"name\", \"department\", \"project_name\", \"description\", \"location\"\n",
    ").orderBy(\"name\").show()\n",
    "\n",
    "# Method 3: Using SQL-style joins\n",
    "employees_df.createOrReplaceTempView(\"employees\")\n",
    "departments_df.createOrReplaceTempView(\"departments\")\n",
    "projects_df.createOrReplaceTempView(\"projects\")\n",
    "\n",
    "sql_join = spark.sql(\"\"\"\n",
    "    SELECT e.name, e.department, p.project_name, d.location,\n",
    "           e.salary, p.team_size\n",
    "    FROM employees e\n",
    "    INNER JOIN departments d ON e.department = d.dept_name\n",
    "    LEFT JOIN projects p ON e.department = p.department\n",
    "    ORDER BY e.salary DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nSQL-style multi-table join:\")\n",
    "sql_join.show()\n",
    "\n",
    "# Performance comparison\n",
    "print(f\"\\nResult counts:\")\n",
    "print(f\"- Sequential joins: {emp_dept_proj.count()} rows\")\n",
    "print(f\"- Comprehensive join: {comprehensive_join.count()} rows\")\n",
    "print(f\"- SQL join: {sql_join.count()} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ›ï¸ Advanced Join Conditions\n",
    "\n",
    "### Non-Equality and Complex Join Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced join conditions\n",
    "print(\"ğŸ›ï¸ ADVANCED JOIN CONDITIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create salary ranges for demonstration\n",
    "salary_ranges = [\n",
    "    (\"Entry\", 0, 60000),\n",
    "    (\"Mid\", 60000, 80000),\n",
    "    (\"Senior\", 80000, 100000),\n",
    "    (\"Executive\", 100000, 999999)\n",
    "]\n",
    "\n",
    "ranges_df = spark.createDataFrame(salary_ranges, \n",
    "    [\"level\", \"min_salary\", \"max_salary\"])\n",
    "\n",
    "print(\"Salary ranges:\")\n",
    "ranges_df.show()\n",
    "\n",
    "# Non-equality join (salary within range)\n",
    "salary_join = employees_df.join(\n",
    "    ranges_df,\n",
    "    (employees_df[\"salary\"] >= ranges_df[\"min_salary\"]) & \n",
    "    (employees_df[\"salary\"] < ranges_df[\"max_salary\"]),\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "print(\"\\nNon-equality join (salary ranges):\")\n",
    "salary_join.select(\n",
    "    \"name\", \"salary\", \"level\", \"min_salary\", \"max_salary\"\n",
    ").orderBy(\"salary\").show()\n",
    "\n",
    "# Self-join example (comparing salaries)\n",
    "self_join = employees_df.alias(\"e1\").join(\n",
    "    employees_df.alias(\"e2\"),\n",
    "    (col(\"e1.department\") == col(\"e2.department\")) & \n",
    "    (col(\"e1.emp_id\") != col(\"e2.emp_id\")) &\n",
    "    (col(\"e1.salary\") < col(\"e2.salary\"))\n",
    ")\n",
    "\n",
    "print(\"\\nSelf-join (lower earners vs higher earners in same dept):\")\n",
    "self_join.select(\n",
    "    col(\"e1.name\").alias(\"lower_earner\"),\n",
    "    col(\"e1.salary\").alias(\"lower_salary\"),\n",
    "    col(\"e2.name\").alias(\"higher_earner\"),\n",
    "    col(\"e2.salary\").alias(\"higher_salary\"),\n",
    "    col(\"e1.department\")\n",
    ").orderBy(\"department\", \"lower_salary\").show()\n",
    "\n",
    "# Complex multi-condition join\n",
    "complex_join = employees_df.join(\n",
    "    projects_df,\n",
    "    (employees_df[\"department\"] == projects_df[\"department\"]) & \n",
    "    (employees_df[\"salary\"] > projects_df[\"team_size\"] * 10000),  # Salary > team_size * 10k\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "print(\"\\nComplex condition join:\")\n",
    "complex_join.select(\n",
    "    \"name\", \"department\", \"salary\", \"project_name\", \"team_size\",\n",
    "    (col(\"team_size\") * 10000).alias(\"threshold\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš¡ Join Performance Optimization\n",
    "\n",
    "### Join Strategies and Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance optimization\n",
    "print(\"âš¡ JOIN PERFORMANCE OPTIMIZATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create larger datasets for performance testing\n",
    "large_employees = [\n",
    "    (i, f\"Employee_{i}\", f\"Dept_{(i%5)+1}\", 50000 + (i % 50000)) \n",
    "    for i in range(10000)\n",
    "]\n",
    "\n",
    "large_departments = [\n",
    "    (f\"Dept_{i}\", f\"Department {i}\", f\"City_{i%10}\") \n",
    "    for i in range(1, 6)\n",
    "]\n",
    "\n",
    "large_emp_df = spark.createDataFrame(large_employees, \n",
    "    [\"emp_id\", \"name\", \"department\", \"salary\"])\n",
    "large_dept_df = spark.createDataFrame(large_departments, \n",
    "    [\"dept_name\", \"description\", \"location\"])\n",
    "\n",
    "print(f\"Large employee dataset: {large_emp_df.count():,} rows\")\n",
    "print(f\"Department dataset: {large_dept_df.count()} rows\")\n",
    "\n",
    "# Test different join configurations\n",
    "import time\n",
    "\n",
    "# Configuration 1: Default join\n",
    "start_time = time.time()\n",
    "default_join = large_emp_df.join(large_dept_df, \n",
    "    large_emp_df[\"department\"] == large_dept_df[\"dept_name\"], \"inner\")\n",
    "default_count = default_join.count()\n",
    "default_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nDefault join: {default_count:,} rows in {default_time:.3f} seconds\")\n",
    "\n",
    "# Configuration 2: Broadcast join (force small table broadcast)\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"1MB\")  # Very small threshold\n",
    "\n",
    "start_time = time.time()\n",
    "broadcast_join = large_emp_df.join(\n",
    "    F.broadcast(large_dept_df), \n",
    "    large_emp_df[\"department\"] == large_dept_df[\"dept_name\"], \n",
    "    \"inner\"\n",
    ")\n",
    "broadcast_count = broadcast_join.count()\n",
    "broadcast_time = time.time() - start_time\n",
    "\n",
    "print(f\"Broadcast join: {broadcast_count:,} rows in {broadcast_time:.3f} seconds\")\n",
    "\n",
    "# Reset configuration\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"10MB\")\n",
    "\n",
    "print(f\"\\nPerformance comparison:\")\n",
    "print(f\"- Default: {default_time:.3f}s\")\n",
    "print(f\"- Broadcast: {broadcast_time:.3f}s\")\n",
    "if default_time > 0:\n",
    "    improvement = default_time / broadcast_time\n",
    "    print(f\"- Broadcast is {improvement:.1f}x faster\")\n",
    "\n",
    "# Best practices summary\n",
    "print(\"\\nğŸš€ JOIN BEST PRACTICES:\")\n",
    "print(\"1. Use broadcast joins for small lookup tables\")\n",
    "print(\"2. Filter data before joining\")\n",
    "print(\"3. Choose appropriate join types\")\n",
    "print(\"4. Partition data on join keys when possible\")\n",
    "print(\"5. Cache frequently joined tables\")\n",
    "print(\"6. Monitor shuffle operations in Spark UI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš¨ Common Join Mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common mistakes\n",
    "print(\"ğŸš¨ COMMON JOIN MISTAKES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Mistake 1: Cartesian explosion\n",
    "print(\"âŒ Mistake: Missing join condition (Cartesian product)\")\n",
    "try:\n",
    "    bad_join = employees_df.join(departments_df)  # No join condition!\n",
    "    print(f\"Cartesian join result: {bad_join.count()} rows\")\n",
    "    print(\"(This creates every possible combination!)\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)[:100]}...\")\n",
    "\n",
    "print(\"\\nâœ… Solution: Always specify join condition\")\n",
    "good_join = employees_df.join(departments_df, \n",
    "    employees_df[\"department\"] == departments_df[\"dept_name\"])\n",
    "print(f\"Proper join result: {good_join.count()} rows\")\n",
    "\n",
    "# Mistake 2: Column name ambiguity\n",
    "print(\"\\nâŒ Mistake: Ambiguous column names\")\n",
    "ambiguous_join = employees_df.join(\n",
    "    projects_df, \n",
    "    employees_df[\"department\"] == projects_df[\"department\"]\n",
    ")\n",
    "print(\"Ambiguous columns after join:\")\n",
    "print(f\"Columns: {[col for col in ambiguous_join.columns if 'department' in col]}\")\n",
    "\n",
    "print(\"\\nâœ… Solution: Use aliases or select specific columns\")\n",
    "clear_join = employees_df.alias(\"emp\").join(\n",
    "    projects_df.alias(\"proj\"), \n",
    "    col(\"emp.department\") == col(\"proj.department\")\n",
    ").select(\n",
    "    col(\"emp.name\"),\n",
    "    col(\"emp.department\").alias(\"emp_dept\"),\n",
    "    col(\"proj.project_name\"),\n",
    "    col(\"proj.department\").alias(\"proj_dept\")\n",
    ")\n",
    "print(\"Clear column names:\")\n",
    "clear_join.show(3)\n",
    "\n",
    "# Mistake 3: Incorrect join type\n",
    "print(\"\\nâŒ Mistake: Using inner join when left join is needed\")\n",
    "wrong_join = employees_df.join(departments_df, \n",
    "    employees_df[\"department\"] == departments_df[\"dept_name\"], \"inner\")\n",
    "print(f\"Inner join: {wrong_join.count()} rows (missing Frank)\")\n",
    "\n",
    "print(\"\\nâœ… Solution: Use left join to preserve all employees\")\n",
    "correct_join = employees_df.join(departments_df, \n",
    "    employees_df[\"department\"] == departments_df[\"dept_name\"], \"left\")\n",
    "print(f\"Left join: {correct_join.count()} rows (all employees included)\")\n",
    "\n",
    "# Mistake 4: Data skew causing performance issues\n",
    "print(\"\\nâŒ Mistake: Ignoring data skew in joins\")\n",
    "print(\"Large partitions can cause slow, uneven task execution\")\n",
    "\n",
    "print(\"\\nâœ… Solutions for data skew:\")\n",
    "print(\"- Use salting: Add random suffix to join keys\")\n",
    "print(\"- Repartition data before joining\")\n",
    "print(\"- Use broadcast joins for small tables\")\n",
    "print(\"- Filter data to reduce skew\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Key Takeaways\n",
    "\n",
    "### What You Learned:\n",
    "- âœ… **`inner` join** - Only matching rows\n",
    "- âœ… **`left` join** - All left rows + matching right rows\n",
    "- âœ… **`right` join** - All right rows + matching left rows\n",
    "- âœ… **`full` join** - All rows from both sides\n",
    "- âœ… **Multiple table joins** - Joining 3+ DataFrames\n",
    "- âœ… **Advanced join conditions** - Non-equality joins\n",
    "- âœ… **Performance optimization** - Broadcast joins, filtering\n",
    "\n",
    "### Join Performance Hierarchy:\n",
    "- ğŸ”¸ **Broadcast Join**: Small table + large table (fastest)\n",
    "- ğŸ”¸ **Shuffle Hash Join**: Medium tables with good key distribution\n",
    "- ğŸ”¸ **Sort-Merge Join**: Large sorted tables\n",
    "- ğŸ”¸ **Nested Loop Join**: Fallback, usually slowest\n",
    "\n",
    "### Best Practices:\n",
    "- ğŸ”¸ **Choose join type** based on data preservation needs\n",
    "- ğŸ”¸ **Filter before joining** to reduce data volume\n",
    "- ğŸ”¸ **Broadcast small tables** explicitly\n",
    "- ğŸ”¸ **Use aliases** to avoid column name conflicts\n",
    "- ğŸ”¸ **Always specify join conditions** to avoid Cartesian products\n",
    "- ğŸ”¸ **Monitor join performance** in Spark UI\n",
    "\n",
    "### Common Patterns:\n",
    "- ğŸ”¸ **Lookup joins**: `large_df.join(broadcast(small_df), \"key\")`\n",
    "- ğŸ”¸ **Preservation joins**: `main_df.join(ref_df, \"key\", \"left\")`\n",
    "- ğŸ”¸ **Reconciliation joins**: `df1.join(df2, \"key\", \"full\")`\n",
    "- ğŸ”¸ **Multi-table**: `df1.join(df2, \"key\").join(df3, \"key\")`\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ Next Steps\n",
    "\n",
    "Now that you master basic joins, you're ready for:\n",
    "\n",
    "1. **Broadcast Join Optimization** - Advanced join performance\n",
    "2. **Skew Join Handling** - Dealing with data skew\n",
    "3. **Spark SQL Joins** - SQL syntax for joins\n",
    "4. **Advanced Analytics** - Complex multi-table queries\n",
    "\n",
    "**Joins are the foundation of relational data processing!**\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ‰ Congratulations! You now wield the power of DataFrame joins like a database expert!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
