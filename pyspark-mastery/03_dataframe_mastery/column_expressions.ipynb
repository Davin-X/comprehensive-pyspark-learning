{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Column Expressions: Advanced DataFrame Operations\n",
    "\n",
    "**Time to complete:** 35 minutes  \n",
    "**Difficulty:** Intermediate  \n",
    "**Prerequisites:** DataFrame basics, SQL knowledge\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will master:\n",
    "- ‚úÖ **Column operations** - select, withColumn, drop\n",
    "- ‚úÖ **Column expressions** - arithmetic, string, date operations\n",
    "- ‚úÖ **Conditional logic** - when/otherwise, case statements\n",
    "- ‚úÖ **Type casting** - cast, data type conversions\n",
    "- ‚úÖ **User Defined Functions (UDFs)** - custom column logic\n",
    "- ‚úÖ **Performance optimization** - efficient column operations\n",
    "\n",
    "**Column expressions are the heart of DataFrame transformations!**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Understanding Column Expressions\n",
    "\n",
    "**Column expressions** are the building blocks of DataFrame operations. Every transformation you perform on columns creates expressions that Spark optimizes and executes.\n",
    "\n",
    "### Expression Types:\n",
    "- **Arithmetic expressions**: `col(\"price\") * 0.8`\n",
    "- **String expressions**: `upper(col(\"name\"))`\n",
    "- **Conditional expressions**: `when(col(\"age\") > 18, \"Adult\").otherwise(\"Minor\")`\n",
    "- **Type cast expressions**: `col(\"price\").cast(\"double\")`\n",
    "- **UDF expressions**: Custom functions applied to columns\n",
    "\n",
    "**All expressions are lazily evaluated and optimized by Catalyst!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, lit, expr, udf\n",
    "from pyspark.sql.types import StringType, DoubleType, IntegerType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Column_Expressions\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"‚úÖ Spark ready - Version: {spark.version}\")\n",
    "\n",
    "# Create sample data\n",
    "data = [\n",
    "    (1, \"Alice\", 25, \"Engineering\", 75000.0, \"2023-01-15\"),\n",
    "    (2, \"Bob\", 30, \"Sales\", 65000.0, \"2023-02-20\"),\n",
    "    (3, \"Charlie\", 35, \"Engineering\", 85000.0, \"2023-03-10\"),\n",
    "    (4, \"Diana\", 28, \"HR\", 55000.0, \"2023-04-05\"),\n",
    "    (5, \"Eve\", 32, \"Sales\", 70000.0, \"2023-05-12\")\n",
    "]\n",
    "\n",
    "columns = [\"id\", \"name\", \"age\", \"department\", \"salary\", \"hire_date\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "print(\"üìä Sample DataFrame:\")\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Basic Column Operations\n",
    "\n",
    "### Selecting Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different ways to select columns\n",
    "print(\"üéØ COLUMN SELECTION METHODS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Method 1: Column names as strings\n",
    "basic_select = df.select(\"name\", \"department\", \"salary\")\n",
    "print(\"Basic column selection:\")\n",
    "basic_select.show()\n",
    "\n",
    "# Method 2: Using col() function\n",
    "col_select = df.select(col(\"name\"), col(\"age\"), col(\"salary\"))\n",
    "print(\"\\nUsing col() function:\")\n",
    "col_select.show()\n",
    "\n",
    "# Method 3: Column expressions\n",
    "expr_select = df.select(\n",
    "    \"name\",\n",
    "    (col(\"salary\") * 1.1).alias(\"salary_with_bonus\"),\n",
    "    (col(\"age\") + 1).alias(\"age_next_year\")\n",
    ")\n",
    "print(\"\\nWith column expressions:\")\n",
    "expr_select.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding and Modifying Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding new columns with expressions\n",
    "print(\"‚ûï ADDING NEW COLUMNS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# withColumn() - add or replace columns\n",
    "df_enhanced = df \\\n",
    "    .withColumn(\"annual_bonus\", col(\"salary\") * 0.1) \\\n",
    "    .withColumn(\"total_compensation\", col(\"salary\") + col(\"annual_bonus\")) \\\n",
    "    .withColumn(\"experience_level\", \n",
    "                when(col(\"age\") < 30, \"Junior\")\n",
    "                .when(col(\"age\") < 35, \"Mid-level\")\n",
    "                .otherwise(\"Senior\"))\n",
    "\n",
    "print(\"Enhanced DataFrame:\")\n",
    "df_enhanced.select(\"name\", \"age\", \"salary\", \"annual_bonus\", \"total_compensation\", \"experience_level\").show()\n",
    "\n",
    "# withColumnRenamed() - rename columns\n",
    "df_renamed = df.withColumnRenamed(\"salary\", \"annual_salary\")\n",
    "print(\"\\nRenamed column:\")\n",
    "df_renamed.select(\"name\", \"annual_salary\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing columns\n",
    "print(\"üóëÔ∏è REMOVING COLUMNS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# drop() method\n",
    "df_reduced = df.drop(\"hire_date\", \"id\")\n",
    "print(\"After dropping columns:\")\n",
    "df_reduced.show()\n",
    "\n",
    "# Drop multiple columns at once\n",
    "df_minimal = df.drop(*[\"id\", \"hire_date\", \"department\"])\n",
    "print(\"\\nMinimal DataFrame:\")\n",
    "df_minimal.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßÆ Arithmetic Expressions\n",
    "\n",
    "### Basic Arithmetic Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arithmetic expressions\n",
    "print(\"üßÆ ARITHMETIC EXPRESSIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create sample sales data\n",
    "sales_data = [\n",
    "    (\"Product_A\", 100, 25.50, 0.15),\n",
    "    (\"Product_B\", 200, 15.75, 0.20),\n",
    "    (\"Product_C\", 150, 30.00, 0.10),\n",
    "    (\"Product_D\", 300, 12.50, 0.25)\n",
    "]\n",
    "\n",
    "sales_df = spark.createDataFrame(sales_data, [\"product\", \"quantity\", \"price\", \"discount_rate\"])\n",
    "\n",
    "# Calculate total revenue with expressions\n",
    "revenue_df = sales_df.withColumn(\n",
    "    \"total_before_discount\", col(\"quantity\") * col(\"price\")\n",
    ").withColumn(\n",
    "    \"discount_amount\", col(\"total_before_discount\") * col(\"discount_rate\")\n",
    ").withColumn(\n",
    "    \"final_revenue\", col(\"total_before_discount\") - col(\"discount_amount\")\n",
    ").withColumn(\n",
    "    \"avg_price_per_unit\", col(\"final_revenue\") / col(\"quantity\")\n",
    ")\n",
    "\n",
    "print(\"Revenue calculations:\")\n",
    "revenue_df.select(\n",
    "    \"product\", \"quantity\", \"price\", \"discount_rate\",\n",
    "    \"total_before_discount\", \"discount_amount\", \"final_revenue\", \"avg_price_per_unit\"\n",
    ").show()\n",
    "\n",
    "# Round results for cleaner display\n",
    "rounded_df = revenue_df.withColumn(\"avg_price_per_unit\", F.round(col(\"avg_price_per_unit\"), 2))\n",
    "print(\"\\nRounded results:\")\n",
    "rounded_df.select(\"product\", \"avg_price_per_unit\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mathematical functions\n",
    "print(\"üî¢ MATHEMATICAL FUNCTIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Sample numerical data\n",
    "nums_df = spark.createDataFrame(\n",
    "    [(1.5,), (2.7,), (-3.2,), (4.8,), (0.0,)], \n",
    "    [\"value\"]\n",
    ")\n",
    "\n",
    "# Apply mathematical functions\n",
    "math_df = nums_df \\\n",
    "    .withColumn(\"abs_value\", F.abs(col(\"value\"))) \\\n",
    "    .withColumn(\"ceil_value\", F.ceil(col(\"value\"))) \\\n",
    "    .withColumn(\"floor_value\", F.floor(col(\"value\"))) \\\n",
    "    .withColumn(\"round_value\", F.round(col(\"value\"))) \\\n",
    "    .withColumn(\"sqrt_value\", F.sqrt(F.abs(col(\"value\")))) \\\n",
    "    .withColumn(\"exp_value\", F.exp(col(\"value\"))) \\\n",
    "    .withColumn(\"log_value\", F.log(F.abs(col(\"value\")) + 1))  # log(0) is undefined\n",
    "\n",
    "print(\"Mathematical transformations:\")\n",
    "math_df.show()\n",
    "\n",
    "# Statistical functions on our employee data\n",
    "stats_df = df.withColumn(\n",
    "    \"salary_zscore\", (col(\"salary\") - F.avg(col(\"salary\")).over()) / F.stddev(col(\"salary\")).over()\n",
    ").withColumn(\n",
    "    \"salary_percentile\", F.percent_rank().over(F.orderBy(\"salary\"))\n",
    ")\n",
    "\n",
    "print(\"\\nStatistical calculations:\")\n",
    "stats_df.select(\"name\", \"salary\", \"salary_zscore\", \"salary_percentile\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù String Expressions\n",
    "\n",
    "### String Manipulation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String expressions\n",
    "print(\"üìù STRING EXPRESSIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Sample string data\n",
    "text_data = [\n",
    "    (\"john doe\", \"engineer\"),\n",
    "    (\"JANE SMITH\", \"manager\"),\n",
    "    (\"bob johnson\", \"analyst\"),\n",
    "    (\"Alice Cooper\", \"director\")\n",
    "]\n",
    "\n",
    "text_df = spark.createDataFrame(text_data, [\"name\", \"title\"])\n",
    "\n",
    "# String transformations\n",
    "string_df = text_df \\\n",
    "    .withColumn(\"name_upper\", F.upper(col(\"name\"))) \\\n",
    "    .withColumn(\"name_lower\", F.lower(col(\"name\"))) \\\n",
    "    .withColumn(\"name_initcap\", F.initcap(col(\"name\"))) \\\n",
    "    .withColumn(\"name_length\", F.length(col(\"name\"))) \\\n",
    "    .withColumn(\"first_name\", F.split(col(\"name\"), \" \")[0]) \\\n",
    "    .withColumn(\"last_name\", F.split(col(\"name\"), \" \")[1]) \\\n",
    "    .withColumn(\"full_title\", F.concat(col(\"first_name\"), F.lit(\" \"), col(\"last_name\"), F.lit(\" - \"), col(\"title\")))\n",
    "\n",
    "print(\"String transformations:\")\n",
    "string_df.select(\"name\", \"name_upper\", \"name_initcap\", \"first_name\", \"last_name\", \"full_title\").show(truncate=False)\n",
    "\n",
    "# String search and replace\n",
    "search_df = text_df \\\n",
    "    .withColumn(\"contains_john\", F.instr(col(\"name\"), \"john\")) \\\n",
    "    .withColumn(\"starts_with_j\", F.col(\"name\").startswith(\"j\")) \\\n",
    "    .withColumn(\"ends_with_n\", F.col(\"name\").endswith(\"n\")) \\\n",
    "    .withColumn(\"replaced\", F.regexp_replace(col(\"name\"), \"john\", \"JAKE\"))\n",
    "\n",
    "print(\"\\nString search and replace:\")\n",
    "search_df.select(\"name\", \"contains_john\", \"starts_with_j\", \"ends_with_n\", \"replaced\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéõÔ∏è Conditional Logic\n",
    "\n",
    "### When/Otherwise Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditional expressions\n",
    "print(\"üéõÔ∏è CONDITIONAL LOGIC\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Using when/otherwise for categorization\n",
    "conditional_df = df \\\n",
    "    .withColumn(\"salary_category\",\n",
    "                when(col(\"salary\") >= 80000, \"High\")\n",
    "                .when(col(\"salary\") >= 60000, \"Medium\")\n",
    "                .otherwise(\"Low\")) \\\n",
    "    .withColumn(\"age_group\",\n",
    "                when(col(\"age\") < 30, \"Young\")\n",
    "                .when(col(\"age\") < 35, \"Mid-age\")\n",
    "                .otherwise(\"Experienced\")) \\\n",
    "    .withColumn(\"performance_rating\",\n",
    "                when((col(\"age\") >= 30) & (col(\"salary\") >= 70000), \"Excellent\")\n",
    "                .when((col(\"age\") >= 25) | (col(\"salary\") >= 65000), \"Good\")\n",
    "                .otherwise(\"Needs Improvement\"))\n",
    "\n",
    "print(\"Conditional categorization:\")\n",
    "conditional_df.select(\"name\", \"age\", \"salary\", \"salary_category\", \"age_group\", \"performance_rating\").show()\n",
    "\n",
    "# Complex business logic\n",
    "business_df = df.withColumn(\n",
    "    \"promotion_eligible\",\n",
    "    when(\n",
    "        (col(\"age\") >= 30) & \n",
    "        (col(\"department\") == \"Engineering\") & \n",
    "        (col(\"salary\") >= 75000),\n",
    "        \"Eligible for Senior Role\"\n",
    "    ).when(\n",
    "        (col(\"age\") >= 28) & \n",
    "        (col(\"department\") == \"Sales\") & \n",
    "        (col(\"salary\") >= 65000),\n",
    "        \"Eligible for Manager Role\"\n",
    "    ).otherwise(\"Not Eligible\")\n",
    ")\n",
    "\n",
    "print(\"\\nBusiness logic example:\")\n",
    "business_df.select(\"name\", \"department\", \"age\", \"salary\", \"promotion_eligible\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Type Casting and Conversion\n",
    "\n",
    "### Data Type Conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type casting\n",
    "print(\"üîÑ TYPE CASTING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create mixed-type data\n",
    "mixed_data = [\n",
    "    (\"100\", \"25.5\", \"1\"),\n",
    "    (\"200\", \"30.7\", \"0\"),\n",
    "    (\"150\", \"28.2\", \"1\")\n",
    "]\n",
    "\n",
    "mixed_df = spark.createDataFrame(mixed_data, [\"str_num\", \"str_float\", \"str_bool\"])\n",
    "print(\"Original string data:\")\n",
    "mixed_df.show()\n",
    "mixed_df.printSchema()\n",
    "\n",
    "# Type casting\n",
    "casted_df = mixed_df \\\n",
    "    .withColumn(\"int_value\", col(\"str_num\").cast(IntegerType())) \\\n",
    "    .withColumn(\"double_value\", col(\"str_float\").cast(DoubleType())) \\\n",
    "    .withColumn(\"bool_value\", col(\"str_bool\").cast(\"boolean\")) \\\n",
    "    .withColumn(\"date_value\", F.to_date(F.lit(\"2023-01-01\"))) \\\n",
    "    .withColumn(\"timestamp_value\", F.to_timestamp(F.lit(\"2023-01-01 12:30:45\")))\n",
    "\n",
    "print(\"\\nAfter type casting:\")\n",
    "casted_df.show()\n",
    "casted_df.printSchema()\n",
    "\n",
    "# Safe casting with error handling\n",
    "safe_cast_df = mixed_df.withColumn(\n",
    "    \"safe_int\", \n",
    "    when(F.col(\"str_num\").rlike(\"^\\\\d+$\"), F.col(\"str_num\").cast(IntegerType()))\n",
    "    .otherwise(F.lit(None))\n",
    ")\n",
    "\n",
    "print(\"\\nSafe casting (handles invalid data):\")\n",
    "safe_cast_df.select(\"str_num\", \"safe_int\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è User Defined Functions (UDFs)\n",
    "\n",
    "### Creating and Using UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User Defined Functions\n",
    "print(\"üõ†Ô∏è USER DEFINED FUNCTIONS (UDFS)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Define Python functions\n",
    "def calculate_tax(salary):\n",
    "    \"\"\"Calculate tax based on salary brackets\"\"\"\n",
    "    if salary >= 80000:\n",
    "        return salary * 0.25\n",
    "    elif salary >= 60000:\n",
    "        return salary * 0.20\n",
    "    else:\n",
    "        return salary * 0.15\n",
    "\n",
    "def format_currency(amount):\n",
    "    \"\"\"Format number as currency string\"\"\"\n",
    "    return f\"${amount:,.2f}\"\n",
    "\n",
    "# Register UDFs\n",
    "tax_udf = udf(calculate_tax, DoubleType())\n",
    "currency_udf = udf(format_currency, StringType())\n",
    "\n",
    "# Apply UDFs\n",
    "udf_df = df \\\n",
    "    .withColumn(\"tax_amount\", tax_udf(col(\"salary\"))) \\\n",
    "    .withColumn(\"net_salary\", col(\"salary\") - col(\"tax_amount\")) \\\n",
    "    .withColumn(\"salary_formatted\", currency_udf(col(\"salary\"))) \\\n",
    "    .withColumn(\"tax_formatted\", currency_udf(col(\"tax_amount\"))) \\\n",
    "    .withColumn(\"net_formatted\", currency_udf(col(\"net_salary\")))\n",
    "\n",
    "print(\"UDF applications:\")\n",
    "udf_df.select(\n",
    "    \"name\", \"salary_formatted\", \"tax_formatted\", \"net_formatted\"\n",
    ").show()\n",
    "\n",
    "# UDF with conditional logic\n",
    "def performance_category(salary, age):\n",
    "    \"\"\"Categorize employee performance\"\"\"\n",
    "    score = (salary / 1000) + age\n",
    "    if score >= 110:\n",
    "        return \"Outstanding\"\n",
    "    elif score >= 90:\n",
    "        return \"Excellent\"\n",
    "    elif score >= 70:\n",
    "        return \"Good\"\n",
    "    else:\n",
    "        return \"Needs Improvement\"\n",
    "\n",
    "performance_udf = udf(performance_category, StringType())\n",
    "\n",
    "performance_df = df.withColumn(\n",
    "    \"performance\", \n",
    "    performance_udf(col(\"salary\"), col(\"age\"))\n",
    ")\n",
    "\n",
    "print(\"\\nPerformance categorization with UDF:\")\n",
    "performance_df.select(\"name\", \"age\", \"salary\", \"performance\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UDF Performance Considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UDF performance comparison\n",
    "print(\"‚ö° UDF PERFORMANCE CONSIDERATIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create larger dataset\n",
    "large_data = [(i, f\"name_{i}\", 25000 + (i * 100)) for i in range(1, 10001)]\n",
    "large_df = spark.createDataFrame(large_data, [\"id\", \"name\", \"salary\"])\n",
    "\n",
    "print(f\"Large dataset: {large_df.count():,} rows\")\n",
    "\n",
    "# Method 1: Built-in functions (fast)\n",
    "start_time = time.time()\n",
    "builtin_result = large_df.withColumn(\"tax\", col(\"salary\") * 0.2).count()\n",
    "builtin_time = time.time() - start_time\n",
    "\n",
    "# Method 2: UDF (slower)\n",
    "def calculate_tax_udf(salary):\n",
    "    return salary * 0.2\n",
    "\n",
    "tax_udf_func = udf(calculate_tax_udf, DoubleType())\n",
    "\n",
    "start_time = time.time()\n",
    "udf_result = large_df.withColumn(\"tax\", tax_udf_func(col(\"salary\"))).count()\n",
    "udf_time = time.time() - start_time\n",
    "\n",
    "print(f\"Built-in functions: {builtin_time:.3f} seconds\")\n",
    "print(f\"UDF approach: {udf_time:.3f} seconds\")\n",
    "if udf_time > 0:\n",
    "    print(f\"UDF is {udf_time/builtin_time:.1f}x slower\")\n",
    "\n",
    "print(\"\\nüí° Performance Tips:\")\n",
    "print(\"1. Use built-in functions when possible\")\n",
    "print(\"2. Avoid UDFs for simple arithmetic\")\n",
    "print(\"3. Consider pandas UDFs for complex operations\")\n",
    "print(\"4. Test UDF performance on realistic data sizes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Complex Expressions with expr()\n",
    "\n",
    "### Using SQL-like Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex expressions using expr()\n",
    "print(\"üéØ COMPLEX EXPRESSIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# expr() allows SQL-like syntax\n",
    "complex_df = df.withColumn(\n",
    "    \"complex_calc\",\n",
    "    expr(\"salary * 1.1 + (age * 100) - CASE WHEN department = 'Engineering' THEN 5000 ELSE 2000 END\")\n",
    ").withColumn(\n",
    "    \"sql_expression\",\n",
    "    expr(\"CASE \"\n",
    "         \"WHEN salary > 70000 THEN 'High Earner' \"\n",
    "         \"WHEN salary > 60000 THEN 'Mid Earner' \"\n",
    "         \"ELSE 'Entry Level' END\")\n",
    ")\n",
    "\n",
    "print(\"Complex expressions with expr():\")\n",
    "complex_df.select(\"name\", \"salary\", \"age\", \"department\", \"complex_calc\", \"sql_expression\").show()\n",
    "\n",
    "# Combining expr() with regular column operations\n",
    "hybrid_df = df.withColumn(\n",
    "    \"bonus_calculation\",\n",
    "    expr(\"salary * (CASE WHEN department = 'Sales' THEN 1.15 WHEN department = 'Engineering' THEN 1.10 ELSE 1.05 END)\")\n",
    ").withColumn(\n",
    "    \"final_package\",\n",
    "    col(\"bonus_calculation\") + col(\"age\") * 100  # Mix expr and col\n",
    ")\n",
    "\n",
    "print(\"\\nHybrid expressions:\")\n",
    "hybrid_df.select(\"name\", \"department\", \"salary\", \"bonus_calculation\", \"final_package\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üö® Common Mistakes and Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common mistakes and solutions\n",
    "print(\"üö® COMMON MISTAKES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Mistake 1: Column name typos\n",
    "try:\n",
    "    bad_column = df.select(\"nonexistent_column\")\n",
    "    bad_column.show()\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Column name error: {str(e)[:100]}...\")\n",
    "\n",
    "# Correct approach\n",
    "print(\"\\n‚úÖ Correct column names:\")\n",
    "df.select(\"name\", \"salary\").show()\n",
    "\n",
    "# Mistake 2: Type mismatch in operations\n",
    "try:\n",
    "    bad_math = df.withColumn(\"bad_result\", col(\"name\") + col(\"salary\"))\n",
    "    bad_math.show()\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Type mismatch error: {str(e)[:100]}...\")\n",
    "\n",
    "# Correct approach\n",
    "print(\"\\n‚úÖ Proper type handling:\")\n",
    "df.withColumn(\"salary_str\", F.concat(col(\"name\"), F.lit(\" earns \"), col(\"salary\").cast(\"string\"))).show()\n",
    "\n",
    "# Mistake 3: Forgetting to handle nulls\n",
    "null_data = [(1, \"Alice\", None), (2, \"Bob\", 30000)]\n",
    "null_df = spark.createDataFrame(null_data, [\"id\", \"name\", \"salary\"])\n",
    "\n",
    "print(\"\\n‚ùå Without null handling:\")\n",
    "try:\n",
    "    null_df.withColumn(\"bonus\", col(\"salary\") * 0.1).show()\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)[:100]}...\")\n",
    "\n",
    "print(\"\\n‚úÖ With null handling:\")\n",
    "null_df.withColumn(\"bonus\", F.coalesce(col(\"salary\"), F.lit(0)) * 0.1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Key Takeaways\n",
    "\n",
    "### What You Learned:\n",
    "- ‚úÖ **`select()`** - Choose specific columns\n",
    "- ‚úÖ **`withColumn()`** - Add/modify columns with expressions\n",
    "- ‚úÖ **Arithmetic expressions** - Mathematical operations on columns\n",
    "- ‚úÖ **String functions** - Text manipulation (upper, lower, concat, etc.)\n",
    "- ‚úÖ **Conditional logic** - `when/otherwise` for complex conditions\n",
    "- ‚úÖ **Type casting** - Convert between data types safely\n",
    "- ‚úÖ **UDFs** - Custom functions (but prefer built-ins when possible)\n",
    "- ‚úÖ **`expr()`** - SQL-like expressions in DataFrames\n",
    "\n",
    "### Performance Best Practices:\n",
    "- üî∏ **Prefer built-in functions** over UDFs for speed\n",
    "- üî∏ **Use `expr()`** for complex SQL-like logic\n",
    "- üî∏ **Chain operations efficiently** to minimize shuffles\n",
    "- üî∏ **Handle nulls explicitly** to avoid runtime errors\n",
    "- üî∏ **Check data types** before operations\n",
    "\n",
    "### Common Patterns:\n",
    "- üî∏ `col(\"column\")` - Reference columns in expressions\n",
    "- üî∏ `when(condition, value).otherwise(default)` - Conditional logic\n",
    "- üî∏ `F.function_name()` - Access PySpark SQL functions\n",
    "- üî∏ `expr(\"SQL expression\")` - SQL syntax in DataFrames\n",
    "- üî∏ `udf(function, return_type)` - Register custom functions\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Next Steps\n",
    "\n",
    "Now that you master column expressions, you're ready for:\n",
    "\n",
    "1. **DataFrame Aggregations** - GroupBy and statistical operations\n",
    "2. **Window Functions** - Advanced analytical operations\n",
    "3. **Joins** - Combining multiple DataFrames\n",
    "4. **Complex Data Types** - Arrays, maps, and structs\n",
    "\n",
    "**Column expressions are fundamental to all DataFrame operations!**\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Congratulations! You now wield the power of DataFrame column expressions like a Spark expert!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
