{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŒŠ Structured Streaming Basics: Real-Time Data Processing\n",
    "\n",
    "**Time to complete:** 40 minutes  \n",
    "**Difficulty:** Intermediate  \n",
    "**Prerequisites:** DataFrames, SQL, basic streaming concepts\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will master:\n",
    "- âœ… **Streaming fundamentals** - Unbounded vs bounded data\n",
    "- âœ… **Structured Streaming concepts** - DataFrames for streams\n",
    "- âœ… **Basic operations** - Reading, transforming, writing streams\n",
    "- âœ… **Output modes** - Append, Update, Complete modes\n",
    "- âœ… **Triggers** - Processing time vs event time\n",
    "- âœ… **Fault tolerance** - Checkpointing and recovery\n",
    "- âœ… **Monitoring** - Streaming query statistics\n",
    "\n",
    "**Structured Streaming brings DataFrame power to real-time processing!**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ” Understanding Streaming Fundamentals\n",
    "\n",
    "### Bounded vs Unbounded Data\n",
    "\n",
    "**Traditional batch processing** works with bounded data:\n",
    "```\n",
    "Batch Processing:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   Input Data    â”‚â”€â”€â”€â”€â–¶â”‚  Processing  â”‚â”€â”€â”€â”€â–¶â”‚  Output      â”‚\n",
    "â”‚   (Fixed Size)  â”‚     â”‚  (One Pass)  â”‚     â”‚  (Complete)  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â†‘                     â†‘                     â†‘\n",
    "     Static Data          Finite Time         Final Result\n",
    "```\n",
    "\n",
    "**Streaming processing** handles unbounded data:\n",
    "```\n",
    "Stream Processing:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   Input Stream  â”‚â”€â”€â”€â”€â–¶â”‚  Continuous  â”‚â”€â”€â”€â”€â–¶â”‚  Continuous  â”‚\n",
    "â”‚   (Never Ends)  â”‚     â”‚  Processing  â”‚     â”‚  Output      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â†‘                     â†‘                     â†‘\n",
    "   Continuous Data      Infinite Time      Evolving Results\n",
    "```\n",
    "\n",
    "**Key differences:**\n",
    "- **Bounded**: Fixed dataset, one-time processing\n",
    "- **Unbounded**: Continuous data, ongoing processing\n",
    "- **Streaming challenge**: Handle late data, state management, fault tolerance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured Streaming Architecture\n",
    "\n",
    "**Structured Streaming** treats streams as unbounded DataFrames:\n",
    "```\n",
    "Streaming DataFrame:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   Input Stream  â”‚â”€â”€â”€â”€â–¶â”‚   DataFrame     â”‚â”€â”€â”€â”€â–¶â”‚   Output Sink   â”‚\n",
    "â”‚   (Kafka/Socket)â”‚     â”‚   Operations    â”‚     â”‚   (Console/File)â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚                       â”‚                       â”‚\n",
    "     Source                Transformations           Sink\n",
    "```\n",
    "\n",
    "**Same DataFrame API** - streaming and batch use identical operations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/12/20 12:12:04 WARN Utils: Your hostname, Devs-MacBook-Air.local, resolves to a loopback address: 127.0.0.1; using 192.168.0.107 instead (on interface en0)\n",
      "25/12/20 12:12:04 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/20 12:12:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Spark ready - Version: 4.1.0\n",
      "Structured Streaming environment configured\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, window, count, sum as sum_func\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "import pyspark.sql.functions as F\n",
    "import time\n",
    "import threading\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Structured_Streaming_Basics\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"âœ… Spark ready - Version: {spark.version}\")\n",
    "\n",
    "# Enable adaptive query execution for streaming\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", \"true\")\n",
    "\n",
    "print(\"Structured Streaming environment configured\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“¡ Creating Your First Streaming Query\n",
    "\n",
    "### Rate Source (For Learning)\n",
    "\n",
    "**Rate source** generates test data automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a streaming DataFrame from rate source\n",
    "print(\"ðŸ“¡ CREATING FIRST STREAMING QUERY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Rate source generates rows with timestamp and value\n",
    "streaming_df = spark.readStream \\\n",
    "    .format(\"rate\") \\\n",
    "    .option(\"rowsPerSecond\", 1) \\\n",
    "    .load()\n",
    "\n",
    "print(\"Streaming DataFrame created:\")\n",
    "streaming_df.printSchema()\n",
    "print(f\"Is streaming: {streaming_df.isStreaming}\")\n",
    "\n",
    "# Show sample data (limited for demo)\n",
    "print(\"\\nSample streaming data:\")\n",
    "try:\n",
    "    # Convert to static DataFrame for demonstration\n",
    "    sample_df = streaming_df.limit(5)\n",
    "    sample_static = spark.createDataFrame(sample_df.collect())\n",
    "    sample_static.show()\n",
    "except:\n",
    "    print(\"Rate source - data will be generated when query starts\")\n",
    "\n",
    "# Basic transformation (same as DataFrame API)\n",
    "transformed_df = streaming_df \\\n",
    "    .withColumn(\"timestamp_str\", col(\"timestamp\").cast(\"string\")) \\\n",
    "    .withColumn(\"value_doubled\", col(\"value\") * 2) \\\n",
    "    .filter(col(\"value\") > 5)\n",
    "\n",
    "print(\"\\nTransformed streaming DataFrame:\")\n",
    "transformed_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Streaming Results\n",
    "\n",
    "**Streaming queries write results continuously:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write streaming results to console\n",
    "print(\"ðŸ“ STREAMING QUERY OUTPUT\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create console output query\n",
    "query = transformed_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .trigger(processingTime=\"5 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"Streaming query started!\")\n",
    "print(f\"Query ID: {query.id}\")\n",
    "print(f\"Query name: {query.name}\")\n",
    "print(f\"Is active: {query.isActive}\")\n",
    "\n",
    "# Let it run for a few seconds\n",
    "time.sleep(15)\n",
    "\n",
    "# Stop the query\n",
    "query.stop()\n",
    "print(\"\\nQuery stopped\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Output Modes\n",
    "\n",
    "**Output modes** determine how results are written:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate different output modes\n",
    "print(\"ðŸ“Š OUTPUT MODES EXPLAINED\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create a simple aggregation for demonstration\n",
    "aggregated_df = streaming_df \\\n",
    "    .groupBy() \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_count\"),\n",
    "        sum_func(\"value\").alias(\"total_sum\"),\n",
    "        F.avg(\"value\").alias(\"avg_value\")\n",
    "    )\n",
    "\n",
    "print(\"Aggregated streaming DataFrame:\")\n",
    "aggregated_df.printSchema()\n",
    "\n",
    "# Output mode comparison\n",
    "output_modes = {\n",
    "    \"complete\": \"All rows in result table (for global aggregations)\",\n",
    "    \"append\": \"Only new rows since last trigger (for append-only operations)\",\n",
    "    \"update\": \"Only changed rows since last trigger (for upserts)\"\n",
    "}\n",
    "\n",
    "print(\"\\nðŸ“‹ OUTPUT MODE COMPARISON:\")\n",
    "for mode, description in output_modes.items():\n",
    "    print(f\"\\n{mode.upper()} MODE:\")\n",
    "    print(f\"  Description: {description}\")\n",
    "\n",
    "    # When to use\n",
    "    if mode == \"complete\":\n",
    "        print(\"  Use for: Global aggregations, running totals\")\n",
    "        print(\"  Example: Total sales, global statistics\")\n",
    "    elif mode == \"append\":\n",
    "        print(\"  Use for: Append-only operations, map operations\")\n",
    "        print(\"  Example: Event processing, filtering\")\n",
    "    elif mode == \"update\":\n",
    "        print(\"  Use for: Upserts, changing aggregations\")\n",
    "        print(\"  Example: Windowed aggregations, deduplication\")\n",
    "\n",
    "# Demonstrate complete mode\n",
    "print(\"\\nðŸš€ DEMONSTRATING COMPLETE MODE:\")\n",
    "complete_query = aggregated_df.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .trigger(processingTime=\"3 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "time.sleep(12)  # Let it run for 12 seconds\n",
    "complete_query.stop()\n",
    "print(\"Complete mode demonstration finished\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## â° Triggers: When to Process Data\n",
    "\n",
    "### Processing Time vs Event Time\n",
    "\n",
    "**Triggers** control when streaming queries process data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigger types demonstration\n",
    "print(\"â° STREAMING TRIGGERS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Trigger types\n",
    "trigger_types = {\n",
    "    \"processingTime\": \"Process based on wall-clock time\",\n",
    "    \"once\": \"Process all available data once, then stop\",\n",
    "    \"continuous\": \"Process data as soon as it arrives (experimental)\"\n",
    "}\n",
    "\n",
    "print(\"TRIGGER TYPES:\")\n",
    "for trigger_type, description in trigger_types.items():\n",
    "    print(f\"\\n{trigger_type.upper()}:\")\n",
    "    print(f\"  {description}\")\n",
    "\n",
    "# Demonstrate different triggers\n",
    "print(\"\\nðŸš€ TRIGGER DEMONSTRATIONS:\")\n",
    "\n",
    "# 1. Processing time trigger (most common)\n",
    "print(\"\\n1. PROCESSING TIME TRIGGER (every 2 seconds):\")\n",
    "pt_query = streaming_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .trigger(processingTime=\"2 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "time.sleep(8)\n",
    "pt_query.stop()\n",
    "print(\"Processing time trigger stopped\")\n",
    "\n",
    "# 2. Once trigger (batch-like)\n",
    "print(\"\\n2. ONCE TRIGGER (process available data once):\")\n",
    "once_query = streaming_df.limit(10).writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .trigger(once=True) \\\n",
    "    .start()\n",
    "\n",
    "once_query.awaitTermination(timeout=10)  # Wait for completion\n",
    "print(\"Once trigger completed\")\n",
    "\n",
    "# 3. Default trigger (as fast as possible)\n",
    "print(\"\\n3. DEFAULT TRIGGER (as fast as possible):\")\n",
    "default_query = streaming_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()  # No trigger specified = default\n",
    "\n",
    "time.sleep(6)\n",
    "default_query.stop()\n",
    "print(\"Default trigger stopped\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ’¾ Fault Tolerance and Checkpointing\n",
    "\n",
    "### Checkpointing for Reliability\n",
    "\n",
    "**Checkpointing** saves streaming query state for fault recovery:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpointing for fault tolerance\n",
    "print(\"ðŸ’¾ FAULT TOLERANCE & CHECKPOINTING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create checkpoint directory\n",
    "import os\n",
    "checkpoint_dir = \"/tmp/streaming_checkpoint\"\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    import shutil\n",
    "    shutil.rmtree(checkpoint_dir)\n",
    "\n",
    "print(f\"Using checkpoint directory: {checkpoint_dir}\")\n",
    "\n",
    "# Streaming query with checkpointing\n",
    "checkpointed_query = streaming_df \\\n",
    "    .withColumn(\"running_count\", count(\"*\").over(\n",
    "        window(windowDuration=\"30 seconds\", slideDuration=\"10 seconds\")\n",
    "    )) \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"checkpointLocation\", checkpoint_dir) \\\n",
    "    .trigger(processingTime=\"3 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"Checkpointed streaming query started\")\n",
    "print(\"Checkpoints save:\")\n",
    "print(\"- Query metadata and configuration\")\n",
    "print(\"- Current batch ID and offsets\")\n",
    "print(\"- Aggregated state for stateful operations\")\n",
    "\n",
    "time.sleep(12)\n",
    "checkpointed_query.stop()\n",
    "print(\"\\nCheckpointed query stopped\")\n",
    "\n",
    "# Check checkpoint directory\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    print(f\"\\nCheckpoint files created in {checkpoint_dir}:\")\n",
    "    for root, dirs, files in os.walk(checkpoint_dir):\n",
    "        level = root.replace(checkpoint_dir, '').count(os.sep)\n",
    "        indent = ' ' * 2 * level\n",
    "        print(f\"{indent}{os.path.basename(root)}/\")\n",
    "        subindent = ' ' * 2 * (level + 1)\n",
    "        for file in files:\n",
    "            print(f\"{subindent}{file}\")\n",
    "else:\n",
    "    print(\"Checkpoint directory not found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Monitoring Streaming Queries\n",
    "\n",
    "### Query Statistics and Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitoring streaming queries\n",
    "print(\"ðŸ“Š STREAMING QUERY MONITORING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Start a query to monitor\n",
    "monitor_query = streaming_df \\\n",
    "    .groupBy(window(\"timestamp\", \"10 seconds\")) \\\n",
    "    .count() \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .trigger(processingTime=\"5 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"Monitoring streaming query...\")\n",
    "\n",
    "# Monitor query status\n",
    "def print_query_status(query):\n",
    "    print(f\"\\nQuery Status:\")\n",
    "    print(f\"  ID: {query.id}\")\n",
    "    print(f\"  Name: {query.name}\")\n",
    "    print(f\"  Active: {query.isActive}\")\n",
    "\n",
    "    if hasattr(query, 'lastProgress'):\n",
    "        progress = query.lastProgress\n",
    "        if progress:\n",
    "            print(f\"  Batch ID: {progress.get('batchId', 'N/A')}\")\n",
    "            print(f\"  Input Rows: {progress.get('numInputRows', 0)}\")\n",
    "            print(f\"  Processed Rows: {progress.get('processedRowsPerSecond', 0):.1f}/sec\")\n",
    "            print(f\"  Batch Duration: {progress.get('batchDuration', 'N/A')}ms\")\n",
    "\n",
    "# Monitor for a few batches\n",
    "for i in range(3):\n",
    "    time.sleep(6)  # Wait for batch to complete\n",
    "    print_query_status(monitor_query)\n",
    "\n",
    "monitor_query.stop()\n",
    "print(\"\\nMonitoring completed\")\n",
    "\n",
    "# List all active streaming queries\n",
    "print(\"\\nActive streaming queries:\")\n",
    "for q in spark.streams.active:\n",
    "    print(f\"- {q.name} ({q.id}): {'Active' if q.isActive else 'Inactive'}\")\n",
    "\n",
    "if not spark.streams.active:\n",
    "    print(\"No active streaming queries\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”§ Advanced Streaming Concepts\n",
    "\n",
    "### Handling Different Data Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced streaming concepts\n",
    "print(\"ðŸ”§ ADVANCED STREAMING CONCEPTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Multiple streaming sources\n",
    "print(\"1. MULTIPLE STREAMING SOURCES\")\n",
    "\n",
    "# Create two different rate streams\n",
    "stream1 = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 2).load()\n",
    "stream2 = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 1).load()\n",
    "\n",
    "# Add source identifier\n",
    "stream1_labeled = stream1.withColumn(\"source\", F.lit(\"stream1\"))\n",
    "stream2_labeled = stream2.withColumn(\"source\", F.lit(\"stream2\"))\n",
    "\n",
    "# Union streams\n",
    "combined_stream = stream1_labeled.union(stream2_labeled)\n",
    "\n",
    "print(\"Combined streaming sources:\")\n",
    "combined_stream.printSchema()\n",
    "\n",
    "# 2. Streaming with static data joins\n",
    "print(\"\\n2. STREAMING WITH STATIC DATA JOINS\")\n",
    "\n",
    "# Create static lookup table\n",
    "lookup_data = [\n",
    "    (0, \"Even\"),\n",
    "    (1, \"Odd\"),\n",
    "    (2, \"Even\"),\n",
    "    (3, \"Odd\")\n",
    "]\n",
    "\n",
    "lookup_df = spark.createDataFrame(lookup_data, [\"value\", \"parity\"])\n",
    "\n",
    "# Join streaming data with static lookup\n",
    "enriched_stream = combined_stream.join(\n",
    "    F.broadcast(lookup_df),\n",
    "    combined_stream[\"value\"] == lookup_df[\"value\"],\n",
    "    \"left\"\n",
    ")\n",
    "\n",
    "print(\"Streaming + static data join:\")\n",
    "enriched_stream.printSchema()\n",
    "\n",
    "# 3. Conditional streaming logic\n",
    "print(\"\\n3. CONDITIONAL STREAMING LOGIC\")\n",
    "\n",
    "conditional_stream = enriched_stream.withColumn(\n",
    "    \"status\",\n",
    "    F.when(col(\"value\") > 5, \"High\")\n",
    "     .when(col(\"value\") > 2, \"Medium\")\n",
    "     .otherwise(\"Low\")\n",
    ").withColumn(\n",
    "    \"alert\",\n",
    "    F.when(col(\"source\") == \"stream1\", \"Priority\")\n",
    "     .otherwise(\"Normal\")\n",
    ")\n",
    "\n",
    "# Demonstrate complex streaming pipeline\n",
    "complex_query = conditional_stream.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .trigger(processingTime=\"4 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "time.sleep(16)\n",
    "complex_query.stop()\n",
    "print(\"\\nComplex streaming pipeline completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš¨ Error Handling and Recovery\n",
    "\n",
    "### Handling Streaming Failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error handling and recovery\n",
    "print(\"ðŸš¨ ERROR HANDLING AND RECOVERY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Graceful shutdown\n",
    "print(\"1. GRACEFUL SHUTDOWN\")\n",
    "\n",
    "def graceful_shutdown(query, timeout_seconds=30):\n",
    "    \"\"\"Gracefully shutdown streaming query\"\"\"\n",
    "    try:\n",
    "        print(f\"Initiating graceful shutdown for query {query.id}...\")\n",
    "\n",
    "        # Stop accepting new data\n",
    "        query.stop()\n",
    "\n",
    "        # Wait for processing to complete\n",
    "        query.awaitTermination(timeout_seconds * 1000)\n",
    "\n",
    "        print(\"âœ… Query shutdown successfully\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Error during shutdown: {e}\")\n",
    "        # Force stop if graceful shutdown fails\n",
    "        query.stop()\n",
    "\n",
    "# 2. Recovery from checkpoint\n",
    "print(\"\\n2. RECOVERY FROM CHECKPOINT\")\n",
    "\n",
    "def recover_or_create_query(checkpoint_path, create_query_func):\n",
    "    \"\"\"Recover query from checkpoint or create new one\"\"\"\n",
    "\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(f\"Found checkpoint at {checkpoint_path}\")\n",
    "        print(\"Attempting to recover query...\")\n",
    "\n",
    "        try:\n",
    "            # In production, you'd recover the specific query\n",
    "            # For demo, we'll create a new one\n",
    "            query = create_query_func()\n",
    "            print(\"âœ… Query recovered successfully\")\n",
    "            return query\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Recovery failed: {e}\")\n",
    "            print(\"Creating new query...\")\n",
    "\n",
    "    # Create new query\n",
    "    query = create_query_func()\n",
    "    print(\"âœ… New query created\")\n",
    "    return query\n",
    "\n",
    "# 3. Monitoring and alerting\n",
    "print(\"\\n3. MONITORING AND ALERTING\")\n",
    "\n",
    "def monitor_streaming_health(query, alert_threshold_seconds=60):\n",
    "    \"\"\"Monitor streaming query health\"\"\"\n",
    "\n",
    "    last_batch_time = time.time()\n",
    "\n",
    "    while query.isActive:\n",
    "        time.sleep(10)  # Check every 10 seconds\n",
    "\n",
    "        current_time = time.time()\n",
    "\n",
    "        # Check if query is still processing\n",
    "        if hasattr(query, 'lastProgress') and query.lastProgress:\n",
    "            progress = query.lastProgress\n",
    "            batch_id = progress.get('batchId', 0)\n",
    "\n",
    "            # Update last batch time\n",
    "            if batch_id > 0:\n",
    "                last_batch_time = current_time\n",
    "\n",
    "        # Alert if no batches processed recently\n",
    "        time_since_last_batch = current_time - last_batch_time\n",
    "        if time_since_last_batch > alert_threshold_seconds:\n",
    "            print(f\"ðŸš¨ ALERT: No batch processed in {time_since_last_batch:.0f} seconds\")\n",
    "            print(\"Query may be stuck or failing\")\n",
    "            break\n",
    "\n",
    "    print(\"Health monitoring stopped\")\n",
    "\n",
    "# Demonstrate error handling\n",
    "print(\"\\nDEMONSTRATING ERROR HANDLING:\")\n",
    "\n",
    "# Create a query that might fail\n",
    "test_query = streaming_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .trigger(processingTime=\"2 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "# Start health monitoring in background\n",
    "monitor_thread = threading.Thread(target=monitor_streaming_health, args=(test_query,))\n",
    "monitor_thread.daemon = True\n",
    "monitor_thread.start()\n",
    "\n",
    "# Let it run for a bit\n",
    "time.sleep(20)\n",
    "\n",
    "# Graceful shutdown\n",
    "graceful_shutdown(test_query, 10)\n",
    "\n",
    "print(\"\\nError handling demonstration completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Streaming Best Practices\n",
    "\n",
    "### Production-Ready Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming best practices\n",
    "print(\"ðŸŽ¯ STREAMING BEST PRACTICES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Resource configuration\n",
    "print(\"1. RESOURCE CONFIGURATION\")\n",
    "streaming_configs = {\n",
    "    \"spark.streaming.backpressure.enabled\": \"true\",\n",
    "    \"spark.streaming.receiver.maxRate\": \"10000\",\n",
    "    \"spark.streaming.kafka.maxRatePerPartition\": \"1000\",\n",
    "    \"spark.sql.shuffle.partitions\": \"50\",\n",
    "    \"spark.streaming.concurrentJobs\": \"3\"\n",
    "}\n",
    "\n",
    "print(\"Recommended streaming configurations:\")\n",
    "for key, value in streaming_configs.items():\n",
    "    print(f\"  {key} = {value}\")\n",
    "\n",
    "# 2. Checkpointing strategy\n",
    "print(\"\\n2. CHECKPOINTING STRATEGY\")\n",
    "checkpoint_best_practices = [\n",
    "    \"Use reliable storage (HDFS/S3) for checkpoints\",\n",
    "    \"Configure appropriate checkpoint intervals\",\n",
    "    \"Monitor checkpoint size and cleanup old checkpoints\",\n",
    "    \"Test recovery from checkpoints regularly\",\n",
    "    \"Use separate checkpoint locations for different queries\"\n",
    "]\n",
    "\n",
    "for practice in checkpoint_best_practices:\n",
    "    print(f\"  âœ“ {practice}\")\n",
    "\n",
    "# 3. Monitoring strategy\n",
    "print(\"\\n3. MONITORING STRATEGY\")\n",
    "monitoring_best_practices = [\n",
    "    \"Monitor batch processing time and throughput\",\n",
    "    \"Set up alerts for query failures and delays\",\n",
    "    \"Track input data rate and backpressure\",\n",
    "    \"Monitor checkpoint and state store sizes\",\n",
    "    \"Log and analyze query progress metrics\"\n",
    "]\n",
    "\n",
    "for practice in monitoring_best_practices:\n",
    "    print(f\"  âœ“ {practice}\")\n",
    "\n",
    "# 4. Error handling strategy\n",
    "print(\"\\n4. ERROR HANDLING STRATEGY\")\n",
    "error_handling = [\n",
    "    \"Implement graceful shutdown procedures\",\n",
    "    \"Use checkpointing for automatic recovery\",\n",
    "    \"Handle deserialization errors gracefully\",\n",
    "    \"Implement dead letter queues for bad records\",\n",
    "    \"Set appropriate timeouts and retry policies\"\n",
    "]\n",
    "\n",
    "for practice in error_handling:\n",
    "    print(f\"  âœ“ {practice}\")\n",
    "\n",
    "# 5. Performance optimization\n",
    "print(\"\\n5. PERFORMANCE OPTIMIZATION\")\n",
    "performance_tips = [\n",
    "    \"Use appropriate output modes for your use case\",\n",
    "    \"Minimize state in stateful operations\",\n",
    "    \"Use watermarking to limit state growth\",\n",
    "    \"Choose efficient data formats (Parquet/ORC)\",\n",
    "    \"Optimize trigger intervals based on latency requirements\"\n",
    "]\n",
    "\n",
    "for tip in performance_tips:\n",
    "    print(f\"  âœ“ {tip}\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ STREAMING BEST PRACTICES SUMMARY:\")\n",
    "print(\"- Start simple, then optimize based on requirements\")\n",
    "print(\"- Always use checkpointing in production\")\n",
    "print(\"- Monitor query performance continuously\")\n",
    "print(\"- Test failure scenarios and recovery procedures\")\n",
    "print(\"- Balance throughput vs latency based on use case\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Key Takeaways\n",
    "\n",
    "### Streaming Fundamentals Learned:\n",
    "- âœ… **Unbounded data** - Continuous processing vs batch\n",
    "- âœ… **Structured Streaming** - DataFrame API for streams\n",
    "- âœ… **Output modes** - Append, Update, Complete semantics\n",
    "- âœ… **Triggers** - Processing time vs event time control\n",
    "- âœ… **Checkpointing** - Fault tolerance and state recovery\n",
    "- âœ… **Monitoring** - Query statistics and health checks\n",
    "\n",
    "### Core Streaming Concepts:\n",
    "- ðŸ”¸ **Source**: Where streaming data comes from (Kafka, files, sockets)\n",
    "- ðŸ”¸ **Processing**: Same DataFrame operations as batch\n",
    "- ðŸ”¸ **Sink**: Where results are written (files, databases, consoles)\n",
    "- ðŸ”¸ **State**: Maintained across batches for aggregations\n",
    "- ðŸ”¸ **Checkpointing**: Saves progress for fault recovery\n",
    "\n",
    "### Key Differences from Batch:\n",
    "- ðŸ”¸ **Continuous**: Never-ending data processing\n",
    "- ðŸ”¸ **Incremental**: Process data as it arrives\n",
    "- ðŸ”¸ **Stateful**: Maintain state across time windows\n",
    "- ðŸ”¸ **Fault-tolerant**: Automatic recovery from failures\n",
    "- ðŸ”¸ **Late data**: Handle out-of-order events\n",
    "\n",
    "### Output Mode Selection:\n",
    "- ðŸ”¸ **Append**: New rows only (map operations, filtering)\n",
    "- ðŸ”¸ **Complete**: All rows (global aggregations)\n",
    "- ðŸ”¸ **Update**: Changed rows only (windowed aggregations)\n",
    "\n",
    "### Trigger Types:\n",
    "- ðŸ”¸ **Processing Time**: Wall-clock based (most common)\n",
    "- ðŸ”¸ **Once**: Process available data once (batch-like)\n",
    "- ðŸ”¸ **Continuous**: As-fast-as-possible (experimental)\n",
    "\n",
    "### Production Considerations:\n",
    "- ðŸ”¸ **Checkpointing**: Essential for fault tolerance\n",
    "- ðŸ”¸ **Monitoring**: Track query health and performance\n",
    "- ðŸ”¸ **Resource tuning**: Memory, CPU, parallelism\n",
    "- ðŸ”¸ **Error handling**: Graceful failure recovery\n",
    "- ðŸ”¸ **Backpressure**: Handle data rate mismatches\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸš€ Next Steps\n",
    "\n",
    "Now that you understand streaming basics, you're ready for:\n",
    "\n",
    "1. **Windowing Operations** - Time-based aggregations\n",
    "2. **Event Time Processing** - Handling late and out-of-order data\n",
    "3. **Watermarking** - Managing state in streaming queries\n",
    "4. **Stateful Operations** - Advanced streaming transformations\n",
    "5. **Streaming Joins** - Joining streaming data\n",
    "6. **Sources and Sinks** - Kafka, files, databases\n",
    "\n",
    "**Structured Streaming makes real-time data processing as easy as batch processing!**\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸŽ‰ Congratulations! You now have streaming superpowers in Spark!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
