{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚ö° Spark Session: Your Gateway to PySpark\n",
    "\n",
    "**Time to complete:** 15 minutes  \n",
    "**Difficulty:** Beginner  \n",
    "**Prerequisites:** Python basics\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- ‚úÖ Understand what a Spark Session is\n",
    "- ‚úÖ Learn to create and configure Spark Sessions\n",
    "- ‚úÖ Know how to check your Spark environment\n",
    "- ‚úÖ Understand basic Spark configuration options\n",
    "- ‚úÖ Be ready to start working with PySpark data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç What is a Spark Session?\n",
    "\n",
    "A **Spark Session** is your entry point to all Spark functionality. Think of it as:\n",
    "\n",
    "- **The front door** to your Spark application\n",
    "- **The configuration manager** for your Spark jobs\n",
    "- **The factory** that creates RDDs, DataFrames, and Datasets\n",
    "- **The coordinator** that manages your cluster resources\n",
    "\n",
    "**Without a Spark Session, you can't use PySpark!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Creating Your First Spark Session\n",
    "\n",
    "Let's start with the most basic Spark Session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Step 2: Create a basic Spark Session\n",
    "spark = SparkSession.builder.appName(\"MyFirstSparkSession\").getOrCreate()\n",
    "\n",
    "# Step 3: Verify it worked\n",
    "print(\"‚úÖ Spark Session created successfully!\")\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Application Name: {spark.sparkContext.appName}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéâ Congratulations!\n",
    "\n",
    "You just created your first Spark Session! This is the foundation for everything you'll do in PySpark.\n",
    "\n",
    "Notice:\n",
    "- `SparkSession.builder` - The builder pattern for configuration\n",
    "- `.appName()` - Gives your application a descriptive name\n",
    "- `.getOrCreate()` - Creates new session or returns existing one\n",
    "- `spark.version` - Shows your Spark version\n",
    "- `spark.sparkContext` - Access to the underlying SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Spark Session Configuration\n",
    "\n",
    "Let's explore more configuration options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a more advanced Spark Session with custom configuration\n",
    "spark_advanced = SparkSession.builder \\\n",
    "    .appName(\"AdvancedSparkSession\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"üîß Advanced Spark Session configured!\")\n",
    "print(f\"Master: {spark_advanced.sparkContext.master}\")\n",
    "print(f\"Hive Support: {'Enabled' if spark_advanced.sparkContext.getConf().get('spark.sql.catalogImplementation') == 'hive' else 'Disabled'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Breakdown:\n",
    "\n",
    "| Configuration | Purpose |\n",
    "|---------------|---------|\n",
    "| `appName()` | Identifies your application in the Spark UI |\n",
    "| `master()` | Specifies cluster manager (local[*] = all cores) |\n",
    "| `spark.sql.adaptive.enabled` | Enables adaptive query execution |\n",
    "| `spark.driver.memory` | Memory for the driver program |\n",
    "| `spark.executor.memory` | Memory for each executor |\n",
    "| `enableHiveSupport()` | Adds Hive functionality |\n",
    "\n",
    "**üí° Pro Tip:** Start with `local[*]` for development, change to cluster URL for production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Exploring Your Spark Environment\n",
    "\n",
    "Let's learn what information we can get from our Spark Session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get basic information\n",
    "print(\"=== SPARK SESSION INFORMATION ===\")\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Python Version: {spark.sparkContext.pythonVer}\")\n",
    "print(f\"Application ID: {spark.sparkContext.applicationId}\")\n",
    "print(f\"Master URL: {spark.sparkContext.master}\")\n",
    "print(f\"UI Available: http://localhost:4040\")\n",
    "\n",
    "# Get configuration details\n",
    "print(\"\\n=== KEY CONFIGURATIONS ===\")\n",
    "config = spark.sparkContext.getConf()\n",
    "print(f\"Driver Memory: {config.get('spark.driver.memory', 'default')}\")\n",
    "print(f\"Executor Memory: {config.get('spark.executor.memory', 'default')}\")\n",
    "print(f\"Default Parallelism: {spark.sparkContext.defaultParallelism}\")\n",
    "print(f\"Default Partitions: {spark.sparkContext.defaultMinPartitions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç What Each Value Means:\n",
    "\n",
    "- **Application ID**: Unique identifier for your Spark job\n",
    "- **Master URL**: Where your Spark job is running (`local[*]` = local machine)\n",
    "- **Spark UI**: Web interface at http://localhost:4040 to monitor jobs\n",
    "- **Parallelism**: How many tasks can run simultaneously\n",
    "- **Partitions**: How data is split for distributed processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Testing Your Spark Session\n",
    "\n",
    "Let's do a quick test to make sure everything works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple DataFrame to test\n",
    "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35)]\n",
    "df = spark.createDataFrame(data, [\"name\", \"age\"])\n",
    "\n",
    "print(\"üéØ Spark Session Test Results:\")\n",
    "print(f\"DataFrame created with {df.count()} rows\")\n",
    "print(\"\\nDataFrame content:\")\n",
    "df.show()\n",
    "\n",
    "print(\"\\n‚úÖ Your Spark Session is working perfectly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßπ Cleaning Up\n",
    "\n",
    "When you're done, it's good practice to stop your Spark Session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Spark Session (optional - Jupyter will do this automatically)\n",
    "# spark.stop()\n",
    "# spark_advanced.stop()\n",
    "\n",
    "print(\"üßπ Spark Sessions can be stopped with spark.stop()\")\n",
    "print(\"üí° In Jupyter, they're usually stopped automatically\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Key Takeaways\n",
    "\n",
    "### What You Learned:\n",
    "- ‚úÖ **SparkSession.builder** creates Spark Sessions\n",
    "- ‚úÖ **.appName()** gives your app a descriptive name\n",
    "- ‚úÖ **.master()** specifies where to run (local for development)\n",
    "- ‚úÖ **.config()** sets various Spark properties\n",
    "- ‚úÖ **.getOrCreate()** creates or returns existing session\n",
    "- ‚úÖ **spark.sparkContext** accesses low-level Spark functionality\n",
    "\n",
    "### Best Practices:\n",
    "- üî∏ Always give your applications descriptive names\n",
    "- üî∏ Use `local[*]` for development, cluster URLs for production\n",
    "- üî∏ Check Spark UI at http://localhost:4040 to monitor jobs\n",
    "- üî∏ Configure memory appropriately for your workload\n",
    "- üî∏ Stop Spark Sessions when done (though Jupyter handles this)\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Next Steps\n",
    "\n",
    "Now that you can create Spark Sessions, you're ready for:\n",
    "\n",
    "1. **RDD Introduction** - Understanding Resilient Distributed Datasets\n",
    "2. **DataFrame Basics** - Working with structured data\n",
    "3. **Transformations vs Actions** - Understanding lazy evaluation\n",
    "\n",
    "**Keep this Spark Session running** - you'll need it for the next notebooks!\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ You've successfully created your first Spark Session! Welcome to the world of distributed computing!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
