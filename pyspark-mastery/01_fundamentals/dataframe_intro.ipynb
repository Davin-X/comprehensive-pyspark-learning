{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“Š DataFrame Introduction: Structured Data Processing\n",
    "\n",
    "**Time to complete:** 30 minutes  \n",
    "**Difficulty:** Beginner  \n",
    "**Prerequisites:** Spark Session and RDD basics\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- âœ… Understand what DataFrames are and why they're important\n",
    "- âœ… Learn how to create DataFrames from different sources\n",
    "- âœ… Know how to inspect and explore DataFrame structure\n",
    "- âœ… Perform basic DataFrame operations (select, filter, sort)\n",
    "- âœ… Understand the relationship between DataFrames and RDDs\n",
    "- âœ… Be ready to work with structured data in PySpark\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ” What is a DataFrame?\n",
    "\n",
    "A **DataFrame** is Spark's primary abstraction for working with **structured data**. Think of it as:\n",
    "\n",
    "- **A distributed table** with named columns\n",
    "- **Schema-aware** data with defined data types\n",
    "- **Higher-level API** compared to RDDs\n",
    "- **Optimized for analytics** and SQL-like operations\n",
    "\n",
    "### Why DataFrames Matter:\n",
    "- Built on top of RDDs but easier to use\n",
    "- Automatic optimization via Catalyst optimizer\n",
    "- Support for SQL queries\n",
    "- Integration with pandas, databases, and file formats\n",
    "\n",
    "**DataFrames are what you'll use for 90% of your PySpark work!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ—ï¸ Creating DataFrames\n",
    "\n",
    "There are many ways to create DataFrames in PySpark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/12/20 12:07:56 WARN Utils: Your hostname, Devs-MacBook-Air.local, resolves to a loopback address: 127.0.0.1; using 192.168.0.107 instead (on interface en0)\n",
      "25/12/20 12:07:56 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/20 12:07:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/12/20 12:07:58 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Spark Session ready - Version: 4.1.0\n"
     ]
    }
   ],
   "source": [
    "# First, create a Spark Session (if not already created)\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataFrame_Introduction\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"âœ… Spark Session ready - Version: {spark.version}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: From Python Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame from Python list of tuples\n",
    "data = [\n",
    "    (\"Alice\", 25, \"Engineering\", 75000),\n",
    "    (\"Bob\", 30, \"Sales\", 65000),\n",
    "    (\"Charlie\", 35, \"Engineering\", 85000),\n",
    "    (\"Diana\", 28, \"HR\", 55000),\n",
    "    (\"Eve\", 32, \"Sales\", 70000)\n",
    "]\n",
    "\n",
    "# Define column names\n",
    "columns = [\"name\", \"age\", \"department\", \"salary\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "print(\"âœ… DataFrame created from Python lists\")\n",
    "print(f\"Type: {type(df)}\")\n",
    "print(f\"Number of rows: {df.count()}\")\n",
    "print(f\"Number of columns: {len(df.columns)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: From Python Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame from list of dictionaries\n",
    "dict_data = [\n",
    "    {\"name\": \"Frank\", \"age\": 29, \"department\": \"Marketing\", \"salary\": 60000},\n",
    "    {\"name\": \"Grace\", \"age\": 31, \"department\": \"Finance\", \"salary\": 72000},\n",
    "    {\"name\": \"Henry\", \"age\": 27, \"department\": \"IT\", \"salary\": 68000}\n",
    "]\n",
    "\n",
    "# Create DataFrame (column names inferred from dict keys)\n",
    "df_dict = spark.createDataFrame(dict_data)\n",
    "\n",
    "print(\"âœ… DataFrame created from dictionaries\")\n",
    "print(f\"Columns: {df_dict.columns}\")\n",
    "df_dict.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 3: From RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RDD first\n",
    "rdd_data = spark.sparkContext.parallelize([\n",
    "    (\"Ivy\", 33, \"Operations\", 71000),\n",
    "    (\"Jack\", 26, \"Engineering\", 78000)\n",
    "])\n",
    "\n",
    "# Convert RDD to DataFrame\n",
    "df_from_rdd = rdd_data.toDF([\"name\", \"age\", \"department\", \"salary\"])\n",
    "\n",
    "print(\"âœ… DataFrame created from RDD\")\n",
    "print(\"RDD â†’ DataFrame conversion:\")\n",
    "df_from_rdd.show()\n",
    "\n",
    "# Combine with existing DataFrame\n",
    "combined_df = df.union(df_from_rdd)\n",
    "print(f\"\\nCombined DataFrame has {combined_df.count()} rows\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 4: From External Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample CSV data for demonstration\n",
    "csv_data = \"\"\"name,age,department,salary\n",
    "Kevin,29,Data Science,80000\n",
    "Luna,31,Product,75000\n",
    "Mike,28,Engineering,82000\"\"\"\n",
    "\n",
    "with open(\"sample_data.csv\", \"w\") as f:\n",
    "    f.write(csv_data)\n",
    "\n",
    "# Read CSV file\n",
    "df_csv = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"sample_data.csv\")\n",
    "\n",
    "print(\"âœ… DataFrame created from CSV file\")\n",
    "df_csv.show()\n",
    "print(f\"Schema: {df_csv.dtypes}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“‹ DataFrame Inspection\n",
    "\n",
    "Let's explore how to examine DataFrame structure and content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use our main DataFrame for exploration\n",
    "print(\"ðŸ” DataFrame Inspection:\")\n",
    "print(f\"Shape: {df.count()} rows Ã— {len(df.columns)} columns\")\n",
    "print(f\"Columns: {df.columns}\")\n",
    "print(f\"Data types: {df.dtypes}\")\n",
    "\n",
    "# Display data\n",
    "print(\"\\nðŸ“Š First 3 rows:\")\n",
    "df.show(3)\n",
    "\n",
    "print(\"ðŸ“‹ Schema information:\")\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Schema\n",
    "\n",
    "The **schema** defines the structure of your DataFrame:\n",
    "- **Column names** and their **data types**\n",
    "- **Nullable** indicates if null values are allowed\n",
    "- **Automatically inferred** or **explicitly defined**\n",
    "\n",
    "Schemas ensure **type safety** and enable **optimizations**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Basic DataFrame Operations\n",
    "\n",
    "Now let's perform common operations on DataFrames:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select specific columns\n",
    "print(\"ðŸŽ¯ Column Selection:\")\n",
    "\n",
    "# Method 1: Using column names\n",
    "name_dept = df.select(\"name\", \"department\")\n",
    "print(\"Names and departments:\")\n",
    "name_dept.show()\n",
    "\n",
    "# Method 2: Using column expressions\n",
    "from pyspark.sql.functions import col\n",
    "name_and_salary = df.select(col(\"name\"), col(\"salary\"))\n",
    "print(\"\\nNames and salaries:\")\n",
    "name_and_salary.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows based on conditions\n",
    "print(\"ðŸ” Row Filtering:\")\n",
    "\n",
    "# Simple filter\n",
    "engineers = df.filter(col(\"department\") == \"Engineering\")\n",
    "print(\"Engineering team:\")\n",
    "engineers.show()\n",
    "\n",
    "# Multiple conditions\n",
    "high_earners = df.filter((col(\"salary\") > 70000) & (col(\"age\") < 35))\n",
    "print(\"\\nHigh earners under 35:\")\n",
    "high_earners.show()\n",
    "\n",
    "# Age range filter\n",
    "young_professionals = df.filter(col(\"age\").between(25, 30))\n",
    "print(\"\\nYoung professionals (25-30):\")\n",
    "young_professionals.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort DataFrame by columns\n",
    "print(\"ðŸ“ˆ Data Sorting:\")\n",
    "\n",
    "# Sort by salary (descending)\n",
    "by_salary = df.orderBy(col(\"salary\").desc())\n",
    "print(\"Sorted by salary (highest first):\")\n",
    "by_salary.show()\n",
    "\n",
    "# Sort by department, then by age\n",
    "by_dept_age = df.orderBy([\"department\", \"age\"])\n",
    "print(\"\\nSorted by department and age:\")\n",
    "by_dept_age.show()\n",
    "\n",
    "# Sort by age (ascending) and salary (descending)\n",
    "complex_sort = df.orderBy(col(\"age\").asc(), col(\"salary\").desc())\n",
    "print(\"\\nSorted by age (asc) and salary (desc):\")\n",
    "complex_sort.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding and Modifying Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new columns and modify existing ones\n",
    "print(\"âž• Column Operations:\")\n",
    "\n",
    "# Add bonus column (10% of salary)\n",
    "with_bonus = df.withColumn(\"bonus\", col(\"salary\") * 0.1)\n",
    "print(\"Added bonus column:\")\n",
    "with_bonus.select(\"name\", \"salary\", \"bonus\").show()\n",
    "\n",
    "# Add experience level based on age\n",
    "from pyspark.sql.functions import when\n",
    "with_experience = df.withColumn(\n",
    "    \"experience_level\",\n",
    "    when(col(\"age\") < 28, \"Junior\")\n",
    "    .when(col(\"age\") < 33, \"Mid-level\")\n",
    "    .otherwise(\"Senior\")\n",
    ")\n",
    "print(\"\\nAdded experience level:\")\n",
    "with_experience.select(\"name\", \"age\", \"experience_level\").show()\n",
    "\n",
    "# Rename column\n",
    "renamed_df = df.withColumnRenamed(\"salary\", \"annual_salary\")\n",
    "print(\"\\nRenamed 'salary' to 'annual_salary':\")\n",
    "print(f\"Columns: {renamed_df.columns}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Aggregation Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group and aggregate data\n",
    "from pyspark.sql.functions import avg, sum, count, max, min\n",
    "\n",
    "print(\"ðŸ“Š Aggregation Operations:\")\n",
    "\n",
    "# Group by department and calculate statistics\n",
    "dept_stats = df.groupBy(\"department\").agg(\n",
    "    count(\"name\").alias(\"employee_count\"),\n",
    "    avg(\"salary\").alias(\"avg_salary\"),\n",
    "    max(\"salary\").alias(\"max_salary\"),\n",
    "    min(\"age\").alias(\"min_age\")\n",
    ")\n",
    "\n",
    "print(\"Department statistics:\")\n",
    "dept_stats.show()\n",
    "\n",
    "# Overall statistics\n",
    "overall_stats = df.agg(\n",
    "    count(\"*\").alias(\"total_employees\"),\n",
    "    avg(\"salary\").alias(\"avg_company_salary\"),\n",
    "    sum(\"salary\").alias(\"total_salary_cost\"),\n",
    "    max(\"age\").alias(\"oldest_employee\"),\n",
    "    min(\"age\").alias(\"youngest_employee\")\n",
    ")\n",
    "\n",
    "print(\"\\nCompany-wide statistics:\")\n",
    "overall_stats.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”„ DataFrames vs RDDs\n",
    "\n",
    "Understanding when to use each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create equivalent RDD and DataFrame\n",
    "sample_data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35)]\n",
    "\n",
    "# RDD approach\n",
    "rdd = spark.sparkContext.parallelize(sample_data)\n",
    "rdd_result = rdd.filter(lambda x: x[1] > 26).map(lambda x: (x[0], x[1] * 2)).collect()\n",
    "\n",
    "# DataFrame approach\n",
    "df_sample = spark.createDataFrame(sample_data, [\"name\", \"age\"])\n",
    "df_result = df_sample.filter(col(\"age\") > 26).withColumn(\"doubled_age\", col(\"age\") * 2)\n",
    "\n",
    "print(\"ðŸ”„ RDD vs DataFrame Comparison:\")\n",
    "print(f\"RDD result: {rdd_result}\")\n",
    "print(\"DataFrame result:\")\n",
    "df_result.show()\n",
    "\n",
    "print(\"\\nKey Differences:\")\n",
    "print(\"- RDDs: Low-level, manual optimization, all data types\")\n",
    "print(\"- DataFrames: High-level, automatic optimization, structured data\")\n",
    "print(\"- Choose DataFrames for 90% of analytics workloads!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§¹ Cleaning Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up sample files\n",
    "import os\n",
    "if os.path.exists(\"sample_data.csv\"):\n",
    "    os.remove(\"sample_data.csv\")\n",
    "\n",
    "print(\"ðŸ§¹ Cleanup completed\")\n",
    "\n",
    "# Note: Spark Session will be cleaned up automatically in Jupyter\n",
    "# In production code, use: spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Key Takeaways\n",
    "\n",
    "### What You Learned:\n",
    "- âœ… **DataFrames** are Spark's primary tool for structured data\n",
    "- âœ… **Schema** defines column names and data types\n",
    "- âœ… **Multiple creation methods** from lists, dicts, RDDs, files\n",
    "- âœ… **Rich operations** for selection, filtering, sorting, aggregation\n",
    "- âœ… **Higher-level API** than RDDs with automatic optimization\n",
    "\n",
    "### DataFrame Advantages:\n",
    "- ðŸ”¸ **Easier to use** than RDDs for structured data\n",
    "- ðŸ”¸ **Automatic optimization** via Catalyst optimizer\n",
    "- ðŸ”¸ **SQL support** for familiar query syntax\n",
    "- ðŸ”¸ **Rich ecosystem** of built-in functions\n",
    "- ðŸ”¸ **Better performance** for analytical workloads\n",
    "\n",
    "### Common Operations:\n",
    "- ðŸ”¸ `select()` - Choose columns\n",
    "- ðŸ”¸ `filter()` - Filter rows by conditions\n",
    "- ðŸ”¸ `orderBy()` - Sort data\n",
    "- ðŸ”¸ `groupBy().agg()` - Group and aggregate\n",
    "- ðŸ”¸ `withColumn()` - Add/modify columns\n",
    "- ðŸ”¸ `show()` - Display results\n",
    "\n",
    "### When to Use DataFrames:\n",
    "- âœ… **Structured data** with defined schema\n",
    "- âœ… **SQL-like operations** (filtering, aggregation)\n",
    "- âœ… **File-based data** (CSV, JSON, Parquet)\n",
    "- âœ… **Analytics and reporting** workloads\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸš€ Next Steps\n",
    "\n",
    "Now that you understand DataFrames, you're ready for:\n",
    "\n",
    "1. **Transformations vs Actions Deep Dive** - Understanding lazy evaluation\n",
    "2. **DataFrame Mastery** - Advanced operations and window functions\n",
    "3. **Spark SQL** - Querying DataFrames with SQL\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸŽ‰ Congratulations! You now have the foundation for working with structured data in PySpark!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
