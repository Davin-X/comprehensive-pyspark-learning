{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ”„ RDD Introduction: The Foundation of Spark\n",
    "\n",
    "**Time to complete:** 25 minutes  \n",
    "**Difficulty:** Beginner  \n",
    "**Prerequisites:** Spark Session basics\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- âœ… Understand what RDDs (Resilient Distributed Datasets) are\n",
    "- âœ… Learn how to create RDDs from different sources\n",
    "- âœ… Know the difference between transformations and actions\n",
    "- âœ… Understand RDD persistence and fault tolerance\n",
    "- âœ… Be able to perform basic RDD operations\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” What is an RDD?\n",
    "\n",
    "**RDD** stands for **Resilient Distributed Dataset**. It's Spark's fundamental data structure.\n",
    "\n",
    "### Key Characteristics:\n",
    "- **Resilient**: Fault-tolerant, automatically recovers from failures\n",
    "- **Distributed**: Data is split across multiple machines\n",
    "- **Dataset**: Collection of data that can be processed in parallel\n",
    "\n",
    "### Why RDDs Matter:\n",
    "- Foundation of all Spark operations\n",
    "- Enable distributed processing\n",
    "- Provide fault tolerance\n",
    "- Support complex transformations\n",
    "\n",
    "**Think of RDDs as distributed, fault-tolerant collections that can be processed in parallel.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ—ï¸ Creating RDDs\n",
    "\n",
    "There are several ways to create RDDs in PySpark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, create a Spark Session (if not already created)\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RDD_Introduction\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "print(f\"âœ… Spark Session ready - Version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: From a Python Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RDD from a Python list\n",
    "data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "rdd_from_list = sc.parallelize(data)\n",
    "\n",
    "print(\"âœ… RDD created from Python list\")\n",
    "print(f\"Type: {type(rdd_from_list)}\")\n",
    "print(f\"Number of partitions: {rdd_from_list.getNumPartitions()}\")\n",
    "print(f\"First element: {rdd_from_list.first()}\")\n",
    "print(f\"Total elements: {rdd_from_list.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: From a Text File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample text file for demonstration\n",
    "sample_text = \"\"\"Hello World\n",
    "Welcome to PySpark\n",
    "RDDs are powerful\n",
    "Distributed computing\n",
    "Fault tolerance\"\"\"\n",
    "\n",
    "with open(\"sample_text.txt\", \"w\") as f:\n",
    "    f.write(sample_text)\n",
    "\n",
    "# Create RDD from text file\n",
    "text_rdd = sc.textFile(\"sample_text.txt\")\n",
    "\n",
    "print(\"âœ… RDD created from text file\")\n",
    "print(f\"Lines in file: {text_rdd.count()}\")\n",
    "print(\"Content:\")\n",
    "for line in text_rdd.collect():\n",
    "    print(f\"  '{line}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 3: From Key-Value Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RDD with key-value pairs\n",
    "kv_data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35), (\"Diana\", 28)]\n",
    "kv_rdd = sc.parallelize(kv_data)\n",
    "\n",
    "print(\"âœ… RDD created with key-value pairs\")\n",
    "print(\"Content:\")\n",
    "for item in kv_rdd.collect():\n",
    "    print(f\"  {item[0]}: {item[1]}\")\n",
    "\n",
    "# Check if it's a pair RDD\n",
    "print(f\"\\nIs Pair RDD: {kv_rdd.map(lambda x: isinstance(x, tuple) and len(x) == 2).reduce(lambda a, b: a and b)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”„ Transformations vs Actions\n",
    "\n",
    "This is the most important concept in Spark!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations (Lazy Operations)\n",
    "\n",
    "**Transformations create new RDDs from existing ones.**\n",
    "\n",
    "They are **lazy** - they don't execute immediately. Spark builds a DAG (Directed Acyclic Graph) of transformations to optimize execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a base RDD\n",
    "numbers = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "\n",
    "# Transformations (lazy - create execution plan)\n",
    "doubled = numbers.map(lambda x: x * 2)        # Double each number\n",
    "filtered = doubled.filter(lambda x: x > 5)    # Keep numbers > 5\n",
    "squared = filtered.map(lambda x: x * x)       # Square the results\n",
    "\n",
    "print(\"âœ… Transformations defined (but not executed yet)\")\n",
    "print(f\"Original RDD: {numbers.collect()}\")\n",
    "print(\"Transformations created a DAG, but no computation happened yet!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions (Eager Operations)\n",
    "\n",
    "**Actions trigger actual computation and return results.**\n",
    "\n",
    "They execute the DAG of transformations and bring results back to the driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actions (eager - trigger computation)\n",
    "print(\"ğŸ¯ Executing transformations with actions:\")\n",
    "\n",
    "# Action 1: collect() - brings all data to driver\n",
    "result = squared.collect()\n",
    "print(f\"Final result: {result}\")\n",
    "\n",
    "# Action 2: count() - counts elements\n",
    "total_count = squared.count()\n",
    "print(f\"Total elements: {total_count}\")\n",
    "\n",
    "# Action 3: first() - gets first element\n",
    "first_element = squared.first()\n",
    "print(f\"First element: {first_element}\")\n",
    "\n",
    "# Action 4: take() - gets first N elements\n",
    "first_five = squared.take(5)\n",
    "print(f\"First 5 elements: {first_five}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ”„ The Lazy Evaluation Process\n",
    "\n",
    "```\n",
    "Transformations (Lazy) â†’ Build DAG â†’ Action (Eager) â†’ Execute DAG â†’ Return Results\n",
    "```\n",
    "\n",
    "**Why lazy evaluation matters:**\n",
    "- Allows Spark to optimize the execution plan\n",
    "- Reduces unnecessary computations\n",
    "- Enables fault tolerance\n",
    "- Supports distributed processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ› ï¸ Common RDD Operations\n",
    "\n",
    "Let's explore some essential RDD operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data for demonstrations\n",
    "sample_data = [\"hello world\", \"welcome to pyspark\", \"rdd operations\", \"distributed computing\"]\n",
    "sample_rdd = sc.parallelize(sample_data)\n",
    "\n",
    "print(\"ğŸ“Š Sample RDD Operations:\")\n",
    "print(f\"Original: {sample_rdd.collect()}\")\n",
    "\n",
    "# map() - Transform each element\n",
    "upper_rdd = sample_rdd.map(lambda x: x.upper())\n",
    "print(f\"\\nmap() - Uppercase: {upper_rdd.collect()}\")\n",
    "\n",
    "# flatMap() - Transform and flatten\n",
    "words_rdd = sample_rdd.flatMap(lambda x: x.split())\n",
    "print(f\"\\nflatMap() - Split into words: {words_rdd.collect()}\")\n",
    "\n",
    "# filter() - Keep elements matching condition\n",
    "long_words = words_rdd.filter(lambda x: len(x) > 4)\n",
    "print(f\"\\nfilter() - Words longer than 4 chars: {long_words.collect()}\")\n",
    "\n",
    "# distinct() - Remove duplicates\n",
    "distinct_words = words_rdd.distinct()\n",
    "print(f\"\\ndistinct() - Unique words: {distinct_words.collect()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§® Numeric RDD Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create numeric RDD\n",
    "nums = sc.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "\n",
    "print(\"ğŸ”¢ Numeric RDD Operations:\")\n",
    "print(f\"Numbers: {nums.collect()}\")\n",
    "\n",
    "# Basic aggregations\n",
    "print(f\"Sum: {nums.sum()}\")\n",
    "print(f\"Mean: {nums.mean():.2f}\")\n",
    "print(f\"Max: {nums.max()}\")\n",
    "print(f\"Min: {nums.min()}\")\n",
    "\n",
    "# Custom reduction\n",
    "product = nums.reduce(lambda x, y: x * y)\n",
    "print(f\"Product: {product}\")\n",
    "\n",
    "# Fold (with initial value)\n",
    "sum_with_start = nums.fold(100, lambda x, y: x + y)\n",
    "print(f\"Sum with initial 100: {sum_with_start}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¾ RDD Persistence\n",
    "\n",
    "RDDs can be cached in memory for faster access:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a large RDD for caching demonstration\n",
    "large_data = list(range(1, 1001))  # 1 to 1000\n",
    "large_rdd = sc.parallelize(large_data)\n",
    "\n",
    "# Apply some transformations\n",
    "processed_rdd = large_rdd \\\n",
    "    .map(lambda x: x * 2) \\\n",
    "    .filter(lambda x: x % 3 == 0) \\\n",
    "    .map(lambda x: x + 10)\n",
    "\n",
    "print(\"ğŸ“ˆ Large RDD processing:\")\n",
    "\n",
    "# First execution (no cache)\n",
    "import time\n",
    "start_time = time.time()\n",
    "count1 = processed_rdd.count()\n",
    "time1 = time.time() - start_time\n",
    "\n",
    "print(f\"First execution: {count1} elements in {time1:.3f} seconds\")\n",
    "\n",
    "# Cache the RDD\n",
    "processed_rdd.cache()\n",
    "print(\"\\nğŸ’¾ RDD cached in memory\")\n",
    "\n",
    "# Second execution (with cache)\n",
    "start_time = time.time()\n",
    "count2 = processed_rdd.count()\n",
    "time2 = time.time() - start_time\n",
    "\n",
    "print(f\"Second execution: {count2} elements in {time2:.3f} seconds\")\n",
    "print(f\"Speedup: {time1/time2:.2f}x faster!\")\n",
    "\n",
    "# Check storage level\n",
    "print(f\"Storage level: {processed_rdd.getStorageLevel()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ›¡ï¸ Fault Tolerance\n",
    "\n",
    "RDDs automatically recover from failures through lineage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD Lineage (execution plan)\n",
    "test_rdd = sc.parallelize([1, 2, 3, 4, 5]) \\\n",
    "    .map(lambda x: x * 2) \\\n",
    "    .filter(lambda x: x > 4) \\\n",
    "    .map(lambda x: x + 10)\n",
    "\n",
    "print(\"ğŸ›¡ï¸ RDD Fault Tolerance:\")\n",
    "print(\"Lineage (execution plan):\")\n",
    "for line in test_rdd.toDebugString().split('\\n'):\n",
    "    print(f\"  {line}\")\n",
    "\n",
    "print(f\"\\nResult: {test_rdd.collect()}\")\n",
    "print(\"\\nIf any partition fails, Spark can recreate it using this lineage!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§¹ Cleaning Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up sample files\n",
    "import os\n",
    "if os.path.exists(\"sample_text.txt\"):\n",
    "    os.remove(\"sample_text.txt\")\n",
    "\n",
    "print(\"ğŸ§¹ Cleanup completed\")\n",
    "\n",
    "# Note: Spark Session will be cleaned up automatically in Jupyter\n",
    "# In production code, use: spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Key Takeaways\n",
    "\n",
    "### What You Learned:\n",
    "- âœ… **RDDs** are Spark's fundamental distributed data structure\n",
    "- âœ… **Transformations** are lazy operations that build execution plans\n",
    "- âœ… **Actions** trigger actual computation and return results\n",
    "- âœ… **Caching** improves performance for reused RDDs\n",
    "- âœ… **Lineage** provides fault tolerance\n",
    "\n",
    "### RDD Characteristics:\n",
    "- ğŸ”¸ **Immutable**: Cannot be changed after creation\n",
    "- ğŸ”¸ **Distributed**: Partitioned across cluster nodes\n",
    "- ğŸ”¸ **Fault-tolerant**: Automatically recovered via lineage\n",
    "- ğŸ”¸ **Lazy evaluation**: Transformations don't execute immediately\n",
    "\n",
    "### Common Operations:\n",
    "- ğŸ”¸ `map()` - Transform each element\n",
    "- ğŸ”¸ `filter()` - Keep elements matching condition\n",
    "- ğŸ”¸ `flatMap()` - Transform and flatten results\n",
    "- ğŸ”¸ `collect()` - Bring results to driver\n",
    "- ğŸ”¸ `count()` - Count elements\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ Next Steps\n",
    "\n",
    "Now that you understand RDDs, you're ready for:\n",
    "\n",
    "1. **DataFrame Introduction** - Higher-level structured data processing\n",
    "2. **Transformations vs Actions Deep Dive** - Understanding lazy evaluation\n",
    "3. **RDD Mastery** - Advanced RDD operations and performance\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ‰ Congratulations! You now understand the foundation of Spark - Resilient Distributed Datasets!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
