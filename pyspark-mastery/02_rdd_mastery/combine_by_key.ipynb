{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83d\udd11 combineByKey Algorithm\n\n",
    "Advanced aggregation operations in PySpark RDDs using `combineByKey()`.\n\n",
    "## \ud83c\udfaf Overview\n\n",
    "`combineByKey()` is a powerful transformation for custom aggregations on key-value RDDs. It allows you to:\n\n",
    "- \u2705 **Create initial accumulator values** for each key\n",
    "- \u2705 **Merge values** into existing accumulators\n",
    "- \u2705 **Combine accumulators** from different partitions\n\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \u2699\ufe0f PySpark Setup\n\n",
    "Initialize Spark for the combineByKey operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"combineByKey_Demo\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(\"Ready for combineByKey operations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcca Basic Example: Word Count\n\n",
    "Classic word count implementation using `combineByKey()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample word count data\n",
    "word_data = [\n",
    "    (\"spark\", 1),\n",
    "    (\"hadoop\", 1),\n",
    "    (\"spark\", 1),\n",
    "    (\"kafka\", 1),\n",
    "    (\"spark\", 1),\n",
    "    (\"hadoop\", 1)\n",
    "]\n",
    "\n",
    "# Create RDD\n",
    "word_rdd = sc.parallelize(word_data)\n",
    "\n",
    "print(\"Input data:\")\n",
    "for word, count in word_data:\n",
    "    print(f\"  {word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combineByKey for word count\n",
    "word_count_result = word_rdd.combineByKey(\n",
    "    lambda v: v,                    # createCombiner: initial value\n",
    "    lambda acc, v: acc + v,         # mergeValue: add to accumulator\n",
    "    lambda acc1, acc2: acc1 + acc2  # mergeCombiners: combine accumulators\n",
    ")\n",
    "\n",
    "print(\"\\nWord Count Results using combineByKey():\")\n",
    "for word, count in sorted(word_count_result.collect()):\n",
    "    print(f\"  {word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcc8 Advanced Example: Statistics per Key\n\n",
    "Calculate min, max, and count for each key using a single `combineByKey()` operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for statistical calculations\n",
    "stats_data = [\n",
    "    (\"A\", 10), (\"A\", 20), (\"A\", 15),\n",
    "    (\"B\", 5), (\"B\", 25), (\"B\", 30),\n",
    "    (\"C\", 8), (\"C\", 12)\n",
    "]\n",
    "\n",
    "stats_rdd = sc.parallelize(stats_data)\n",
    "\n",
    "print(\"Input data for statistics:\")\n",
    "for key, value in stats_data:\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate min, max, count for each key using combineByKey\n",
    "stats_result = stats_rdd.combineByKey(\n",
    "    lambda v: (v, v, 1),                    # createCombiner: (min, max, count)\n",
    "    lambda acc, v: (min(acc[0], v), max(acc[1], v), acc[2] + 1),  # mergeValue\n",
    "    lambda acc1, acc2: (min(acc1[0], acc2[0]), max(acc1[1], acc2[1]), acc1[2] + acc2[2])  # mergeCombiners\n",
    ")\n",
    "\n",
    "print(\"\\nStatistics per Key:\")\n",
    "for key, (min_val, max_val, count) in sorted(stats_result.collect()):\n",
    "    print(f\"  {key}: min={min_val}, max={max_val}, count={count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \u26a1 Performance Comparison\n\n",
    "Why `combineByKey()` is more efficient than `groupByKey()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Large dataset for performance comparison\n",
    "large_data = [(f\"key_{i % 100}\", i) for i in range(10000)]\n",
    "large_rdd = sc.parallelize(large_data)\n",
    "\n",
    "print(f\"Dataset size: {large_rdd.count()} elements\")\n",
    "print(f\"Unique keys: {large_rdd.map(lambda x: x[0]).distinct().count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Method 1: Using groupByKey (less efficient)\n",
    "print(\"=== Method 1: groupByKey + mapValues ===\")\n",
    "start_time = time.time()\n",
    "\n",
    "groupby_result = large_rdd.groupByKey() \\\n",
    "    .mapValues(lambda values: sum(values)) \\\n",
    "    .collect()\n",
    "\n",
    "groupby_time = time.time() - start_time\n",
    "print(f\"groupByKey time: {groupby_time:.3f} seconds\")\n",
    "print(f\"Results count: {len(groupby_result)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Using combineByKey (more efficient)\n",
    "print(\"\\n=== Method 2: combineByKey ===\")\n",
    "start_time = time.time()\n",
    "\n",
    "combinebykey_result = large_rdd.combineByKey(\n",
    "    lambda v: v,                    # createCombiner\n",
    "    lambda acc, v: acc + v,         # mergeValue\n",
    "    lambda acc1, acc2: acc1 + acc2  # mergeCombiners\n",
    ").collect()\n",
    "\n",
    "combinebykey_time = time.time() - start_time\n",
    "print(f\"combineByKey time: {combinebykey_time:.3f} seconds\")\n",
    "print(f\"Results count: {len(combinebykey_result)}\")\n",
    "\n",
    "print(f\"\\nPerformance improvement: {groupby_time/combinebykey_time:.2f}x faster\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83c\udfaf Interview Questions & Key Takeaways\n\n",
    "### Common Interview Questions:\n",
    "1. **What is the difference between `reduceByKey()` and `combineByKey()`?**\n",
    "2. **When would you use `combineByKey()` instead of `groupByKey()`?**\n",
    "3. **How does `combineByKey()` handle combiner functions?**\n\n",
    "### Key Takeaways:\n",
    "- \u2705 `combineByKey()` provides fine-grained control over aggregation\n",
    "- \u2705 **More efficient** than `groupByKey()` for most use cases\n",
    "- \u2705 **Essential** for complex custom aggregations\n",
    "- \u2705 **Supports incremental updates** and memory efficiency\n",
    "- \u2705 **Critical** for production distributed computing\n\n",
    "---\n\n",
    "**\ud83d\ude80 Ready to master advanced PySpark aggregations? `combineByKey()` is your gateway to efficient big data processing!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}