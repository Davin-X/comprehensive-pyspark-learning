{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”‘ groupByKey vs reduceByKey: Key-Value RDD Operations\n",
    "\n",
    "**Time to complete:** 35 minutes  \n",
    "**Difficulty:** Intermediate  \n",
    "**Prerequisites:** RDD basics, key-value operations\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will master:\n",
    "- âœ… **`groupByKey()`** - Group values by key (wide transformation)\n",
    "- âœ… **`reduceByKey()`** - Aggregate values by key efficiently\n",
    "- âœ… **Performance differences** - Why reduceByKey is usually better\n",
    "- âœ… **When to use each** - Choosing the right operation\n",
    "- âœ… **Advanced patterns** - Complex aggregations\n",
    "\n",
    "**This is one of the most important concepts for interview preparation!**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ” Understanding Key-Value RDDs\n",
    "\n",
    "**Key-Value RDDs** are the foundation of distributed aggregations. Each element is a tuple `(key, value)`.\n",
    "\n",
    "### Creating Key-Value RDDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"GroupByKey_vs_ReduceByKey\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "print(f\"âœ… Spark ready - Version: {spark.version}\")\n",
    "\n",
    "# Create key-value RDD\n",
    "data = [\n",
    "    (\"Alice\", 100),\n",
    "    (\"Bob\", 200),\n",
    "    (\"Alice\", 150),\n",
    "    (\"Charlie\", 300),\n",
    "    (\"Bob\", 50),\n",
    "    (\"Alice\", 75)\n",
    "]\n",
    "\n",
    "kv_rdd = sc.parallelize(data)\n",
    "print(f\"Key-Value RDD: {kv_rdd.collect()}\")\n",
    "print(f\"Is Pair RDD: {kv_rdd.map(lambda x: isinstance(x, tuple) and len(x) == 2).reduce(lambda a, b: a and b)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ—‚ï¸ groupByKey() Operation\n",
    "\n",
    "**`groupByKey()` groups all values for each key into an iterable.**\n",
    "\n",
    "### Characteristics:\n",
    "- **Wide transformation** - Requires data shuffling across network\n",
    "- **Returns**: `(key, [value1, value2, ...])` for each key\n",
    "- **Memory intensive** - All values for a key must fit in memory\n",
    "- **Lazy evaluation** - Doesn't execute until action called\n",
    "\n",
    "### When to Use:\n",
    "- When you need all values for a key (not just aggregated result)\n",
    "- For complex operations that can't be done with simple aggregation\n",
    "- When working with small datasets or few distinct keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupByKey example\n",
    "grouped = kv_rdd.groupByKey()\n",
    "\n",
    "print(\"ðŸ“Š GROUPBYKEY OPERATION\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Original data:\")\n",
    "for item in data:\n",
    "    print(f\"  {item}\")\n",
    "\n",
    "print(\"\\ngroupByKey() result:\")\n",
    "for key, values_iterable in grouped.collect():\n",
    "    values_list = list(values_iterable)\n",
    "    print(f\"  {key}: {values_list}\")\n",
    "    \n",
    "# Manual aggregation after groupByKey\n",
    "print(\"\\nManual aggregation after groupByKey:\")\n",
    "for key, values_iterable in grouped.collect():\n",
    "    values_list = list(values_iterable)\n",
    "    total = sum(values_list)\n",
    "    count = len(values_list)\n",
    "    avg = total / count\n",
    "    print(f\"  {key}: sum={total}, count={count}, avg={avg:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš¡ reduceByKey() Operation\n",
    "\n",
    "**`reduceByKey(func)` aggregates values for each key using a reduction function.**\n",
    "\n",
    "### Characteristics:\n",
    "- **Wide transformation** - Also requires shuffling, but optimized\n",
    "- **Returns**: `(key, aggregated_value)` for each key\n",
    "- **Memory efficient** - Combines values locally before shuffling\n",
    "- **Lazy evaluation** - Doesn't execute until action called\n",
    "\n",
    "### When to Use:\n",
    "- For simple aggregations (sum, count, min, max, etc.)\n",
    "- When you only need the aggregated result (not individual values)\n",
    "- For most aggregation use cases (preferred over groupByKey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduceByKey example\n",
    "sum_by_key = kv_rdd.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "print(\"âš¡ REDUCEBYKEY OPERATION\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Original data:\")\n",
    "for item in data:\n",
    "    print(f\"  {item}\")\n",
    "\n",
    "print(\"\\nreduceByKey(lambda a, b: a + b) result:\")\n",
    "for key, total in sum_by_key.collect():\n",
    "    print(f\"  {key}: {total}\")\n",
    "\n",
    "# Multiple aggregations with reduceByKey\n",
    "count_by_key = kv_rdd.mapValues(lambda v: 1).reduceByKey(lambda a, b: a + b)\n",
    "max_by_key = kv_rdd.reduceByKey(lambda a, b: max(a, b))\n",
    "min_by_key = kv_rdd.reduceByKey(lambda a, b: min(a, b))\n",
    "\n",
    "print(\"\\nAdvanced reduceByKey operations:\")\n",
    "print(f\"Count: {dict(count_by_key.collect())}\")\n",
    "print(f\"Max: {dict(max_by_key.collect())}\")\n",
    "print(f\"Min: {dict(min_by_key.collect())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ Performance Comparison\n",
    "\n",
    "**Why reduceByKey is usually much faster than groupByKey!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create larger dataset for meaningful comparison\n",
    "large_data = []\n",
    "for i in range(10000):\n",
    "    key = f\"key_{i % 100}\"  # 100 distinct keys\n",
    "    value = i % 1000        # Values 0-999\n",
    "    large_data.append((key, value))\n",
    "\n",
    "large_rdd = sc.parallelize(large_data)\n",
    "print(f\"Large dataset: {len(large_data):,} elements, {len(set(k for k,v in large_data))} distinct keys\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: groupByKey + manual aggregation\n",
    "print(\"=== Method 1: groupByKey + sum() ===\")\n",
    "start_time = time.time()\n",
    "\n",
    "groupby_result = large_rdd.groupByKey().mapValues(lambda values: sum(values)).collect()\n",
    "groupby_time = time.time() - start_time\n",
    "\n",
    "print(f\"groupByKey time: {groupby_time:.3f} seconds\")\n",
    "print(f\"Results count: {len(groupby_result)}\")\n",
    "\n",
    "# Show first few results\n",
    "print(\"Sample results:\")\n",
    "for key, total in sorted(groupby_result)[:5]:\n",
    "    print(f\"  {key}: {total}\")\n",
    "print(f\"  ... and {len(groupby_result)-5} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: reduceByKey (efficient)\n",
    "print(\"\\n=== Method 2: reduceByKey ===\")\n",
    "start_time = time.time()\n",
    "\n",
    "reducebykey_result = large_rdd.reduceByKey(lambda a, b: a + b).collect()\n",
    "reducebykey_time = time.time() - start_time\n",
    "\n",
    "print(f\"reduceByKey time: {reducebykey_time:.3f} seconds\")\n",
    "print(f\"Results count: {len(reducebykey_result)}\")\n",
    "\n",
    "# Show first few results\n",
    "print(\"Sample results:\")\n",
    "for key, total in sorted(reducebykey_result)[:5]:\n",
    "    print(f\"  {key}: {total}\")\n",
    "print(f\"  ... and {len(reducebykey_result)-5} more\")\n",
    "\n",
    "# Performance comparison\n",
    "print(f\"\\nðŸŽ¯ PERFORMANCE COMPARISON:\")\n",
    "print(f\"groupByKey: {groupby_time:.3f}s\")\n",
    "print(f\"reduceByKey: {reducebykey_time:.3f}s\")\n",
    "if groupby_time > 0:\n",
    "    speedup = groupby_time / reducebykey_time\n",
    "    print(f\"reduceByKey is {speedup:.1f}x faster!\")\n",
    "else:\n",
    "    print(\"reduceByKey is significantly faster!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ” Why reduceByKey is Faster\n",
    "\n",
    "**The key difference: Local aggregation before shuffling!**\n",
    "\n",
    "```\n",
    "groupByKey():\n",
    "1. Shuffle ALL values for each key across network\n",
    "2. Group them in memory on target executors\n",
    "3. Apply aggregation function\n",
    "âŒ High network traffic, high memory usage\n",
    "\n",
    "reduceByKey():\n",
    "1. Pre-aggregate values locally on each executor\n",
    "2. Shuffle only aggregated results\n",
    "3. Final aggregation on target executors\n",
    "âœ… Minimal network traffic, efficient memory usage\n",
    "```\n",
    "\n",
    "**Network traffic reduction can be 10x-100x for large datasets!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ When to Use Each Operation\n",
    "\n",
    "### Use `groupByKey()` when:\n",
    "- âœ… You need **all individual values** for a key (not just aggregated result)\n",
    "- âœ… You need **complex operations** that can't be done with simple reduction\n",
    "- âœ… You have **few distinct keys** and small datasets\n",
    "- âœ… You're doing **multiple different aggregations** on the same grouped data\n",
    "\n",
    "### Use `reduceByKey()` when:\n",
    "- âœ… You need **simple aggregations** (sum, count, min, max, etc.)\n",
    "- âœ… You only need the **aggregated result** (not individual values)\n",
    "- âœ… You have **many distinct keys** or large datasets\n",
    "- âœ… **Performance matters** (which it usually does!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples of when to use each\n",
    "\n",
    "print(\"ðŸŽ¯ WHEN TO USE EACH OPERATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Example data: Student grades\n",
    "grades_data = [\n",
    "    (\"Math\", 85), (\"Math\", 92), (\"Math\", 78),\n",
    "    (\"Science\", 88), (\"Science\", 95),\n",
    "    (\"English\", 82), (\"English\", 79), (\"English\", 87)\n",
    "]\n",
    "\n",
    "grades_rdd = sc.parallelize(grades_data)\n",
    "\n",
    "# Use reduceByKey for simple aggregation\n",
    "print(\"reduceByKey - Simple aggregations:\")\n",
    "avg_grades = grades_rdd.mapValues(lambda grade: (grade, 1)) \\\n",
    "    .reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1])) \\\n",
    "    .mapValues(lambda total_count: total_count[0] / total_count[1])\n",
    "\n",
    "for subject, avg in avg_grades.collect():\n",
    "    print(f\"  {subject}: {avg:.1f}\")\n",
    "\n",
    "# Use groupByKey when you need all values\n",
    "print(\"\\ngroupByKey - Access to all values:\")\n",
    "grouped_grades = grades_rdd.groupByKey()\n",
    "\n",
    "for subject, grade_iterable in grouped_grades.collect():\n",
    "    grades = list(grade_iterable)\n",
    "    print(f\"  {subject}: {grades} (max: {max(grades)}, min: {min(grades)})\")\n",
    "\n",
    "# Use groupByKey for complex operations\n",
    "print(\"\\ngroupByKey - Complex analysis:\")\n",
    "complex_analysis = grades_rdd.groupByKey().mapValues(\n",
    "    lambda grades: {\n",
    "        'count': len(grades),\n",
    "        'sum': sum(grades),\n",
    "        'avg': sum(grades) / len(grades),\n",
    "        'grades_above_85': len([g for g in grades if g >= 85])\n",
    "    }\n",
    ")\n",
    "\n",
    "for subject, analysis in complex_analysis.collect():\n",
    "    print(f\"  {subject}: {analysis}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Interview Questions & Key Takeaways\n",
    "\n",
    "### Common Interview Questions:\n",
    "1. **What's the difference between groupByKey and reduceByKey?**\n",
    "2. **Why is reduceByKey more efficient?**\n",
    "3. **When would you use groupByKey instead of reduceByKey?**\n",
    "4. **How does data shuffling work in these operations?**\n",
    "5. **What's the performance impact of choosing the wrong operation?**\n",
    "\n",
    "### Answers:\n",
    "- **groupByKey**: Groups all values for each key, returns iterators, requires shuffling all data\n",
    "- **reduceByKey**: Aggregates values locally before shuffling, much more efficient\n",
    "- **Use groupByKey**: When you need all individual values or complex operations\n",
    "- **Shuffling**: Network data movement required for wide transformations\n",
    "- **Performance**: reduceByKey can be 10-100x faster for large datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§¹ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Spark Session will be cleaned up automatically in Jupyter\n",
    "# In production code, use: spark.stop()\n",
    "\n",
    "print(\"ðŸ§¹ Session cleanup complete\")\n",
    "\n",
    "print(\"\\nðŸ“š KEY TAKEAWAYS:\")\n",
    "print(\"- groupByKey: Use when you need all values for complex operations\")\n",
    "print(\"- reduceByKey: Use for efficient aggregations (preferred)\")\n",
    "print(\"- Performance: reduceByKey can be dramatically faster\")\n",
    "print(\"- Memory: groupByKey requires more memory for large datasets\")\n",
    "print(\"- Interviews: This topic comes up frequently!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
