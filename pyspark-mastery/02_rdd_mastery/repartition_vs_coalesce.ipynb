{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîÄ repartition vs coalesce: RDD Data Partitioning\n",
    "\n",
    "**Time to complete:** 25 minutes  \n",
    "**Difficulty:** Intermediate  \n",
    "**Prerequisites:** RDD basics, transformations\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand:\n",
    "- ‚úÖ **`repartition()`** - Increase/decrease partitions with shuffle\n",
    "- ‚úÖ **`coalesce()`** - Decrease partitions without shuffle (usually)\n",
    "- ‚úÖ **When to use each** - Performance implications\n",
    "- ‚úÖ **Partitioning best practices** - Optimizing data distribution\n",
    "- ‚úÖ **Monitoring partitions** - Checking and debugging\n",
    "\n",
    "**Partitioning is crucial for distributed computing performance!**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Understanding RDD Partitioning\n",
    "\n",
    "**Partitions** are how Spark divides data across the cluster. Each partition:\n",
    "- Contains a subset of the RDD data\n",
    "- Is processed by one executor core\n",
    "- Can be on different machines\n",
    "- Affects parallelism and performance\n",
    "\n",
    "**Why partitioning matters:**\n",
    "- Too few partitions = Underutilized cluster\n",
    "- Too many partitions = Excessive overhead\n",
    "- Wrong data distribution = Performance bottlenecks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Repartition_vs_Coalesce\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "print(f\"‚úÖ Spark ready - Default parallelism: {sc.defaultParallelism}\")\n",
    "print(f\"‚úÖ Available cores: {sc.defaultParallelism}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating RDDs with Different Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RDDs with different partition counts\n",
    "data = list(range(1, 21))  # 20 elements\n",
    "\n",
    "# Default partitioning (based on cluster size)\n",
    "rdd_default = sc.parallelize(data)\n",
    "print(f\"Default partitions: {rdd_default.getNumPartitions()}\")\n",
    "\n",
    "# Explicit partitioning\n",
    "rdd_2_parts = sc.parallelize(data, 2)\n",
    "rdd_4_parts = sc.parallelize(data, 4)\n",
    "rdd_8_parts = sc.parallelize(data, 8)\n",
    "\n",
    "print(f\"2 partitions: {rdd_2_parts.getNumPartitions()}\")\n",
    "print(f\"4 partitions: {rdd_4_parts.getNumPartitions()}\")\n",
    "print(f\"8 partitions: {rdd_8_parts.getNumPartitions()}\")\n",
    "\n",
    "# Show data distribution\n",
    "print(\"\\nData distribution in 4 partitions:\")\n",
    "for i, partition in enumerate(rdd_4_parts.glom().collect()):\n",
    "    print(f\"Partition {i}: {partition}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÄ repartition() Operation\n",
    "\n",
    "**`repartition(n)` changes the number of partitions with a full shuffle.**\n",
    "\n",
    "### Characteristics:\n",
    "- **Wide transformation** - Always causes shuffling\n",
    "- **Can increase or decrease** partition count\n",
    "- **Full data redistribution** across the cluster\n",
    "- **Expensive operation** - Network intensive\n",
    "- **Evenly distributes data** - Good for balancing\n",
    "\n",
    "### When to Use:\n",
    "- Increasing partition count\n",
    "- Needing even data distribution\n",
    "- Preparing for operations requiring more parallelism\n",
    "- When current partitioning is skewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repartition examples\n",
    "original_rdd = sc.parallelize(range(1, 21), 2)  # Start with 2 partitions\n",
    "print(f\"Original: {original_rdd.getNumPartitions()} partitions\")\n",
    "print(f\"Original distribution: {original_rdd.glom().collect()}\")\n",
    "\n",
    "# Increase partitions (always requires shuffle)\n",
    "repartitioned_up = original_rdd.repartition(6)\n",
    "print(f\"\\nAfter repartition(6): {repartitioned_up.getNumPartitions()} partitions\")\n",
    "print(f\"New distribution: {repartitioned_up.glom().collect()}\")\n",
    "\n",
    "# Decrease partitions (still requires shuffle)\n",
    "repartitioned_down = original_rdd.repartition(1)\n",
    "print(f\"\\nAfter repartition(1): {repartitioned_down.getNumPartitions()} partitions\")\n",
    "print(f\"New distribution: {repartitioned_down.glom().collect()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîó coalesce() Operation\n",
    "\n",
    "**`coalesce(n)` decreases partitions with minimal shuffling (usually).**\n",
    "\n",
    "### Characteristics:\n",
    "- **Can only decrease** partition count (n < current partitions)\n",
    "- **Narrow transformation** when decreasing significantly\n",
    "- **Minimal data movement** - avoids full shuffle when possible\n",
    "- **More efficient** than repartition for reducing partitions\n",
    "- **May not evenly distribute** data\n",
    "\n",
    "### When to Use:\n",
    "- Decreasing partition count\n",
    "- Reducing parallelism after filtering\n",
    "- Preparing for final output stages\n",
    "- When you don't need perfect data distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coalesce examples\n",
    "original_rdd = sc.parallelize(range(1, 21), 8)  # Start with 8 partitions\n",
    "print(f\"Original: {original_rdd.getNumPartitions()} partitions\")\n",
    "print(f\"Original distribution: {original_rdd.glom().collect()}\")\n",
    "\n",
    "# Decrease partitions with coalesce (efficient)\n",
    "coalesced = original_rdd.coalesce(3)\n",
    "print(f\"\\nAfter coalesce(3): {coalesced.getNumPartitions()} partitions\")\n",
    "print(f\"New distribution: {coalesced.glom().collect()}\")\n",
    "\n",
    "# Note: coalesce cannot increase partitions\n",
    "# coalesced_up = original_rdd.coalesce(12)  # This won't increase partitions\n",
    "print(f\"\\ncoalesce(12) on 8 partitions: still {original_rdd.coalesce(12).getNumPartitions()} partitions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö° Performance Comparison\n",
    "\n",
    "**Why coalesce is usually faster than repartition for reducing partitions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Create large dataset\n",
    "large_data = list(range(1, 100001))  # 100,000 elements\n",
    "large_rdd = sc.parallelize(large_data, 16)  # 16 partitions\n",
    "\n",
    "print(f\"Large dataset: {large_rdd.count():,} elements, {large_rdd.getNumPartitions()} partitions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: repartition (always shuffles)\n",
    "print(\"=== Method 1: repartition(4) ===\")\n",
    "start_time = time.time()\n",
    "repartitioned = large_rdd.repartition(4)\n",
    "repartition_time = time.time() - start_time\n",
    "\n",
    "print(f\"repartition(4) time: {repartition_time:.3f} seconds\")\n",
    "print(f\"Result partitions: {repartitioned.getNumPartitions()}\")\n",
    "print(f\"Data preserved: {repartitioned.count() == large_rdd.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: coalesce (minimizes shuffling)\n",
    "print(\"\\n=== Method 2: coalesce(4) ===\")\n",
    "start_time = time.time()\n",
    "coalesced = large_rdd.coalesce(4)\n",
    "coalesce_time = time.time() - start_time\n",
    "\n",
    "print(f\"coalesce(4) time: {coalesce_time:.3f} seconds\")\n",
    "print(f\"Result partitions: {coalesced.getNumPartitions()}\")\n",
    "print(f\"Data preserved: {coalesced.count() == large_rdd.count()}\")\n",
    "\n",
    "# Performance comparison\n",
    "print(f\"\\nüéØ PERFORMANCE COMPARISON:\")\n",
    "print(f\"repartition: {repartition_time:.3f}s\")\n",
    "print(f\"coalesce: {coalesce_time:.3f}s\")\n",
    "if repartition_time > 0:\n",
    "    speedup = repartition_time / coalesce_time\n",
    "    print(f\"coalesce is {speedup:.1f}x faster for reducing partitions!\")\n",
    "else:\n",
    "    print(\"coalesce is significantly faster!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Performance Differences:\n",
    "\n",
    "```\n",
    "repartition():\n",
    "‚îú‚îÄ‚îÄ Always does full shuffle\n",
    ‚îú‚îÄ‚îÄ Redistributes all data evenly\n",
    ‚îú‚îÄ‚îÄ Expensive for large datasets\n",
    ‚îî‚îÄ‚îÄ Good for increasing partitions\n",
    "\n",
    "coalesce():\n",
    "‚îú‚îÄ‚îÄ Minimizes data movement\n",
    "‚îú‚îÄ‚îÄ May leave some partitions empty\n",
    "‚îú‚îÄ‚îÄ Efficient for reducing partitions\n",
    "‚îî‚îÄ‚îÄ Cannot increase partition count\n",
    "```\n",
    "\n",
    "**Rule of thumb:** Use `coalesce()` to reduce partitions, `repartition()` to increase them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ When to Use Each Operation\n",
    "\n",
    "### Use `repartition()` when:\n",
    "- ‚úÖ **Increasing partition count**\n",
    "- ‚úÖ **Needing even data distribution**\n",
    "- ‚úÖ **Fixing data skew** across partitions\n",
    "- ‚úÖ **Preparing for joins** or operations needing more parallelism\n",
    "\n",
    "### Use `coalesce()` when:\n",
    "- ‚úÖ **Decreasing partition count**\n",
    "- ‚úÖ **Reducing parallelism** after filtering/aggregation\n",
    "- ‚úÖ **Optimizing for final output** stages\n",
    "- ‚úÖ **Minimizing network traffic**\n",
    "\n",
    "### Never Use:\n",
    "- ‚ùå `repartition()` to reduce partitions (use `coalesce()`)\n",
    "- ‚ùå `coalesce()` to increase partitions (use `repartition()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real-world usage patterns\n",
    "\n",
    "print(\"üéØ REAL-WORLD USAGE PATTERNS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Pattern 1: After filtering (reduce partitions)\n",
    "large_dataset = sc.parallelize(range(1, 10001), 20)\n",
    "filtered = large_dataset.filter(lambda x: x % 100 == 0)  # Keep only multiples of 100\n",
    "print(f\"After filtering: {filtered.count()} elements remain\")\n",
    "\n",
    "# Use coalesce to reduce partitions efficiently\n",
    "optimized = filtered.coalesce(4)\n",
    "print(f\"Optimized to {optimized.getNumPartitions()} partitions\")\n",
    "\n",
    "# Pattern 2: Before wide transformations (increase partitions)\n",
    "small_dataset = sc.parallelize(range(1, 101), 2)\n",
    "print(f\"\\nSmall dataset: {small_dataset.getNumPartitions()} partitions\")\n",
    "\n",
    "# Use repartition for even distribution before joins\n",
    "prepared_for_join = small_dataset.repartition(8)\n",
    "print(f\"Prepared for join: {prepared_for_join.getNumPartitions()} partitions\")\n",
    "\n",
    "# Pattern 3: Final output optimization\n",
    "processed_data = sc.parallelize(range(1, 1001), 16)\n",
    "final_output = processed_data.coalesce(1)  # Single file output\n",
    "print(f\"\\nFinal output: {final_output.getNumPartitions()} partition (single file)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Advanced Partitioning Concepts\n",
    "\n",
    "### Understanding Partition Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing partition distribution\n",
    "data = list(range(1, 21))  # 20 elements\n",
    "rdd = sc.parallelize(data, 4)  # 4 partitions\n",
    "\n",
    "print(\"üìä PARTITION ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total elements: {rdd.count()}\")\n",
    "print(f\"Partitions: {rdd.getNumPartitions()}\")\n",
    "\n",
    "# See data in each partition\n",
    "partitions = rdd.glom().collect()\n",
    "for i, partition in enumerate(partitions):\n",
    "    print(f\"Partition {i}: {len(partition)} elements - {partition}\")\n",
    "\n",
    "# Check partition sizes\n",
    "partition_sizes = rdd.mapPartitions(lambda partition: [sum(1 for _ in partition)]).collect()\n",
    "print(f\"\\nPartition sizes: {partition_sizes}\")\n",
    "print(f\"Min/Max partition sizes: {min(partition_sizes)} / {max(partition_sizes)}\")\n",
    "\n",
    "# Repartition for balance\n",
    "balanced = rdd.repartition(4)  # Even redistribution\n",
    "balanced_sizes = balanced.mapPartitions(lambda p: [sum(1 for _ in p)]).collect()\n",
    "print(f\"After repartition: {balanced_sizes}\")\n",
    "print(f\"Balanced min/max: {min(balanced_sizes)} / {max(balanced_sizes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Partitioning Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom partitioning example\n",
    "from pyspark.rdd import RDD\n",
    "\n",
    "# Create key-value data for custom partitioning\n",
    "kv_data = [\n",
    "    (\"A\", 1), (\"B\", 2), (\"C\", 3), (\"A\", 4), (\"B\", 5), (\"C\", 6),\n",
    "    (\"A\", 7), (\"B\", 8), (\"C\", 9), (\"A\", 10), (\"B\", 11), (\"C\", 12)\n",
    "]\n",
    "\n",
    "kv_rdd = sc.parallelize(kv_data)\n",
    "\n",
    "# Partition by key (grouping related data)\n",
    "partitioned_by_key = kv_rdd.partitionBy(3)  # 3 partitions\n",
    "\n",
    "print(\"üé® CUSTOM PARTITIONING\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Data partitioned by key:\")\n",
    "for i, partition in enumerate(partitioned_by_key.glom().collect()):\n",
    "    print(f\"Partition {i}: {partition}\")\n",
    "\n",
    "# Check which keys went to which partitions\n",
    "key_to_partition = kv_rdd.map(lambda x: (x[0], partitioned_by_key.getNumPartitions()))\n",
    "print(\"\\nKey distribution across partitions helps with:\")\n",
    "print(\"- Grouping related data together\")\n",
    "print(\"- Optimizing join operations\")\n",
    "print(\"- Reducing shuffle in subsequent operations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üö® Common Mistakes & Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common mistakes\n",
    "\n",
    "print(\"üö® COMMON MISTAKES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Mistake 1: Using coalesce to increase partitions\n",
    "rdd = sc.parallelize(range(1, 21), 2)\n",
    "print(f\"Original: {rdd.getNumPartitions()} partitions\")\n",
    "\n",
    "# This won't work!\n",
    "tried_to_increase = rdd.coalesce(8)\n",
    "print(f\"coalesce(8): {tried_to_increase.getNumPartitions()} partitions (no change!)\")\n",
    "\n",
    "# Correct way\n",
    "correct_increase = rdd.repartition(8)\n",
    "print(f\"repartition(8): {correct_increase.getNumPartitions()} partitions (works!)\")\n",
    "\n",
    "# Mistake 2: Too many partitions\n",
    "print(\"\\n‚ùå Too many partitions (overhead):\")\n",
    "small_data = sc.parallelize(range(1, 11), 100)  # 100 partitions for 10 elements!\n",
    "print(f\"10 elements, 100 partitions: {small_data.getNumPartitions()} partitions\")\n",
    "\n",
    "# Mistake 3: Ignoring partition distribution\n",
    "print(\"\\n‚ùå Ignoring data skew:\")\n",
    "skewed_data = [(\"A\", 1), (\"A\", 2), (\"A\", 3), (\"B\", 4), (\"C\", 5)]\n",
    "skewed_rdd = sc.parallelize(skewed_data, 3)\n",
    "print(\"Skewed data distribution:\")\n",
    "for i, p in enumerate(skewed_rdd.glom().collect()):\n",
    "    print(f\"Partition {i}: {p}\")\n",
    "\n",
    "# Fix with repartition\n",
    "balanced = skewed_rdd.repartition(3)\n",
    "print(\"\\nAfter repartition (more balanced):\")\n",
    "for i, p in enumerate(balanced.glom().collect()):\n",
    "    print(f\"Partition {i}: {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Interview Questions & Key Takeaways\n",
    "\n",
    "### Common Interview Questions:\n",
    "1. **What's the difference between repartition and coalesce?**\n",
    "2. **When would you use repartition vs coalesce?**\n",
    "3. **How does partitioning affect Spark performance?**\n",
    "4. **What happens if you have too few/many partitions?**\n",
    "\n",
    "### Answers:\n",
    "- **repartition**: Can increase/decrease partitions, always shuffles, evenly distributes data\n",
    "- **coalesce**: Can only decrease partitions, minimizes shuffling, may leave uneven distribution\n",
    "- **Use repartition**: When increasing partitions or needing even distribution\n",
    "- **Use coalesce**: When decreasing partitions efficiently\n",
    "- **Too few partitions**: Underutilizes cluster, slow processing\n",
    "- **Too many partitions**: Excessive overhead, task scheduling costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßπ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Spark Session will be cleaned up automatically in Jupyter\n",
    "# In production code, use: spark.stop()\n",
    "\n",
    "print(\"üßπ Session cleanup complete\")\n",
    "\n",
    "print(\"\\nüìö KEY TAKEAWAYS:\")\n",
    "print(\"- repartition: Increase partitions or need even distribution\")\n",
    "print(\"- coalesce: Decrease partitions efficiently\")\n",
    "print(\"- Right partitioning = optimal performance\")\n",
    "print(\"- Monitor partition sizes and distribution\")\n",
    "print(\"- Balance between parallelism and overhead\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
