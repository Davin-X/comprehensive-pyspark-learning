{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”„ Map, FlatMap & Filter: Core RDD Transformations\n",
    "\n",
    "**Time to complete:** 30 minutes  \n",
    "**Difficulty:** Intermediate  \n",
    "**Prerequisites:** RDD basics, transformations vs actions\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will master:\n",
    "- âœ… **map()** - Transform each element one-to-one\n",
    "- âœ… **flatMap()** - Transform and flatten results\n",
    "- âœ… **filter()** - Select elements based on conditions\n",
    "- âœ… **flatMap vs map** - When to use each\n",
    "- âœ… **Performance implications** - Choosing the right transformation\n",
    "- âœ… **Real-world use cases** - Practical applications\n",
    "\n",
    "**These are the most fundamental RDD operations you'll use daily!**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ” Understanding the Core Transformations\n",
    "\n",
    "`map()`, `flatMap()`, and `filter()` are the **bread and butter** of RDD operations. They form the foundation for most data processing pipelines.\n",
    "\n",
    "### Quick Comparison:\n",
    "\n",
    "| Operation | Input:Output | Use Case |\n",
    "|-----------|-------------|----------|\n",
    "| `map()` | 1 â†’ 1 | Transform each element |\n",
    "| `flatMap()` | 1 â†’ Many | Split/combine elements |\n",
    "| `filter()` | 1 â†’ 0 or 1 | Select elements |\n",
    "\n",
    "**All are narrow transformations** - no data shuffling required! âš¡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš™ï¸ Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MapFlatMapFilter\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "print(f\"âœ… Spark ready - Version: {spark.version}\")\n",
    "print(f\"âœ… Using {sc.defaultParallelism} partitions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ—ºï¸ Map Transformation\n",
    "\n",
    "**`map(func)` applies a function to each element and returns a new RDD with the results.**\n",
    "\n",
    "### Characteristics:\n",
    "- **One-to-one transformation**: Each input produces exactly one output\n",
    "- **Same number of elements**: Input count = Output count\n",
    "- **Type transformation**: Can change element types\n",
    "- **Lazy evaluation**: Doesn't execute until action called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data\n",
    "numbers = [1, 2, 3, 4, 5]\n",
    "numbers_rdd = sc.parallelize(numbers)\n",
    "\n",
    "print(\"ðŸ“Š MAP TRANSFORMATION EXAMPLES\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Original RDD: {numbers_rdd.collect()}\")\n",
    "\n",
    "# Example 1: Double each number\n",
    "doubled = numbers_rdd.map(lambda x: x * 2)\n",
    "print(f\"\\nDoubled (x * 2): {doubled.collect()}\")\n",
    "\n",
    "# Example 2: Square each number\n",
    "squared = numbers_rdd.map(lambda x: x ** 2)\n",
    "print(f\"Squared (xÂ²): {squared.collect()}\")\n",
    "\n",
    "# Example 3: Convert to string with formatting\n",
    "formatted = numbers_rdd.map(lambda x: f\"Number: {x}\")\n",
    "print(f\"Formatted strings: {formatted.collect()}\")\n",
    "\n",
    "# Example 4: Type conversion\n",
    "float_numbers = numbers_rdd.map(lambda x: float(x))\n",
    "print(f\"Converted to float: {float_numbers.collect()}\")\n",
    "print(f\"Type changed: {type(numbers_rdd.collect()[0])} â†’ {type(float_numbers.collect()[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Map Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with complex data\n",
    "employees = [\n",
    "    (\"Alice\", 25, 50000),\n",
    "    (\"Bob\", 30, 60000),\n",
    "    (\"Charlie\", 35, 70000)\n",
    "]\n",
    "\n",
    "employees_rdd = sc.parallelize(employees)\n",
    "print(f\"\\nEmployee data: {employees_rdd.collect()}\")\n",
    "\n",
    "# Extract just names\n",
    "names_only = employees_rdd.map(lambda emp: emp[0])\n",
    "print(f\"Names only: {names_only.collect()}\")\n",
    "\n",
    "# Calculate bonus (10% of salary)\n",
    "with_bonus = employees_rdd.map(lambda emp: (emp[0], emp[2], emp[2] * 0.1))\n",
    "print(f\"With bonus: {with_bonus.collect()}\")\n",
    "\n",
    "# Create employee summary\n",
    "summaries = employees_rdd.map(\n",
    "    lambda emp: f\"{emp[0]} is {emp[1]} years old and earns ${emp[2]:,}\"\n",
    ")\n",
    "print(f\"\\nEmployee summaries:\")\n",
    "for summary in summaries.collect():\n",
    "    print(f\"  {summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ—‚ï¸ FlatMap Transformation\n",
    "\n",
    "**`flatMap(func)` applies a function to each element and flattens the results.**\n",
    "\n",
    "### Characteristics:\n",
    "- **One-to-many transformation**: Each input can produce multiple outputs\n",
    "- **Flattening**: Nested structures become flat\n",
    "- **Variable output count**: Input count â‰¤ Output count\n",
    "- **Perfect for splitting**: Text processing, expanding collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text processing examples\n",
    "sentences = [\n",
    "    \"Hello world welcome to Spark\",\n",
    "    \"PySpark is powerful and fast\",\n",
    "    \"RDD transformations are fun\"\n",
    "]\n",
    "\n",
    "sentences_rdd = sc.parallelize(sentences)\n",
    "\n",
    "print(\"ðŸ“– FLATMAP TRANSFORMATION EXAMPLES\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Original sentences:\")\n",
    "for i, sentence in enumerate(sentences_rdd.collect(), 1):\n",
    "    print(f\"  {i}. '{sentence}'\")\n",
    "\n",
    "# Example 1: Split sentences into words\n",
    "words = sentences_rdd.flatMap(lambda sentence: sentence.split())\n",
    "print(f\"\\nAfter flatMap(split): {words.collect()}\")\n",
    "print(f\"Word count: {words.count()} (was {sentences_rdd.count()} sentences)\")\n",
    "\n",
    "# Example 2: Multiple words per input\n",
    "multi_words = sentences_rdd.flatMap(lambda s: [s.upper(), s.lower()])\n",
    "print(f\"\\nMultiple outputs per input: {multi_words.collect()}\")\n",
    "print(f\"Output count: {multi_words.count()} (2 Ã— input count)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FlatMap vs Map Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ” FLATMAP vs MAP COMPARISON\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Same input\n",
    "test_sentences = sc.parallelize([\"Hello world\", \"Spark is great\"])\n",
    "\n",
    "# Using map (splits but keeps nested structure)\n",
    "map_result = test_sentences.map(lambda s: s.split())\n",
    "print(f\"map(split): {map_result.collect()}\")\n",
    "print(f\"Type: {type(map_result.collect()[0])} - Nested lists\")\n",
    "\n",
    "# Using flatMap (splits and flattens)\n",
    "flatmap_result = test_sentences.flatMap(lambda s: s.split())\n",
    "print(f\"\\nflatMap(split): {flatmap_result.collect()}\")\n",
    "print(f\"Type: {type(flatmap_result.collect()[0])} - Flat list\")\n",
    "\n",
    "# Word count example\n",
    "text = sc.parallelize([\"Big data processing\", \"Spark RDD operations\"])\n",
    "word_count_map = text.map(lambda line: len(line.split()))\n",
    "word_count_flatmap = text.flatMap(lambda line: line.split()).count()\n",
    "\n",
    "print(f\"\\nmap() result: {word_count_map.collect()} (words per line)\")\n",
    "print(f\"flatMap() result: {word_count_flatmap} (total words)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced FlatMap Use Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expanding collections\n",
    "nested_data = [\n",
    "    [\"apple\", \"banana\", \"cherry\"],\n",
    "    [\"dog\", \"cat\"],\n",
    "    [\"red\", \"green\", \"blue\", \"yellow\"]\n",
    "]\n",
    "\n",
    "nested_rdd = sc.parallelize(nested_data)\n",
    "print(f\"\\nNested data: {nested_rdd.collect()}\")\n",
    "\n",
    "# Flatten the nested lists\n",
    "flattened = nested_rdd.flatMap(lambda x: x)\n",
    "print(f\"Flattened: {flattened.collect()}\")\n",
    "\n",
    "# Generate multiple related items\n",
    "products = sc.parallelize([\"laptop\", \"phone\"])\n",
    "product_features = products.flatMap(lambda p: [f\"{p}_price\", f\"{p}_rating\", f\"{p}_reviews\"])\n",
    "print(f\"\\nProduct features: {product_features.collect()}\")\n",
    "\n",
    "# Text analysis - n-grams\n",
    "sentence = sc.parallelize([\"The quick brown fox jumps\"])\n",
    "bigrams = sentence.flatMap(lambda s: [f\"{s.split()[i]} {s.split()[i+1]}\" \n",
    "                                       for i in range(len(s.split())-1)])\n",
    "print(f\"\\nBigrams: {bigrams.collect()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ” Filter Transformation\n",
    "\n",
    "**`filter(func)` keeps only elements that satisfy a condition.**\n",
    "\n",
    "### Characteristics:\n",
    "- **Conditional selection**: Elements pass through or are dropped\n",
    "- **Zero-to-one transformation**: Each input produces 0 or 1 output\n",
    "- **Reduced dataset**: Output count â‰¤ Input count\n",
    "- **Predicate function**: Returns True (keep) or False (drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric filtering\n",
    "all_numbers = sc.parallelize(range(1, 21))  # 1 to 20\n",
    "\n",
    "print(\"ðŸ” FILTER TRANSFORMATION EXAMPLES\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"All numbers: {all_numbers.collect()}\")\n",
    "\n",
    "# Example 1: Even numbers only\n",
    "evens = all_numbers.filter(lambda x: x % 2 == 0)\n",
    "print(f\"\\nEven numbers: {evens.collect()}\")\n",
    "\n",
    "# Example 2: Numbers greater than 10\n",
    "big_numbers = all_numbers.filter(lambda x: x > 10)\n",
    "print(f\"Numbers > 10: {big_numbers.collect()}\")\n",
    "\n",
    "# Example 3: Prime numbers (basic check)\n",
    "def is_prime(n):\n",
    "    if n < 2:\n",
    "        return False\n",
    "    for i in range(2, int(n**0.5) + 1):\n",
    "        if n % i == 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "primes = all_numbers.filter(is_prime)\n",
    "print(f\"Prime numbers: {primes.collect()}\")\n",
    "\n",
    "# Example 4: Complex conditions\n",
    "special = all_numbers.filter(lambda x: x % 3 == 0 and x > 5)\n",
    "print(f\"Multiples of 3 > 5: {special.collect()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String and Complex Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text filtering\n",
    "words_list = [\n",
    "    \"hello\", \"world\", \"spark\", \"data\", \"python\", \"big\",\n",
    "    \"small\", \"fast\", \"slow\", \"machine\", \"learning\", \"AI\"\n",
    "]\n",
    "\n",
    "words_rdd = sc.parallelize(words_list)\n",
    "print(f\"\\nAll words: {words_rdd.collect()}\")\n",
    "\n",
    "# Filter by length\n",
    "long_words = words_rdd.filter(lambda w: len(w) > 5)\n",
    "print(f\"Words > 5 chars: {long_words.collect()}\")\n",
    "\n",
    "# Filter by starting letter\n",
    "starts_with_s = words_rdd.filter(lambda w: w.startswith('s'))\n",
    "print(f\"Words starting with 's': {starts_with_s.collect()}\")\n",
    "\n",
    "# Complex object filtering\n",
    "products = [\n",
    "    {\"name\": \"Laptop\", \"price\": 1200, \"category\": \"Electronics\"},\n",
    "    {\"name\": \"Book\", \"price\": 25, \"category\": \"Education\"},\n",
    "    {\"name\": \"Chair\", \"price\": 150, \"category\": \"Furniture\"},\n",
    "    {\"name\": \"Phone\", \"price\": 800, \"category\": \"Electronics\"}\n",
    "]\n",
    "\n",
    "products_rdd = sc.parallelize(products)\n",
    "\n",
    "# Filter expensive electronics\n",
    "expensive_electronics = products_rdd.filter(\n",
    "    lambda p: p[\"category\"] == \"Electronics\" and p[\"price\"] > 500\n",
    ")\n",
    "\n",
    "print(f\"\\nExpensive electronics:\")\n",
    "for product in expensive_electronics.collect():\n",
    "    print(f\"  {product['name']}: ${product['price']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”— Chaining Transformations\n",
    "\n",
    "**Combine map, flatMap, and filter for powerful data processing pipelines.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text processing pipeline\n",
    "raw_text = [\n",
    "    \"Hello World! Welcome to PySpark.\",\n",
    "    \"This is a SAMPLE text with Numbers 123.\",\n",
    "    \"Spark RDDs are powerful tools!\"\n",
    "]\n",
    "\n",
    "text_rdd = sc.parallelize(raw_text)\n",
    "\n",
    "print(\"ðŸ”— TRANSFORMATION CHAINING EXAMPLES\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Raw text:\")\n",
    "for i, line in enumerate(text_rdd.collect(), 1):\n",
    "    print(f\"  {i}. '{line}'\")\n",
    "\n",
    "# Complete text processing pipeline\n",
    "processed_words = text_rdd \\\n",
    "    .flatMap(lambda line: line.split()) \\\n",
    "    .map(lambda word: word.lower().strip('.,!')) \\\n",
    "    .filter(lambda word: len(word) > 2) \\\n",
    "    .filter(lambda word: word.isalpha()) \\\n",
    "    .distinct()\n",
    "\n",
    "print(f\"\\nProcessed unique words: {sorted(processed_words.collect())}\")\n",
    "\n",
    "# Word frequency analysis\n",
    "word_freq = text_rdd \\\n",
    "    .flatMap(lambda line: line.split()) \\\n",
    "    .map(lambda word: word.lower().strip('.,!')) \\\n",
    "    .filter(lambda word: word.isalpha()) \\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .filter(lambda pair: pair[1] > 1)  # Only words that appear multiple times\n",
    "\n",
    "print(f\"\\nWords appearing multiple times:\")\n",
    "for word, count in sorted(word_freq.collect()):\n",
    "    print(f\"  {word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš¡ Performance Comparison\n",
    "\n",
    "**Understanding the performance characteristics of each transformation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Create large dataset for performance testing\n",
    "large_data = list(range(1, 100001))  # 100,000 numbers\n",
    "large_rdd = sc.parallelize(large_data)\n",
    "\n",
    "print(\"âš¡ PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Dataset size: {large_rdd.count():,} elements\")\n",
    "\n",
    "# Test map performance\n",
    "start = time.time()\n",
    "map_result = large_rdd.map(lambda x: x * 2).count()\n",
    "map_time = time.time() - start\n",
    "\n",
    "# Test filter performance\n",
    "start = time.time()\n",
    "filter_result = large_rdd.filter(lambda x: x % 1000 == 0).count()\n",
    "filter_time = time.time() - start\n",
    "\n",
    "# Test flatMap performance\n",
    "start = time.time()\n",
    "flatmap_result = large_rdd.flatMap(lambda x: [x, x*2]).count()\n",
    "flatmap_time = time.time() - start\n",
    "\n",
    "print(f\"\\nmap() time: {map_time:.3f}s (result: {map_result:,})\")\n",
    "print(f\"filter() time: {filter_time:.3f}s (result: {filter_result})\")\n",
    "print(f\"flatMap() time: {flatmap_time:.3f}s (result: {flatmap_result:,})\")\n",
    "\n",
    "print(\"\\nðŸ’¡ All are narrow transformations - no data shuffling!\")\n",
    "print(\"ðŸ’¡ Performance depends on function complexity, not operation type.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Best Practices & Common Patterns\n",
    "\n",
    "### When to Use Each Transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern examples\n",
    "\n",
    "# 1. MAP: One-to-one transformations\n",
    "data = sc.parallelize([1, 2, 3, 4, 5])\n",
    "squares = data.map(lambda x: x ** 2)  # Each input â†’ one output\n",
    "\n",
    "# 2. FLATMAP: Splitting or expanding\n",
    "sentences = sc.parallelize([\"hello world\", \"spark rules\"])\n",
    "words = sentences.flatMap(lambda s: s.split())  # One input â†’ multiple outputs\n",
    "\n",
    "# 3. FILTER: Selection based on conditions\n",
    "numbers = sc.parallelize(range(1, 21))\n",
    "evens = numbers.filter(lambda x: x % 2 == 0)  # Keep only even numbers\n",
    "\n",
    "# 4. Common pattern: Word count\n",
    "text = sc.parallelize([\"big data spark\", \"spark is fast\"])\n",
    "word_count = text \\\n",
    "    .flatMap(lambda line: line.split()) \\\n",
    "    .filter(lambda word: len(word) > 0) \\\n",
    "    .map(lambda word: (word.lower(), 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "print(\"ðŸŽ¯ COMMON PATTERNS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Squares: {squares.collect()}\")\n",
    "print(f\"Words: {words.collect()}\")\n",
    "print(f\"Evens: {evens.collect()}\")\n",
    "print(f\"Word count: {dict(word_count.collect())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš¨ Common Mistakes & Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mistake examples and fixes\n",
    "\n",
    "print(\"ðŸš¨ COMMON MISTAKES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Mistake 1: Using map when you need flatMap\n",
    "wrong = sc.parallelize([\"hello world\"]).map(lambda s: s.split())\n",
    "print(f\"Wrong - map(split): {wrong.collect()}  # Nested list\")\n",
    "\n",
    "correct = sc.parallelize([\"hello world\"]).flatMap(lambda s: s.split())\n",
    "print(f\"Correct - flatMap(split): {correct.collect()}  # Flat list\")\n",
    "\n",
    "# Mistake 2: Forgetting to handle edge cases in filter\n",
    "data_with_nulls = sc.parallelize([\"hello\", None, \"world\", \"\"])\n",
    "# wrong_filter = data_with_nulls.filter(lambda x: len(x) > 0)  # Error!\n",
    "\n",
    "safe_filter = data_with_nulls.filter(lambda x: x is not None and len(x) > 0)\n",
    "print(f\"\\nSafe filter result: {safe_filter.collect()}\")\n",
    "\n",
    "# Mistake 3: Complex operations in transformations\n",
    "numbers = sc.parallelize(range(1, 11))\n",
    "# Avoid this in production:\n",
    "complex_map = numbers.map(lambda x: sum(range(x)))  # Very slow!\n",
    "print(f\"\\nComplex operation result: {complex_map.collect()}\")\n",
    "print(\"ðŸ’¡ Move complex logic outside transformations when possible!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§¹ Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up sample files if they exist\n",
    "import os\n",
    "sample_files = [\"sample_text.txt\", \"sample_data.csv\"]\n",
    "for file in sample_files:\n",
    "    if os.path.exists(file):\n",
    "        os.remove(file)\n",
    "\n",
    "print(\"ðŸ§¹ Cleanup completed\")\n",
    "\n",
    "# Note: Spark Session will be cleaned up automatically in Jupyter\n",
    "# In production code, use: spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Key Takeaways\n",
    "\n",
    "### What You Learned:\n",
    "- âœ… **`map()`** - Transform each element one-to-one\n",
    "- âœ… **`flatMap()`** - Transform and flatten results (one-to-many)\n",
    "- âœ… **`filter()`** - Select elements based on conditions\n",
    "- âœ… **Narrow transformations** - No data shuffling required\n",
    "- âœ… **Chaining** - Combine for powerful data processing pipelines\n",
    "\n",
    "### Performance Characteristics:\n",
    "- ðŸ”¸ **All are fast** - No network communication\n",
    "- ðŸ”¸ **Parallel execution** - Operations run on multiple cores\n",
    "- ðŸ”¸ **Memory efficient** - Process elements as streams\n",
    "- ðŸ”¸ **Composable** - Build complex pipelines easily\n",
    "\n",
    "### Common Use Cases:\n",
    "- ðŸ”¸ **map()**: Data type conversion, simple transformations\n",
    "- ðŸ”¸ **flatMap()**: Text splitting, expanding collections\n",
    "- ðŸ”¸ **filter()**: Data cleaning, conditional selection\n",
    "\n",
    "### Best Practices:\n",
    "- ðŸ”¸ Use `flatMap()` when splitting data (text processing)\n",
    "- ðŸ”¸ Combine transformations to minimize passes through data\n",
    "- ðŸ”¸ Handle edge cases in filter conditions\n",
    "- ðŸ”¸ Keep transformation functions simple and testable\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸš€ Next Steps\n",
    "\n",
    "Now that you master basic transformations, you're ready for:\n",
    "\n",
    "1. **Key-Value Operations** - `groupByKey` vs `reduceByKey`\n",
    "2. **Advanced Aggregations** - `combineByKey` patterns\n",
    "3. **Partitioning** - `repartition` vs `coalesce`\n",
    "4. **Wide Transformations** - Understanding shuffle operations\n",
    "\n",
    "**These form the foundation of 90% of your RDD operations!**\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸŽ‰ Congratulations! You now command the core RDD transformations that power distributed data processing!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
