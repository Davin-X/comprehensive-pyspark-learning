{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîó Join Optimization: Advanced Techniques\n",
    "\n",
    "**Time to complete:** 40 minutes  \n",
    "**Difficulty:** Advanced  \n",
    "**Prerequisites:** Basic joins, partitioning, caching\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will master:\n",
    "- ‚úÖ **Join strategies** - Broadcast, shuffle hash, sort-merge\n",
    "- ‚úÖ **Join optimization techniques** - Salting, skew handling\n",
    "- ‚úÖ **Join reordering** - Catalyst optimizer join planning\n",
    "- ‚úÖ **Memory management** - Join memory optimization\n",
    "- ‚úÖ **Monitoring joins** - Performance analysis and tuning\n",
    "\n",
    "**Joins are often the most expensive operations - master them!**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Understanding Join Strategies\n",
    "\n",
    "**Spark supports multiple join algorithms optimized for different data sizes and distributions.**\n",
    "\n",
    "### Join Strategy Decision Tree:\n",
    "```\n",
    "Join Request\n",
    "    ‚îÇ\n",
    "    ‚îú‚îÄ‚îÄ Small table (< 10MB) ‚Üí Broadcast Join\n",
    "    ‚îÇ\n",
    "    ‚îú‚îÄ‚îÄ Large tables with sort ‚Üí Sort-Merge Join\n",
    "    ‚îÇ\n",
    "    ‚îú‚îÄ‚îÄ Large tables (hash) ‚Üí Shuffle Hash Join\n",
    "    ‚îÇ\n",
    "    ‚îî‚îÄ‚îÄ Fallback ‚Üí Cartesian/Nested Loop\n",
    "```\n",
    "\n",
    "**Right strategy = 10-100x performance difference!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, broadcast, count, sum as sum_func\n",
    "import pyspark.sql.functions as F\n",
    "import time\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Join_Optimization\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"‚úÖ Spark ready - Version: {spark.version}\")\n",
    "\n",
    "# Create test datasets of different sizes\n",
    "small_data = [\n",
    "    (\"US\", \"United States\", \"North America\"),\n",
    "    (\"UK\", \"United Kingdom\", \"Europe\"),\n",
    "    (\"JP\", \"Japan\", \"Asia\"),\n",
    "    (\"DE\", \"Germany\", \"Europe\"),\n",
    "    (\"FR\", \"France\", \"Europe\")\n",
    "]\n",
    "\n",
    "# Medium dimension table\n",
    "medium_data = [\n",
    "    (f\"prod_{i}\", f\"Category_{(i%5)+1}\", f\"Brand_{(i%3)+1}\", 10 + (i % 90))\n",
    "    for i in range(1, 101)\n",
    "]\n",
    "\n",
    "# Large fact table\n",
    "large_data = [\n",
    "    (i, f\"cust_{i%1000}\", f\"prod_{(i%100)+1}\", 1 + (i % 10), f\"2023-{(i%12)+1:02d}-01\")\n",
    "    for i in range(50000)\n",
    "]\n",
    "\n",
    "# Create DataFrames\n",
    "countries_df = spark.createDataFrame(small_data, [\"country_code\", \"country_name\", \"continent\"])\n",
    "products_df = spark.createDataFrame(medium_data, [\"product_id\", \"category\", \"brand\", \"price\"])\n",
    "sales_df = spark.createDataFrame(large_data, [\"sale_id\", \"customer_id\", \"product_id\", \"quantity\", \"sale_date\"])\n",
    "\n",
    "print(\"üìä Test Datasets:\")\n",
    "print(f\"Countries: {countries_df.count()} rows (small)\")\n",
    "print(f\"Products: {products_df.count()} rows (medium)\")\n",
    "print(f\"Sales: {sales_df.count():,} rows (large)\")\n",
    "\n",
    "# Check current broadcast threshold\n",
    "threshold = spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")\n",
    "print(f\"\\nBroadcast threshold: {threshold} bytes ({int(threshold)//(1024*1024)}MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì° Broadcast Join Deep Dive\n",
    "\n",
    "### When Broadcast Join is Optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcast join analysis\n",
    "print(\"üì° BROADCAST JOIN ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test different table sizes with broadcast\n",
    "test_cases = [\n",
    "    (\"Countries (small)\", countries_df),\n",
    "    (\"Products (medium)\", products_df),\n",
    "]\n",
    "\n",
    "for name, small_df in test_cases:\n",
    "    print(f\"\\n=== Testing {name} ===\")\n",
    "    \n",
    "    # Check if auto-broadcast\n",
    "    join_result = sales_df.join(small_df, \n",
    "        sales_df[\"customer_id\"].substr(1, 2) == small_df[\"country_code\"], \"inner\")\n",
    "    \n",
    "    print(f\"Auto-broadcast result: {join_result.count():,} rows\")\n",
    "    \n",
    "    # Force broadcast and compare\n",
    "    forced_broadcast = sales_df.join(\n",
    "        broadcast(small_df),\n",
    "        sales_df[\"customer_id\"].substr(1, 2) == small_df[\"country_code\"], \n",
    "        \"inner\"\n",
    "    )\n",
    "    \n",
    "    print(f\"Forced broadcast result: {forced_broadcast.count():,} rows\")\n",
    "    \n",
    "    # Check execution plans\n",
    "    print(\"\\nExecution plan:\")\n",
    "    forced_broadcast.explain(mode=\"simple\")\n",
    "    \n",
    "    # Performance comparison\n",
    "    start_time = time.time()\n",
    "    auto_count = join_result.count()\n",
    "    auto_time = time.time() - start_time\n",
    "    \n",
    "    start_time = time.time()\n",
    "    forced_count = forced_broadcast.count()\n",
    "    forced_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\nPerformance:\")\n",
    "    print(f\"Auto: {auto_time:.3f}s, Forced: {forced_time:.3f}s\")\n",
    "    if auto_time > 0 and forced_time > 0:\n",
    "        speedup = auto_time / forced_time\n",
    "        print(f\"Forced broadcast is {speedup:.1f}x {'faster' if speedup > 1 else 'slower'}\")\n",
    "\n",
    "# Optimal broadcast scenarios\n",
    "print(\"\\nüéØ OPTIMAL BROADCAST SCENARIOS:\")\n",
    "print(\"1. Small lookup tables (< 100MB)\")\n",
    "print(\"2. Dimension tables in star schema\")\n",
    "print(\"3. Reference data used in multiple joins\")\n",
    "print(\"4. Small tables joined with large fact tables\")\n",
    "print(\"5. When shuffle cost > broadcast cost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcast Join Limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcast limitations\n",
    "print(\"‚ö†Ô∏è BROADCAST JOIN LIMITATIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create large table that exceeds broadcast threshold\n",
    "large_dimension = [\n",
    "    (f\"dim_{i}\", f\"Category_{(i%10)+1}\", f\"Subcategory_{(i%5)+1}\", f\"Description_{i}\", i * 10)\n",
    "    for i in range(50000)\n",
    "]\n",
    "\n",
    "large_dim_df = spark.createDataFrame(large_dimension, \n",
    "    [\"dim_key\", \"category\", \"subcategory\", \"description\", \"value\"])\n",
    "\n",
    "print(f\"Large dimension table: {large_dim_df.count():,} rows\")\n",
    "\n",
    "# Try broadcast with large table\n",
    "print(\"\\nTesting broadcast with large table:\")\n",
    "\n",
    "# Temporarily increase threshold for testing\n",
    "original_threshold = spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\")\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"1GB\")  # Very high\n",
    "\n",
    "try:\n",
    "    large_broadcast = sales_df.join(\n",
    "        broadcast(large_dim_df),\n",
    "        sales_df[\"product_id\"] == large_dim_df[\"dim_key\"],\n",
    "        \"inner\"\n",
    "    )\n",
    "    print(f\"Large broadcast succeeded: {large_broadcast.count()} rows\")\n",
    "    large_broadcast.explain(mode=\"simple\")\n",
    "except Exception as e:\n",
    "    print(f\"Large broadcast failed: {str(e)[:100]}...\")\n",
    "\n",
    "# Reset threshold\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", original_threshold)\n",
    "\n",
    "# Compare with shuffle join\n",
    "print(\"\\nComparing with shuffle join:\")\n",
    "shuffle_join = sales_df.join(\n",
    "    large_dim_df,\n",
    "    sales_df[\"product_id\"] == large_dim_df[\"dim_key\"],\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "shuffle_join.explain(mode=\"simple\")\n",
    "\n",
    "print(\"\\nüìä BROADCAST VS SHUFFLE:\")\n",
    "print(\"Broadcast: No shuffle, but memory intensive\")\n",
    "print(\"Shuffle: Network traffic, but scales better\")\n",
    "print(\"Choose based on table sizes and cluster resources!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÄ Shuffle Hash Join vs Sort-Merge Join\n",
    "\n",
    "### Understanding Join Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join algorithm comparison\n",
    "print(\"üîÄ JOIN ALGORITHM COMPARISON\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create datasets for algorithm testing\n",
    "left_data = [\n",
    "    (f\"key_{i%100}\", f\"left_value_{i}\", i) \n",
    "    for i in range(10000)\n",
    "]\n",
    "\n",
    "right_data = [\n",
    "    (f\"key_{i%100}\", f\"right_value_{i}\", i * 2) \n",
    "    for i in range(10000)\n",
    "]\n",
    "\n",
    "left_df = spark.createDataFrame(left_data, [\"join_key\", \"left_val\", \"left_num\"])\n",
    "right_df = spark.createDataFrame(right_data, [\"join_key\", \"right_val\", \"right_num\"])\n",
    "\n",
    "# Force different join algorithms\n",
    "print(\"=== Testing Join Algorithms ===\")\n",
    "\n",
    "# 1. Default join (Catalyst chooses)\n",
    "print(\"1. Default join (Catalyst optimizer chooses):\")\n",
    "default_join = left_df.join(right_df, \"join_key\", \"inner\")\n",
    "default_join.explain(mode=\"formatted\")\n",
    "\n",
    "# 2. Force shuffle hash join\n",
    "print(\"\\n2. Forced Shuffle Hash Join:\")\n",
    "hash_join = left_df.join(right_df.hint(\"SHUFFLE_HASH\"), \"join_key\", \"inner\")\n",
    "hash_join.explain(mode=\"formatted\")\n",
    "\n",
    "# 3. Force sort-merge join\n",
    "print(\"\\n3. Forced Sort-Merge Join:\")\n",
    "merge_join = left_df.join(right_df.hint(\"MERGE\"), \"join_key\", \"inner\")\n",
    "merge_join.explain(mode=\"formatted\")\n",
    "\n",
    "# Performance comparison\n",
    "def benchmark_join(join_df, name):\n",
    "    start_time = time.time()\n",
    "    count = join_df.count()\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"{name}: {count:,} rows in {elapsed:.3f} seconds\")\n",
    "    return elapsed\n",
    "\n",
    "print(\"\\n‚è±Ô∏è PERFORMANCE COMPARISON:\")\n",
    "default_time = benchmark_join(default_join, \"Default\")\n",
    "hash_time = benchmark_join(hash_join, \"Hash\")\n",
    "merge_time = benchmark_join(merge_join, \"Merge\")\n",
    "\n",
    "# Algorithm recommendations\n",
    "print(\"\\nüìö JOIN ALGORITHM GUIDE:\")\n",
    "print(\"Shuffle Hash Join:\")\n",
    "print(\"  ‚úÖ Good for: Large tables, good hash distribution\")\n",
    "print(\"  ‚úÖ Fast: Single pass, in-memory hash tables\")\n",
    "print(\"  ‚ùå Bad for: Skewed data, memory pressure\")\n",
    "\n",
    "print(\"\\nSort-Merge Join:\")\n",
    "print(\"  ‚úÖ Good for: Very large tables, sorted data\")\n",
    "print(\"  ‚úÖ Scalable: External sorting if needed\")\n",
    "print(\"  ‚ùå Bad for: Small tables, random access patterns\")\n",
    "\n",
    "print(\"\\nBroadcast Join:\")\n",
    "print(\"  ‚úÖ Good for: Small lookup tables\")\n",
    "print(\"  ‚úÖ Fastest: No shuffle\")\n",
    "print(\"  ‚ùå Bad for: Large tables, memory constraints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßÇ Salting: Handling Data Skew\n",
    "\n",
    "### Salting Technique for Balanced Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salting for skew handling\n",
    "print(\"üßÇ SALTING FOR DATA SKEW\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create skewed data\n",
    "skewed_left = [\n",
    "    (\"hot_key\", f\"left_{i}\", i) for i in range(9000)  # 90% of data\n",
    "] + [\n",
    "    (f\"normal_key_{i}\", f\"left_{i+9000}\", i+9000) for i in range(1000)  # 10% of data\n",
    "]\n",
    "\n",
    "skewed_right = [\n",
    "    (\"hot_key\", f\"right_{i}\", i*2) for i in range(9000)  # 90% of data\n",
    "] + [\n",
    "    (f\"normal_key_{i}\", f\"right_{i+9000}\", (i+9000)*2) for i in range(1000)  # 10% of data\n",
    "]\n",
    "\n",
    "skewed_left_df = spark.createDataFrame(skewed_left, [\"join_key\", \"left_val\", \"left_id\"])\n",
    "skewed_right_df = spark.createDataFrame(skewed_right, [\"join_key\", \"right_val\", \"right_id\"])\n",
    "\n",
    "print(\"Skewed datasets created\")\n",
    "print(f\"Left: {skewed_left_df.count()} rows\")\n",
    "print(f\"Right: {skewed_right_df.count()} rows\")\n",
    "\n",
    "# Check skew\n",
    "key_dist = skewed_left_df.groupBy(\"join_key\").count().orderBy(\"count\", ascending=False)\n",
    "print(\"\\nKey distribution (top 5):\")\n",
    "key_dist.show(5)\n",
    "\n",
    "# Regular join (will be slow due to skew)\n",
    "print(\"\\n=== Regular Join (skewed) ===\")\n",
    "regular_skewed = skewed_left_df.join(skewed_right_df, \"join_key\", \"inner\")\n",
    "start_time = time.time()\n",
    "regular_count = regular_skewed.count()\n",
    "regular_time = time.time() - start_time\n",
    "print(f\"Regular join: {regular_count} rows in {regular_time:.3f} seconds\")\n",
    "\n",
    "# Salting solution\n",
    "def salt_dataframe(df, key_col, num_salts=10):\n",
    "    \"\"\"Add salt to DataFrame for balanced joins\"\"\"\n",
    "    return df.withColumn(\"salt\", F.floor(F.rand() * num_salts)) \\\n",
    "             .withColumn(\"salted_key\", F.concat(F.col(key_col), F.lit(\"_\"), F.col(\"salt\")))\n",
    "\n",
    "def unsalt_dataframe(df, original_key_col):\n",
    "    \"\"\"Remove salt from joined DataFrame\"\"\"\n",
    "    return df.withColumn(original_key_col, \n",
    "                        F.split(\"salted_key\", \"_\")[0]) \\\n",
    "             .drop(\"salt\", \"salted_key\")\n",
    "\n",
    "# Apply salting\n",
    "num_salts = 20  # Distribute hot key across 20 partitions\n",
    "\n",
    "salted_left = salt_dataframe(skewed_left_df, \"join_key\", num_salts)\n",
    "salted_right = salt_dataframe(skewed_right_df, \"join_key\", num_salts)\n",
    "\n",
    "# Create expanded right side for all salts\n",
    "all_salts = spark.range(0, num_salts).toDF(\"salt\")\n",
    "salted_right_expanded = skewed_right_df.crossJoin(all_salts) \\\n",
    "    .withColumn(\"salted_key\", F.concat(\"join_key\", F.lit(\"_\"), \"salt\"))\n",
    "\n",
    "print(\"\\n=== Salted Join ===\")\n",
    "# Join on salted keys\n",
    "salted_join = salted_left.join(salted_right_expanded, \"salted_key\", \"inner\")\n",
    "\n",
    "start_time = time.time()\n",
    "salted_count = salted_join.count()\n",
    "salted_time = time.time() - start_time\n",
    "print(f\"Salted join: {salted_count} rows in {salted_time:.3f} seconds\")\n",
    "\n",
    "# Performance comparison\n",
    "print(f\"\\nüéØ PERFORMANCE COMPARISON:\")\n",
    "print(f\"Regular join: {regular_time:.3f}s\")\n",
    "print(f\"Salted join: {salted_time:.3f}s\")\n",
    "if regular_time > 0:\n",
    "    improvement = regular_time / salted_time\n",
    "    print(f\"Salted join is {improvement:.1f}x faster!\")\n",
    "\n",
    "# Verify results are equivalent\n",
    "print(f\"Results match: {regular_count == salted_count}\")\n",
    "\n",
    "# Check partition distribution after salting\n",
    "salt_dist = salted_join.groupBy(\"salt\").count().orderBy(\"count\")\n",
    "print(\"\\nSalt distribution (should be balanced):\")\n",
    "salt_dist.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Join Reordering and Optimization\n",
    "\n",
    "### Catalyst Join Reordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join reordering\n",
    "print(\"üîÑ JOIN REORDERING OPTIMIZATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create multiple tables for complex joins\n",
    "customers_data = [\n",
    "    (f\"cust_{i}\", f\"Customer_{i}\", f\"segment_{(i%3)+1}\") \n",
    "    for i in range(1000)\n",
    "]\n",
    "\n",
    "orders_data = [\n",
    "    (f\"order_{i}\", f\"cust_{(i%1000)}\", f\"prod_{(i%100)+1}\", 10 + (i % 90))\n",
    "    for i in range(10000)\n",
    "]\n",
    "\n",
    "reviews_data = [\n",
    "    (f\"review_{i}\", f\"prod_{(i%100)+1}\", 1 + (i % 5), f\"Great product! {i}\")\n",
    "    for i in range(5000)\n",
    "]\n",
    "\n",
    "customers_df = spark.createDataFrame(customers_data, [\"customer_id\", \"customer_name\", \"segment\"])\n",
    "orders_df = spark.createDataFrame(orders_data, [\"order_id\", \"customer_id\", \"product_id\", \"amount\"])\n",
    "reviews_df = spark.createDataFrame(reviews_data, [\"review_id\", \"product_id\", \"rating\", \"comment\"])\n",
    "\n",
    "print(\"Created tables for complex joins:\")\n",
    "print(f\"Customers: {customers_df.count()} rows\")\n",
    "print(f\"Orders: {orders_df.count()} rows\")\n",
    "print(f\"Reviews: {reviews_df.count()} rows\")\n",
    "\n",
    "# Complex join (Catalyst will reorder for optimal performance)\n",
    "complex_join = customers_df.join(\n",
    "    orders_df, \"customer_id\", \"inner\"\n",
    ").join(\n",
    "    reviews_df, \"product_id\", \"left\"\n",
    ").join(\n",
    "    products_df, \"product_id\", \"inner\"  # products_df from earlier\n",
    ")\n",
    "\n",
    "print(\"\\nComplex 4-table join - Catalyst will optimize order:\")\n",
    "complex_join.explain(mode=\"formatted\")\n",
    "\n",
    "# Force specific join order (not recommended unless you know better)\n",
    "forced_order = customers_df.hint(\"LEADING\").join(\n",
    "    orders_df.hint(\"LEADING\"), \"customer_id\", \"inner\"\n",
    ").join(\n",
    "    reviews_df.hint(\"LEADING\"), \"product_id\", \"left\"\n",
    ")\n",
    "\n",
    "print(\"\\nForced join order (usually not optimal):\")\n",
    "forced_order.explain(mode=\"formatted\")\n",
    "\n",
    "# Performance comparison\n",
    "start_time = time.time()\n",
    "catalyst_count = complex_join.count()\n",
    "catalyst_time = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "forced_count = forced_order.count()\n",
    "forced_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nüéØ PERFORMANCE:\")\n",
    "print(f\"Catalyst optimized: {catalyst_count} rows in {catalyst_time:.3f}s\")\n",
    "print(f\"Forced order: {forced_count} rows in {forced_time:.3f}s\")\n",
    "print(f\"Results match: {catalyst_count == forced_count}\")\n",
    "\n",
    "# Best practice: Let Catalyst optimize!\n",
    "print(\"\\nüí° BEST PRACTICE:\")\n",
    "print(\"Trust Catalyst's join reordering - it's usually optimal!\")\n",
    "print(\"Only force join order if you have specific performance evidence.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Join Memory Management\n",
    "\n",
    "### Optimizing Memory Usage in Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory management\n",
    "print(\"üìä JOIN MEMORY MANAGEMENT\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Configure memory settings\n",
    "print(\"Current memory settings:\")\n",
    "memory_configs = [\n",
    "    \"spark.sql.shuffle.partitions\",\n",
    "    \"spark.sql.adaptive.enabled\",\n",
    "    \"spark.sql.adaptive.coalescePartitions.enabled\",\n",
    "    \"spark.memory.fraction\",\n",
    "    \"spark.memory.storageFraction\"\n",
    "]\n",
    "\n",
    "for config in memory_configs:\n",
    "    value = spark.conf.get(config, \"Not set\")\n",
    "    print(f\"{config}: {value}\")\n",
    "\n",
    "# Memory-aware join strategies\n",
    "def choose_memory_aware_join(left_df, right_df, join_key):\n",
    "    \"\"\"Choose join strategy based on memory constraints\"\"\"\n",
    "    \n",
    "    # Estimate sizes\n",
    "    left_size = left_df.count() * len(left_df.columns)\n",
    "    right_size = right_df.count() * len(right_df.columns)\n",
    "    \n",
    "    broadcast_threshold = int(spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\"))\n",
    "    \n",
    "    print(f\"Left table size: ~{left_size:,} cells\")\n",
    "    print(f\"Right table size: ~{right_size:,} cells\")\n",
    "    print(f\"Broadcast threshold: {broadcast_threshold:,} bytes\")\n",
    "    \n",
    "    # Choose strategy\n",
    "    if min(left_size, right_size) < broadcast_threshold // 100:  # Conservative\n",
    "        print(\"Using broadcast join (memory efficient)\")\n",
    "        if left_size < right_size:\n",
    "            return left_df.join(broadcast(right_df), join_key)\n",
    "        else:\n",
    "            return right_df.join(broadcast(left_df), join_key)\n",
    "    else:\n",
    "        print(\"Using shuffle join (memory scalable)\")\n",
    "        return left_df.join(right_df, join_key)\n",
    "\n",
    "# Test memory-aware joins\n",
    "memory_join = choose_memory_aware_join(orders_df, countries_df, \n",
    "    orders_df[\"customer_id\"].substr(1, 2) == countries_df[\"country_code\"])\n",
    "print(f\"Memory-aware join result: {memory_join.count()} rows\")\n",
    "\n",
    "# Spill monitoring\n",
    "print(\"\\nMonitoring for memory spills:\")\n",
    "print(\"Check Spark UI for:\")\n",
    "print(\"- Shuffle spill (memory) - should be minimal\")\n",
    "print(\"- Disk spill (disk) - indicates memory pressure\")\n",
    "print(\"- GC time - high values indicate memory issues\")\n",
    "\n",
    "# Memory optimization techniques\n",
    "print(\"\\nüõ†Ô∏è MEMORY OPTIMIZATION TECHNIQUES:\")\n",
    "print(\"1. Increase executor memory: --executor-memory 4g\")\n",
    "print(\"2. Reduce shuffle partitions: spark.sql.shuffle.partitions=200\")\n",
    "print(\"3. Enable adaptive execution: spark.sql.adaptive.enabled=true\")\n",
    "print(\"4. Use broadcast joins for small tables\")\n",
    "print(\"5. Cache frequently used tables\")\n",
    "print(\"6. Monitor and tune garbage collection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Advanced Join Patterns\n",
    "\n",
    "### Complex Join Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced join patterns\n",
    "print(\"üìà ADVANCED JOIN PATTERNS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Conditional joins\n",
    "print(\"1. CONDITIONAL JOINS\")\n",
    "conditional_join = sales_df.join(\n",
    "    products_df,\n",
    "    (sales_df[\"product_id\"] == products_df[\"product_id\"]) &\n",
    "    (sales_df[\"quantity\"] > 5),  # Only join for large orders\n",
    "    \"left\"\n",
    ")\n",
    "print(f\"Conditional join: {conditional_join.count()} rows\")\n",
    "\n",
    "# 2. Range joins\n",
    "print(\"\\n2. RANGE JOINS\")\n",
    "# Create salary ranges\n",
    "salary_ranges = [\n",
    "    (\"Entry\", 0, 50000),\n",
    "    (\"Mid\", 50000, 80000),\n",
    "    (\"Senior\", 80000, 120000)\n",
    "]\n",
    "\n",
    "ranges_df = spark.createDataFrame(salary_ranges, [\"level\", \"min_salary\", \"max_salary\"])\n",
    "\n",
    "# Range join (salary within range)\n",
    "range_join = employees_df.join(\n",
    "    ranges_df,\n",
    "    (employees_df[\"salary\"] >= ranges_df[\"min_salary\"]) &\n",
    "    (employees_df[\"salary\"] < ranges_df[\"max_salary\"]),\n",
    "    \"inner\"\n",
    ")\n",
    "print(f\"Range join: {range_join.count()} rows\")\n",
    "\n",
    "# 3. Multiple key joins\n",
    "print(\"\\n3. MULTIPLE KEY JOINS\")\n",
    "multi_key_join = sales_df.join(\n",
    "    products_df,\n",
    "    [\"product_id\"],  # List of join keys\n",
    "    \"inner\"\n",
    ")\n",
    "print(f\"Multi-key join: {multi_key_join.count()} rows\")\n",
    "\n",
    "# 4. Self joins\n",
    "print(\"\\n4. SELF JOINS\")\n",
    "self_join = employees_df.alias(\"e1\").join(\n",
    "    employees_df.alias(\"e2\"),\n",
    "    (col(\"e1.department\") == col(\"e2.department\")) &\n",
    "    (col(\"e1.emp_id\") != col(\"e2.emp_id\")) &\n",
    "    (col(\"e1.salary\") < col(\"e2.salary\")),\n",
    "    \"inner\"\n",
    ").select(\n",
    "    col(\"e1.name\").alias(\"lower_earner\"),\n",
    "    col(\"e1.salary\").alias(\"lower_salary\"),\n",
    "    col(\"e2.name\").alias(\"higher_earner\"),\n",
    "    col(\"e2.salary\").alias(\"higher_salary\")\n",
    ")\n",
    "print(f\"Self join: {self_join.count()} rows\")\n",
    "\n",
    "# 5. Anti joins (NOT IN)\n",
    "print(\"\\n5. ANTI JOINS\")\n",
    "anti_join = employees_df.join(\n",
    "    departments_df,\n",
    "    employees_df[\"department\"] == departments_df[\"dept_name\"],\n",
    "    \"left_anti\"  # Keep only rows NOT in right table\n",
    ")\n",
    "print(f\"Anti join (employees without departments): {anti_join.count()} rows\")\n",
    "\n",
    "# 6. Semi joins (EXISTS)\n",
    "print(\"\\n6. SEMI JOINS\")\n",
    "semi_join = employees_df.join(\n",
    "    departments_df,\n",
    "    employees_df[\"department\"] == departments_df[\"dept_name\"],\n",
    "    \"left_semi\"  # Keep left rows that have matches in right\n",
    ")\n",
    "print(f\"Semi join (employees with departments): {semi_join.count()} rows\")\n",
    "\n",
    "print(\"\\nüîß JOIN TYPE SUMMARY:\")\n",
    "print(\"inner: Only matching rows\")\n",
    "print(\"left: All left + matching right\")\n",
    "print(\"right: All right + matching left\")\n",
    "print(\"full: All rows from both\")\n",
    "print(\"left_semi: Left rows with matches (no right columns)\")\n",
    "print(\"left_anti: Left rows without matches\")\n",
    "print(\"cross: Cartesian product (avoid!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Join Performance Monitoring\n",
    "\n",
    "### Analyzing Join Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance monitoring\n",
    "print(\"üìä JOIN PERFORMANCE MONITORING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def analyze_join_performance(join_df, join_name=\"Join\"):\n",
    "    \"\"\"Comprehensive join performance analysis\"\"\"\n",
    "    \n",
    "    print(f\"=== {join_name} Performance Analysis ===\")\n",
    "    \n",
    "    # Execution plan analysis\n",
    "    plan = join_df.explain(mode=\"formatted\")\n",
    "    \n",
    "    join_type = \"Unknown\"\n",
    "    if \"BroadcastHashJoin\" in str(plan):\n",
    "        join_type = \"Broadcast Hash Join\"\n",
    "    elif \"SortMergeJoin\" in str(plan):\n",
    "        join_type = \"Sort Merge Join\"\n",
    "    elif \"ShuffledHashJoin\" in str(plan):\n",
    "        join_type = \"Shuffle Hash Join\"\n",
    "    \n",
    "    print(f\"Join Type: {join_type}\")\n",
    "    \n",
    "    # Performance metrics\n",
    "    start_time = time.time()\n",
    "    result_count = join_df.count()\n",
    "    execution_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"Result Count: {result_count:,} rows\")\n",
    "    print(f\"Execution Time: {execution_time:.3f} seconds\")\n",
    "    \n",
    "    # Throughput\n",
    "    if execution_time > 0:\n",
    "        throughput = result_count / execution_time\n",
    "        print(f\"Throughput: {throughput:,.0f} rows/second\")\n",
    "    \n",
    "    # Data distribution check\n",
    "    partition_counts = join_df.rdd.mapPartitionsWithIndex(\n",
    "        lambda idx, iter: [(idx, sum(1 for _ in iter))]\n",
    "    ).collect()\n",
    "    \n",
    "    counts = [count for _, count in partition_counts]\n",
    "    if counts:\n",
    "        avg_count = sum(counts) / len(counts)\n",
    "        max_count = max(counts)\n",
    "        skew_ratio = max_count / avg_count if avg_count > 0 else 0\n",
    "        \n",
    "        print(f\"Partition Distribution:\")\n",
    "        print(f\"  Average: {avg_count:,.0f} rows/partition\")\n",
    "        print(f\"  Maximum: {max_count:,.0f} rows/partition\")\n",
    "        print(f\"  Skew Ratio: {skew_ratio:.2f}x\")\n",
    "        \n",
    "        if skew_ratio > 3:\n",
    "            print(\"  ‚ö†Ô∏è  HIGH SKEW DETECTED\")\n",
    "        else:\n",
    "            print(\"  ‚úÖ Good distribution\")\n",
    "    \n",
    "    return {\n",
    "        'join_type': join_type,\n",
    "        'execution_time': execution_time,\n",
    "        'result_count': result_count,\n",
    "        'skew_ratio': skew_ratio if 'skew_ratio' in locals() else None\n",
    "    }\n",
    "\n",
    "# Compare different join strategies\n",
    "print(\"Comparing join strategies:\")\n",
    "\n",
    "# Broadcast join\n",
    "broadcast_join = sales_df.join(broadcast(countries_df), \n",
    "    sales_df[\"customer_id\"].substr(1, 2) == countries_df[\"country_code\"])\n",
    "broadcast_stats = analyze_join_performance(broadcast_join, \"Broadcast Join\")\n",
    "\n",
    "# Regular join\n",
    "regular_join = sales_df.join(countries_df, \n",
    "    sales_df[\"customer_id\"].substr(1, 2) == countries_df[\"country_code\"])\n",
    "regular_stats = analyze_join_performance(regular_join, \"Regular Join\")\n",
    "\n",
    "# Performance comparison\n",
    "if broadcast_stats['execution_time'] > 0 and regular_stats['execution_time'] > 0:\n",
    "    speedup = regular_stats['execution_time'] / broadcast_stats['execution_time']\n",
    "    print(f\"\\nüéØ Broadcast join is {speedup:.1f}x {'faster' if speedup > 1 else 'slower'} than regular join\")\n",
    "\n",
    "# Recommendations\n",
    "print(\"\\nüí° PERFORMANCE RECOMMENDATIONS:\")\n",
    "print(\"1. Use broadcast joins for small lookup tables\")\n",
    "print(\"2. Monitor execution plans for optimal join strategies\")\n",
    "print(\"3. Check partition skew and balance if needed\")\n",
    "print(\"4. Enable adaptive query execution\")\n",
    "print(\"5. Cache frequently joined tables\")\n",
    "print(\"6. Use salting for highly skewed join keys\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Key Takeaways\n",
    "\n",
    "### Join Strategy Selection:\n",
    "\n",
    "| Scenario | Recommended Strategy | Reason |\n",
    "|----------|---------------------|---------|\n",
    "| Small table + Large table | Broadcast Join | No shuffle, fastest |\n",
    "| Large tables, good distribution | Shuffle Hash Join | In-memory, fast |\n",
    "| Large tables, sorted | Sort-Merge Join | External sorting |\n",
    "| Skewed join keys | Salting + Join | Balanced distribution |\n",
    "| Multiple small tables | Broadcast all | Minimize network |\n",
    "\n",
    "### Performance Optimization:\n",
    "- **Broadcast threshold**: Default 10MB, tune based on cluster\n",
    "- **Join reordering**: Trust Catalyst optimizer\n",
    "- **Memory management**: Monitor spills and GC\n",
    "- **Skew handling**: Salt keys or repartition\n",
    "- **Caching**: Cache broadcast tables\n",
    "\n",
    "### Monitoring Essentials:\n",
    "- **Execution plans**: Check for optimal strategies\n",
    "- **Partition distribution**: Identify skew\n",
    "- **Shuffle metrics**: Network and disk I/O\n",
    "- **Memory usage**: GC time and spills\n",
    "\n",
    "**Mastering join optimization can improve performance by 10-100x!**\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ You now have the power to optimize joins for any data size and distribution!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
