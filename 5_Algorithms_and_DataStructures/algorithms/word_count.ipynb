{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Count Algorithm\n",
    "\nClassic MapReduce example in PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Input Data\n",
    "\nFirst, let's create some sample text data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data\n",
    "data = [\n",
    "    \"crazy crazy fox jumped\",\n",
    "    \"crazy fox jumped\", \n",
    "    \"fox is fast\",\n",
    "    \"fox is smart\",\n",
    "    \"dog is smart\"\n",
    "]\n",
    "\n",
    "# Write to file\n",
    "with open(\"data.txt\", \"w\") as f:\n",
    "    for line in data:\n",
    "        f.write(line + \"\n\")\n",
    "\n",
    "print(\"Sample data created:\")\n",
    "with open(\"data.txt\", \"r\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Count Implementation\n",
    "\nNow let's implement the classic word count algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder \\n",
    "    .appName(\"WordCount\") \\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read text file\n",
    "text_file = spark.sparkContext.textFile(\"data.txt\")\n",
    "\n",
    "# Word count implementation\n",
    "counts = text_file \\n",
    "    .flatMap(lambda line: line.split(\" \")) \\n",
    "    .map(lambda word: (word, 1)) \\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Collect and display results\n",
    "results = counts.collect()\n",
    "print(\"Word Count Results:\")\n",
    "for word, count in sorted(results):\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative Implementation\n",
    "\nUsing DataFrame API for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, explode, col\n",
    "\n",
    "# Read as DataFrame\n",
    "df = spark.read.text(\"data.txt\")\n",
    "\n",
    "# Word count using DataFrame API\n",
    "word_counts_df = df \\n",
    "    .select(split(col(\"value\"), \" \").alias(\"words\")) \\n",
    "    .select(explode(col(\"words\")).alias(\"word\")) \\n",
    "    .groupBy(\"word\") \\n",
    "    .count() \\n",
    "    .orderBy(\"word\")\n",
    "\n",
    "print(\"DataFrame Word Count Results:\")\n",
    "word_counts_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Concepts\n",
    "\n- **flatMap**: Split lines into individual words\n",
    "- **map**: Transform each word to (word, 1) pair\n",
    "- **reduceByKey**: Aggregate counts by word\n",
    "- **collect**: Bring results to driver (use carefully with large data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}