{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# combineByKey Algorithm\n",
    "\nAdvanced aggregation operations in PySpark RDDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n is a powerful transformation for custom aggregations on key-value RDDs. It allows you to:\n",
    "- Create initial accumulator values\n",
    "- Merge values into accumulators\n",
    "- Combine accumulators from different partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder \\n",
    "    .appName(\"combineByKey\") \\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Example: Word Count\n",
    "\nLet's start with a simple word count using combineByKey:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "data = [\n",
    "    (\"spark\", 1),\n",
    "    (\"hadoop\", 1),\n",
    "    (\"spark\", 1),\n",
    "    (\"kafka\", 1),\n",
    "    (\"spark\", 1),\n",
    "    (\"hadoop\", 1)\n",
    "]\n",
    "\n",
    "# Create RDD\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# combineByKey for word count\n",
    "result = rdd.combineByKey(\n",
    "    lambda v: v,                    # createCombiner: initial value\n",
    "    lambda acc, v: acc + v,         # mergeValue: add to accumulator\n",
    "    lambda acc1, acc2: acc1 + acc2  # mergeCombiners: combine accumulators\n",
    ")\n",
    "\n",
    "print(\"Word Count Results:\")\n",
    "for word, count in sorted(result.collect()):\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Example: Statistics\n",
    "\nCalculate min, max, and count for each key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data: (key, value)\n",
    "stats_data = [\n",
    "    (\"A\", 10), (\"A\", 20), (\"A\", 15),\n",
    "    (\"B\", 5), (\"B\", 25), (\"B\", 30),\n",
    "    (\"C\", 8), (\"C\", 12)\n",
    "]\n",
    "\n",
    "stats_rdd = sc.parallelize(stats_data)\n",
    "\n",
    "# Calculate min, max, count for each key\n",
    "stats_result = stats_rdd.combineByKey(\n",
    "    lambda v: (v, v, 1),                    # (min, max, count)\n",
    "    lambda acc, v: (min(acc[0], v), max(acc[1], v), acc[2] + 1),\n",
    "    lambda acc1, acc2: (min(acc1[0], acc2[0]), max(acc1[1], acc2[1]), acc1[2] + acc2[2])\n",
    ")\n",
    "\n",
    "print(\"Statistics per Key:\")\n",
    "for key, (min_val, max_val, count) in sorted(stats_result.collect()):\n",
    "    print(f\"{key}: min={min_val}, max={max_val}, count={count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex Example: Moving Average\n",
    "\nCalculate moving averages with combineByKey:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series data: (key, (timestamp, value))\n",
    "time_data = [\n",
    "    (\"stock_A\", (1, 100)),\n",
    "    (\"stock_A\", (2, 105)),\n",
    "    (\"stock_A\", (3, 102)),\n",
    "    (\"stock_B\", (1, 50)),\n",
    "    (\"stock_B\", (2, 52)),\n",
    "    (\"stock_B\", (3, 48))\n",
    "]\n",
    "\n",
    "time_rdd = sc.parallelize(time_data)\n",
    "\n",
    "# Calculate moving averages\n",
    "moving_avg = time_rdd.combineByKey(\n",
    "    lambda v: ([v[1]], v[1]),                    # (values_list, current_sum)\n",
    "    lambda acc, v: (acc[0] + [v[1]], acc[1] + v[1]),\n",
    "    lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1])\n",
    ")\n",
    "\n",
    "print(\"Moving Averages:\")\n",
    "for key, (values, total) in sorted(moving_avg.collect()):\n",
    "    avg = total / len(values)\n",
    "    print(f\"{key}: values={values}, average={avg:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Parameters\n",
    "\n- **createCombiner**: Creates initial accumulator for each key\n",
    "- **mergeValue**: Merges a new value into existing accumulator\n",
    "- **mergeCombiners**: Combines accumulators from different partitions\n",
    "\n## Performance Notes\n",
    "\n- More efficient than  + custom aggregation\n",
    "- Allows for incremental updates and memory efficiency\n",
    "- Essential for complex aggregations in distributed computing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}